#=========================================================================
# File name         : optimizer_config.yaml
# File Description  : Parameter & Hyperparameter Optimizer Configurations
# File Version      : 1.0
# Author            : Srini Ananthakrishnan
# Date created      : 04/28/2017
# Date last modified: 05/05/2017
#=========================================================================
#
# OPTIMIZER CONFIGURATION FILE
# File contains dictionary like items in yaml. Format of which is key: value(s)
# User may change value(s) here and sacred framework will reflect in running configuration
#
# Example Usage:
# python optimizer.py with optimizer_config.yaml
#
# ------------------------------------------
# Class object 'name' instantiated by sacred
# ------------------------------------------
trainer: Optimizer         # Optimizer Class Name. DONT CHANGE THIS !!!

# --------------------------------------
# File to be forked for distributed jobs
# --------------------------------------
file2distribute: nn_distributed.py

# ------------------------------------------
# Neural Network Model
# ------------------------------------------
nn_model:
- Regressor
#- Custom

# ------------------------------------------
# Algorithm for Hyperparameter Optimization
# ------------------------------------------
hyperparam_opt: RS        # hyperparameter optimizer algorithm. Default RS: Random Search


# ----------------------------------------------------------------
# Stages: Optimizer and Training Epochs Stages
# Usage:
# Stage 1 - largest search:
#   Inner iterations = 1K
#   Outer iterations = 10K
#
# Stage 2 - search with smaller volume around best pt of Stage 1:
#   Inner iterations = 5K
#   Outer iterations = 2K
#
# Stage 3 - search with smaller volume around best pt from Stage 2:
#   Inner iterations = 10K
#   Outer iterations = 1K
# -----------------------------------------------------------------
running_stage: 0          # placeholder to indicate running stage. Not a configuration.
# Default config to run on CPU
opt_stages:               # Optimizer and Training Epochs Stages
- - 1000
  - 4
- - 700
  - 3
- - 500
  - 2
## For Training on Large GPU/CPU Servers
#opt_stages:
#- - 1000
#  - 1000
#- - 5000
#  - 750
#- - 10000
#  - 500

# ---------------------------------
# Hyperparameters For Optimization
# ---------------------------------
nn_dimensions:            # number of feed forward neural network nodes for [input, output]
- 4
- 1
hidden_layers:            # hidden_layer [min, max]
- 4
- 10
batch_size:               # batch size as [lower_bound, upper_bound]
- 100
- 2000
activation:               # activation for non-linearity. Default is 'relu'. Supports 'tanh'
- relu
- tanh
learning_rate:            # learning rate as [lower_bound, upper_bound]
- 0.001
- 0.0001
train_optimizer:          # optimizer to be used for train. Default is 'Adam'. Supports 'sgd', 'Adagrad'
- Adam
- sgd
- Adagrad
opt_tolerance: 1.0e-05    # outer optimizer loop loss convergence threshold
train_tolerance: 1.0e-08  # inner train loop loss convergence threshold
opt_epoch: 3              # optimizer epoch is the outer loop for optimizing hyperparameters
train_epoch: 1000         # training epoch is the inner loop for training input data
rnn_max_seq_length: 100   # int Maximum length of the traning sequences to generate
rnn_state_dim:            # int or int[] State dimension. Can use a list to stack RNNs.
- 50
- 50
- 50

# ---------------------------------
# Directory for loading real data
# Valid ONLY when load_data is true
# ---------------------------------
load_data: false          # When 'true' Load real data(X) of shape (N, M) and labels(Y) of shape (N,). Otherwise generate synthetic data.
load_data_dir: data/source_code  # Real data directory (like Boston, Iris, etc.)

# --------------------------------------------------
# Parameters for regressor synthetic data generation
# Valid ONLY when load_data is false
# --------------------------------------------------
add_cosine: false         # transforms Y to element wise cosine (used if power_method is True)
add_noise: false          # adds random noise to output Y (used if power_method is True)
power_method: false       # generate matrix polynomial of incremental "power" of (M). Used ONLY load_data is false
data_instances: 10000     # number of input data instances (N). Used ONLY load_data is false
data_features: 4          # number of input data features (M). Used ONLY load_data is false


# ---------------------------------
# Logging Directory
# ---------------------------------
train_log_dir: /tmp/nn_dist/train_logs

# ----------------------------------------------------
# TensorFlow Sync_Replicas (synchronized workers) mode
# In this mode the parameter updates from workers are
# aggregated before applied to avoid stale gradients
# ----------------------------------------------------
sync_replicas: false    # Use the sync_replicas (synchronized replicas) mode

# -------------------------------------
# TensorFlow GPU config
# -------------------------------------
num_gpus: 1             # Number of GPUs. If > 0 GPUs will be used by Worker. Otherwise CPUs.

# -------------------------------------
# Tensorflow Distributed Cluster Config
# Format:
# localhost:port_number (or)
# 193.168.xxx.xxx:port_number
# -------------------------------------
# Distribued
ps_hosts: localhost:2223,localhost:2224 # parameter server config
worker_hosts: localhost:2225,localhost:2226,localhost:2227,localhost:2228 # worker server config
# Simple
#ps_hosts: localhost:2223 # parameter server config
#worker_hosts: localhost:2225 # worker server config

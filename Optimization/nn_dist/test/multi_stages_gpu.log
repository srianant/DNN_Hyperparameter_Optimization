START OF STAGED EPOCH #######################>> [ 1 ]
nn_model must be specified. Usage: python optimizer.py nn_model='Regressor'
Defaulting to 'Regressor
'Epoch [1] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 938,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008629066907706081,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 5, 4, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 1,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 1, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202352.549401)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202386.78418)
('Worker processing elapsed time: ', 34.234778881073, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[1]', 'EPOCH LOSS:', 20.532908621920374, 'BEST LOSS:', 20.532908621920374)
('END OF Optimizer EPOCH =====================>>[', 1, ']')


'Epoch [2] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1855,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007432640300832089,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 7, 5, 6, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 2,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 2, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202386.790451)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202422.129785)
('Worker processing elapsed time: ', 35.33933401107788, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[2]', 'EPOCH LOSS:', 0.021974338319052097, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 2, ']')


'Epoch [3] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1676,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000717283097414873,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 3,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 3, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202422.136025)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202456.853622)
('Worker processing elapsed time: ', 34.717597007751465, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[3]', 'EPOCH LOSS:', 0.030553589929894272, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 3, ']')


'Epoch [4] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 543,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008762889935920681,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 6, 7, 6, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 4,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 4, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202456.858537)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202491.23925)
('Worker processing elapsed time: ', 34.38071298599243, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[4]', 'EPOCH LOSS:', 1529.6512362100184, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 4, ']')


'Epoch [5] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 670,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006635844447253083,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 8, 4, 8, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 5,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 5, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202491.2439)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202526.661161)
('Worker processing elapsed time: ', 35.41726088523865, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[5]', 'EPOCH LOSS:', 0.026502710972817856, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 5, ']')


'Epoch [6] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1323,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016315816432672372,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 5, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 6,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 6, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202526.666228)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202560.420632)
('Worker processing elapsed time: ', 33.754403829574585, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[6]', 'EPOCH LOSS:', 0.025096825273188086, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 6, ']')


'Epoch [7] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1312,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006712428043221428,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 4, 6, 7, 9, 6, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 7,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 7, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202560.425769)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202594.685393)
('Worker processing elapsed time: ', 34.259624004364014, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[7]', 'EPOCH LOSS:', 0.025442916120596983, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 7, ']')


'Epoch [8] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1781,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005024756118983292,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 4, 8, 9, 5, 4, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 8,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 8, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202594.691163)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202629.006704)
('Worker processing elapsed time: ', 34.31554102897644, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[8]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 8, ']')


'Epoch [9] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1146,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007840514392135807,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 6, 4, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 9,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 9, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202629.01179)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202663.954404)
('Worker processing elapsed time: ', 34.94261407852173, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[9]', 'EPOCH LOSS:', 14.496251690129425, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 9, ']')


'Epoch [10] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 482,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00015470457264652598,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 7, 6, 7, 9, 6, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 10,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 10, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202663.959578)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202698.189028)
('Worker processing elapsed time: ', 34.2294499874115, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[10]', 'EPOCH LOSS:', 0.023192413168380468, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 10, ']')


'Epoch [11] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 389,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006381031146107138,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 5, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 11,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 11, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202698.194778)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202731.886552)
('Worker processing elapsed time: ', 33.691774129867554, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[11]', 'EPOCH LOSS:', 0.02402895786432039, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 11, ']')


'Epoch [12] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1642,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00030790777297070627,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 8, 7, 5, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 12,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 12, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202731.891424)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202767.20453)
('Worker processing elapsed time: ', 35.313106060028076, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[12]', 'EPOCH LOSS:', 1.6126269660661463, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 12, ']')


'Epoch [13] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1149,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00015710001174587649,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 13,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 13, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202767.210158)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202801.655029)
('Worker processing elapsed time: ', 34.444870948791504, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[13]', 'EPOCH LOSS:', 7.110874064125337, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 13, ']')


'Epoch [14] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1329,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000596444354966951,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 9, 4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 14,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 14, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202801.660226)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202836.750635)
('Worker processing elapsed time: ', 35.09040880203247, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[14]', 'EPOCH LOSS:', 0.025654597207641801, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 14, ']')


'Epoch [15] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 835,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008419095352845895,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 6, 8, 4, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 15,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 15, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202836.755891)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202870.720118)
('Worker processing elapsed time: ', 33.964226961135864, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[15]', 'EPOCH LOSS:', 0.024962069910064659, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 15, ']')


'Epoch [16] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 420,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00032239267926556785,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 6, 4, 6, 6, 7, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 16,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 16, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202870.725502)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202906.573033)
('Worker processing elapsed time: ', 35.84753108024597, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[16]', 'EPOCH LOSS:', 0.024171521318324188, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 16, ']')


'Epoch [17] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1513,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000986272291879099,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 4, 8, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 17,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 17, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202906.578867)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202940.423343)
('Worker processing elapsed time: ', 33.844475984573364, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[17]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 17, ']')


'Epoch [18] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1668,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007605416070305035,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 4, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 18,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 18, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202940.429163)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494202975.013029)
('Worker processing elapsed time: ', 34.583866119384766, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[18]', 'EPOCH LOSS:', 0.22371420330085703, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 18, ']')


'Epoch [19] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 729,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008867339609819933,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 7, 6, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 19,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 19, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494202975.018226)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203010.192898)
('Worker processing elapsed time: ', 35.17467212677002, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[19]', 'EPOCH LOSS:', 0.03457121338386588, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 19, ']')


'Epoch [20] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1791,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009672861303664043,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 7, 4, 5, 4, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 20,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 20, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203010.19844)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203045.883643)
('Worker processing elapsed time: ', 35.685202836990356, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[20]', 'EPOCH LOSS:', 0.0232149986733215, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 20, ']')


'Epoch [21] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 767,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00018703903575982756,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 8, 7, 5, 5, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 21,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 21, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203045.888886)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203080.53116)
('Worker processing elapsed time: ', 34.642274141311646, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[21]', 'EPOCH LOSS:', 159.42694707160931, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 21, ']')


'Epoch [22] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 831,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00042938505048993885,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 22,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 22, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203080.536106)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203114.412253)
('Worker processing elapsed time: ', 33.87614679336548, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[22]', 'EPOCH LOSS:', 4.0621976816837053, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 22, ']')


'Epoch [23] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 318,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004909155012498058,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 8, 5, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 23,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 23, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203114.417533)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203149.487045)
('Worker processing elapsed time: ', 35.069512128829956, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[23]', 'EPOCH LOSS:', 7.3596667252852717, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 23, ']')


'Epoch [24] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 192,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008149268880619054,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 8, 6, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 24,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 24, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203149.492634)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203183.702459)
('Worker processing elapsed time: ', 34.20982503890991, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[24]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 24, ']')


'Epoch [25] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 855,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004995425609012819,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 9, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 25,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 25, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203183.707326)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203217.767465)
('Worker processing elapsed time: ', 34.060139179229736, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[25]', 'EPOCH LOSS:', 1381.353928195676, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 25, ']')


'Epoch [26] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1024,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006048854600241501,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 5, 8, 6, 6, 5, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 26,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 26, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203217.772191)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203252.137748)
('Worker processing elapsed time: ', 34.365556955337524, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[26]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 26, ']')


'Epoch [27] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1925,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004444223496045236,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 5, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 27,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 27, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203252.142412)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203286.847447)
('Worker processing elapsed time: ', 34.70503497123718, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[27]', 'EPOCH LOSS:', 0.37719919363813037, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 27, ']')


'Epoch [28] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1931,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007267022398154874,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 8, 6, 6, 8, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 28,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 28, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203286.852528)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203321.553213)
('Worker processing elapsed time: ', 34.700684785842896, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[28]', 'EPOCH LOSS:', 159.76035812672143, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 28, ']')


'Epoch [29] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1333,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00044894173368517534,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 29,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 29, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203321.557899)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203355.148236)
('Worker processing elapsed time: ', 33.59033703804016, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[29]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 29, ']')


'Epoch [30] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1480,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007774468829927837,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 6, 7, 9, 9, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 30,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 30, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203355.153094)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203389.23788)
('Worker processing elapsed time: ', 34.08478593826294, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[30]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 30, ']')


'Epoch [31] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1158,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004073055301315741,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 5, 8, 5, 5, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 31,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 31, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203389.243182)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203423.415148)
('Worker processing elapsed time: ', 34.17196607589722, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[31]', 'EPOCH LOSS:', 0.025712780406127615, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 31, ']')


'Epoch [32] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1527,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003344314116561346,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 8, 4, 7, 6, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 32,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 32, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203423.420745)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203459.047823)
('Worker processing elapsed time: ', 35.62707805633545, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[32]', 'EPOCH LOSS:', 0.029334070972634498, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 32, ']')


'Epoch [33] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1394,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007035924250499205,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 7, 8, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 33,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 33, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203459.05355)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203493.284427)
('Worker processing elapsed time: ', 34.23087692260742, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[33]', 'EPOCH LOSS:', 836.94605005917481, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 33, ']')


'Epoch [34] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 486,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007191307298147854,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 5, 4, 6, 5, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 34,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 34, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203493.289656)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203528.022177)
('Worker processing elapsed time: ', 34.732521057128906, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[34]', 'EPOCH LOSS:', 0.69043465675249482, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 34, ']')


'Epoch [35] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 559,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005813514093325565,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 9, 9, 6, 6, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 35,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 35, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203528.027469)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203563.694766)
('Worker processing elapsed time: ', 35.66729712486267, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[35]', 'EPOCH LOSS:', 0.029286853229666659, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 35, ']')


'Epoch [36] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1946,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000344429705750173,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 4, 5, 6, 4, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 36,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 36, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203563.699818)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203597.865904)
('Worker processing elapsed time: ', 34.166086196899414, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[36]', 'EPOCH LOSS:', 0.023491915582192942, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 36, ']')


'Epoch [37] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1247,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00045894564741893605,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 7, 4, 9, 4, 6, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 37,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 37, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203597.870877)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203632.83196)
('Worker processing elapsed time: ', 34.96108293533325, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[37]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 37, ']')


'Epoch [38] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 672,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002270881261166891,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 9, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 38,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 38, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203632.837957)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203667.133928)
('Worker processing elapsed time: ', 34.295971155166626, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[38]', 'EPOCH LOSS:', 8.4472305084171815, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 38, ']')


'Epoch [39] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1568,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001913316031245736,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 6, 7, 7, 4, 7, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 39,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 39, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203667.139756)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203703.008893)
('Worker processing elapsed time: ', 35.86913704872131, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[39]', 'EPOCH LOSS:', 23.243213865868576, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 39, ']')


'Epoch [40] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 897,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005160125767058553,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 7, 5, 8, 9, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 40,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 40, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203703.014647)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203738.66299)
('Worker processing elapsed time: ', 35.648343086242676, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[40]', 'EPOCH LOSS:', 0.033418610181769663, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 40, ']')


'Epoch [41] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1363,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009884598524841322,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 4, 6, 7, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 41,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 41, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203738.667984)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203773.897522)
('Worker processing elapsed time: ', 35.22953796386719, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[41]', 'EPOCH LOSS:', 0.040157956693762084, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 41, ']')


'Epoch [42] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1883,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009175448142361719,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 42,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 42, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203773.90268)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203807.724541)
('Worker processing elapsed time: ', 33.821861028671265, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[42]', 'EPOCH LOSS:', 1.1005565058469506, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 42, ']')


'Epoch [43] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1935,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004792492691015508,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 43,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 43, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203807.730287)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203841.672701)
('Worker processing elapsed time: ', 33.94241380691528, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[43]', 'EPOCH LOSS:', 0.11851492452275972, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 43, ']')


'Epoch [44] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 723,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007903159004082784,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 44,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 44, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203841.678605)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203875.959589)
('Worker processing elapsed time: ', 34.28098392486572, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[44]', 'EPOCH LOSS:', 0.024217401781166568, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 44, ']')


'Epoch [45] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1554,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004785803273251409,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 5, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 45,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 45, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203875.964904)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203910.596439)
('Worker processing elapsed time: ', 34.631534814834595, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[45]', 'EPOCH LOSS:', 8.2073227186443507, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 45, ']')


'Epoch [46] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1429,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003146492024173885,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 7, 7, 7, 5, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 46,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 46, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203910.602128)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203945.334755)
('Worker processing elapsed time: ', 34.73262691497803, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[46]', 'EPOCH LOSS:', 148.69001883589732, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 46, ']')


'Epoch [47] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 924,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008153601526055227,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 9, 4, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 47,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 47, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203945.340872)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494203980.281076)
('Worker processing elapsed time: ', 34.94020390510559, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[47]', 'EPOCH LOSS:', 0.043787510280643573, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 47, ']')


'Epoch [48] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 175,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002414670694969621,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 48,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 48, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494203980.28606)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204014.173518)
('Worker processing elapsed time: ', 33.887457847595215, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[48]', 'EPOCH LOSS:', 2.8065235735866816, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 48, ']')


'Epoch [49] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 993,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002464495139687492,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 5, 5, 5, 5, 4, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 49,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 49, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204014.179176)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204048.442053)
('Worker processing elapsed time: ', 34.262876987457275, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[49]', 'EPOCH LOSS:', 0.024701696610170815, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 49, ']')


'Epoch [50] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 701,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00040444163967928975,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 7, 7, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 50,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 50, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204048.447385)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204082.720342)
('Worker processing elapsed time: ', 34.27295684814453, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[50]', 'EPOCH LOSS:', 3982.3345121314414, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 50, ']')


'Epoch [51] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1516,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00043561047367143217,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 51,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 51, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204082.726399)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204116.352735)
('Worker processing elapsed time: ', 33.626336097717285, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[51]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 51, ']')


'Epoch [52] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1752,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007644513711753665,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 4, 9, 8, 5, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 52,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 52, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204116.358322)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204151.023501)
('Worker processing elapsed time: ', 34.66517901420593, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[52]', 'EPOCH LOSS:', 1.1290602101240392, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 52, ']')


'Epoch [53] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 580,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007891214276224893,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 4, 7, 8, 5, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 53,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 53, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204151.029224)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204185.780614)
('Worker processing elapsed time: ', 34.75138998031616, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[53]', 'EPOCH LOSS:', 3.7714453989918826, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 53, ']')


'Epoch [54] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1907,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004975935962105775,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 6, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 54,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 54, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204185.785937)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204219.499646)
('Worker processing elapsed time: ', 33.71370887756348, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[54]', 'EPOCH LOSS:', 0.023440624154718571, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 54, ']')


'Epoch [55] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1949,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006062607043234022,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 8, 7, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 55,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 55, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204219.50455)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204253.821416)
('Worker processing elapsed time: ', 34.31686592102051, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[55]', 'EPOCH LOSS:', 3916.2163643566073, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 55, ']')


'Epoch [56] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1326,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00032652957943140194,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 4, 7, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 56,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 56, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204253.826604)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204287.722488)
('Worker processing elapsed time: ', 33.89588403701782, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[56]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 56, ']')


'Epoch [57] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1200,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004923430483367256,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 57,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 57, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204287.728381)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204321.306539)
('Worker processing elapsed time: ', 33.578158140182495, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[57]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 57, ']')


'Epoch [58] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1073,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009394137690414163,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 9, 4, 9, 4, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 58,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 58, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204321.31269)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204356.032347)
('Worker processing elapsed time: ', 34.7196569442749, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[58]', 'EPOCH LOSS:', 31.573879843927376, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 58, ']')


'Epoch [59] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1178,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008275103195886547,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 9, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 59,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 59, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204356.037451)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204390.702728)
('Worker processing elapsed time: ', 34.66527700424194, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[59]', 'EPOCH LOSS:', 0.023402037376913681, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 59, ']')


'Epoch [60] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1641,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009031347547730023,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 60,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 60, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204390.708603)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204424.607691)
('Worker processing elapsed time: ', 33.89908814430237, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[60]', 'EPOCH LOSS:', 28.709549280270377, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 60, ']')


'Epoch [61] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 446,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002714420602621333,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 6, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 61,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 61, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204424.612804)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204458.679282)
('Worker processing elapsed time: ', 34.06647801399231, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[61]', 'EPOCH LOSS:', 70.251823671097242, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 61, ']')


'Epoch [62] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 405,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003226760652990712,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 7, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 62,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 62, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204458.685324)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204492.369529)
('Worker processing elapsed time: ', 33.684205055236816, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[62]', 'EPOCH LOSS:', 0.024333767902301379, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 62, ']')


'Epoch [63] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1218,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007905415668866267,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 4, 7, 8, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 63,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 63, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204492.375476)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204526.425353)
('Worker processing elapsed time: ', 34.04987716674805, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[63]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 63, ']')


'Epoch [64] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 147,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009793476305281564,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 6, 6, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 64,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 64, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204526.430545)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204560.692425)
('Worker processing elapsed time: ', 34.26187992095947, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[64]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 64, ']')


'Epoch [65] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1693,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008070378493214366,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 8, 6, 4, 4, 4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 65,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 65, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204560.698107)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204595.028616)
('Worker processing elapsed time: ', 34.33050894737244, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[65]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 65, ']')


'Epoch [66] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1587,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007205852056322722,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 6, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 66,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 66, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204595.033715)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204629.687789)
('Worker processing elapsed time: ', 34.65407395362854, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[66]', 'EPOCH LOSS:', 0.024063058643463316, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 66, ']')


'Epoch [67] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 232,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002315649981075058,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 8, 7, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 67,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 67, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204629.693605)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204663.501182)
('Worker processing elapsed time: ', 33.80757713317871, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[67]', 'EPOCH LOSS:', 0.025391133475958075, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 67, ']')


'Epoch [68] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1029,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004510910341184748,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 7, 5, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 68,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 68, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204663.506329)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204698.467554)
('Worker processing elapsed time: ', 34.9612250328064, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[68]', 'EPOCH LOSS:', 0.17872171110405988, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 68, ']')


'Epoch [69] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1801,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007284652645531493,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 5, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 69,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 69, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204698.473552)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204733.248966)
('Worker processing elapsed time: ', 34.77541399002075, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[69]', 'EPOCH LOSS:', 0.6019890126599684, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 69, ']')


'Epoch [70] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 915,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009209411041381279,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 70,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 70, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204733.254071)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204766.786913)
('Worker processing elapsed time: ', 33.53284192085266, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[70]', 'EPOCH LOSS:', 0.024848970897365926, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 70, ']')


'Epoch [71] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1030,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007387236822083526,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 5, 8, 6, 8, 4, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 71,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 71, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204766.792286)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204801.048845)
('Worker processing elapsed time: ', 34.25655913352966, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[71]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 71, ']')


'Epoch [72] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1339,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007512226502696623,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 7, 6, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 72,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 72, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204801.054294)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204834.937367)
('Worker processing elapsed time: ', 33.88307285308838, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[72]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 72, ']')


'Epoch [73] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 977,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007010085414218003,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 4, 6, 4, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 73,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 73, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204834.942807)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204870.630592)
('Worker processing elapsed time: ', 35.687785148620605, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[73]', 'EPOCH LOSS:', 0.02341451601337215, 'BEST LOSS:', 0.021974338319052097)
('END OF Optimizer EPOCH =====================>>[', 73, ']')


'Epoch [74] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 689,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000730738747475649,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 4, 8, 6, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 74,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 74, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204870.63542)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204906.007723)
('Worker processing elapsed time: ', 35.3723030090332, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[74]', 'EPOCH LOSS:', 0.019290518209551695, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 74, ']')


'Epoch [75] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 801,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00015099236515354842,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 7, 5, 9, 9, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 75,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 75, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204906.013649)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204940.141737)
('Worker processing elapsed time: ', 34.12808799743652, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[75]', 'EPOCH LOSS:', 0.023613103391150164, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 75, ']')


'Epoch [76] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 562,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008537812046932942,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 4, 5, 5, 8, 7, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 76,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 76, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204940.146754)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494204976.122583)
('Worker processing elapsed time: ', 35.975828886032104, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[76]', 'EPOCH LOSS:', 0.027216348593973948, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 76, ']')


'Epoch [77] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 776,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005351878835487863,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 8, 6, 7, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 77,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 77, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494204976.128066)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205010.125597)
('Worker processing elapsed time: ', 33.997530937194824, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[77]', 'EPOCH LOSS:', 0.024015500443788314, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 77, ']')


'Epoch [78] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 417,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006267030345383239,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 6, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 78,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 78, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205010.131225)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205043.894527)
('Worker processing elapsed time: ', 33.763301849365234, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[78]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 78, ']')


'Epoch [79] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 781,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016363880483312107,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 5, 6, 7, 6, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 79,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 79, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205043.900712)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205077.955682)
('Worker processing elapsed time: ', 34.054970026016235, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[79]', 'EPOCH LOSS:', 0.023907154003712381, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 79, ']')


'Epoch [80] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1995,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004288448775238053,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 8, 6, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 80,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 80, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205077.960745)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205112.2735)
('Worker processing elapsed time: ', 34.31275486946106, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[80]', 'EPOCH LOSS:', 0.65307670920518068, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 80, ']')


'Epoch [81] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 827,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009752007325643707,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 81,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 81, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205112.279321)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205146.092666)
('Worker processing elapsed time: ', 33.813344955444336, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[81]', 'EPOCH LOSS:', 0.30898962621788573, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 81, ']')


'Epoch [82] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1654,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009948978534013685,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 5, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 82,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 82, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205146.098287)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205180.701952)
('Worker processing elapsed time: ', 34.60366487503052, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[82]', 'EPOCH LOSS:', 0.027366585608273705, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 82, ']')


'Epoch [83] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1354,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007362385191087926,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 5, 7, 5, 6, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 83,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 83, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205180.707432)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205214.811271)
('Worker processing elapsed time: ', 34.10383892059326, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[83]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 83, ']')


'Epoch [84] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1017,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005235871751643189,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 4, 9, 8, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 84,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 84, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205214.816482)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205248.843267)
('Worker processing elapsed time: ', 34.026784896850586, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[84]', 'EPOCH LOSS:', 0.024614859021953452, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 84, ']')


'Epoch [85] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 853,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004544003671272924,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 7, 6, 5, 8, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 85,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 85, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205248.849544)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205282.954722)
('Worker processing elapsed time: ', 34.105177879333496, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[85]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 85, ']')


'Epoch [86] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1062,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008008793759669501,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 86,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 86, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205282.959621)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205316.834041)
('Worker processing elapsed time: ', 33.874420166015625, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[86]', 'EPOCH LOSS:', 2.2166285157644907, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 86, ']')


'Epoch [87] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1350,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004970176414205998,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 8, 5, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 87,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 87, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205316.839815)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205351.152957)
('Worker processing elapsed time: ', 34.31314206123352, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[87]', 'EPOCH LOSS:', 7.7173202531814455, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 87, ']')


'Epoch [88] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1196,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004454413199915149,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 9, 6, 4, 6, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 88,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 88, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205351.15812)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205385.273275)
('Worker processing elapsed time: ', 34.11515498161316, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[88]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 88, ']')


'Epoch [89] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 526,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009426479440090448,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 6, 6, 7, 8, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 89,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 89, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205385.278415)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205419.964563)
('Worker processing elapsed time: ', 34.686147928237915, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[89]', 'EPOCH LOSS:', 16.026787395658314, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 89, ']')


'Epoch [90] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 837,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00021346674100276657,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 8, 9, 8, 7, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 90,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 90, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205419.970127)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205455.659747)
('Worker processing elapsed time: ', 35.68961977958679, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[90]', 'EPOCH LOSS:', 0.025674226308496844, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 90, ']')


'Epoch [91] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 625,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016963515704788296,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 7, 7, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 91,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 91, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205455.665964)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205489.535552)
('Worker processing elapsed time: ', 33.869588136672974, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[91]', 'EPOCH LOSS:', 0.025456821237594345, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 91, ']')


'Epoch [92] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 881,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00040591336467311087,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 4, 6, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 92,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 92, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205489.540558)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205523.918205)
('Worker processing elapsed time: ', 34.377646923065186, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[92]', 'EPOCH LOSS:', 55.558358021920618, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 92, ']')


'Epoch [93] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1045,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00047494456677148753,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 5, 7, 6, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 93,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 93, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205523.923652)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205558.003694)
('Worker processing elapsed time: ', 34.080042123794556, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[93]', 'EPOCH LOSS:', 0.025583552581625094, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 93, ']')


'Epoch [94] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 501,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003259592310697163,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 6, 7, 9, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 94,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 94, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205558.009474)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205593.360158)
('Worker processing elapsed time: ', 35.35068392753601, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[94]', 'EPOCH LOSS:', 64.913948414225601, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 94, ']')


'Epoch [95] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 948,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007768347680828815,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 5, 6, 9, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 95,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 95, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205593.365786)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205627.465513)
('Worker processing elapsed time: ', 34.0997269153595, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[95]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 95, ']')


'Epoch [96] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1037,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016362380822035154,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 96,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 96, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205627.471524)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205661.416968)
('Worker processing elapsed time: ', 33.945444107055664, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[96]', 'EPOCH LOSS:', 9.1065784261079177, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 96, ']')


'Epoch [97] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 761,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001723604670966363,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 9, 7, 5, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 97,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 97, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205661.422302)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205695.423832)
('Worker processing elapsed time: ', 34.001529932022095, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[97]', 'EPOCH LOSS:', 0.024362199576412412, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 97, ']')


'Epoch [98] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1099,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000623264204160414,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 98,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 98, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205695.429234)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205729.080097)
('Worker processing elapsed time: ', 33.6508629322052, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[98]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 98, ']')


'Epoch [99] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1252,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005808343132763437,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 7, 6, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 99,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 99, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205729.085854)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205764.136385)
('Worker processing elapsed time: ', 35.05053091049194, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[99]', 'EPOCH LOSS:', 2.6386375347115454, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 99, ']')


'Epoch [100] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1349,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00030643620342168845,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 4, 8, 6, 6, 8, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 100,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 100, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205764.141926)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205799.098549)
('Worker processing elapsed time: ', 34.956622838974, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[100]', 'EPOCH LOSS:', 73.272895364708049, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 100, ']')


'Epoch [101] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 970,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008328535526325045,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 9, 7, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 101,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 101, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205799.104038)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205832.993424)
('Worker processing elapsed time: ', 33.88938593864441, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[101]', 'EPOCH LOSS:', 0.025118526098150414, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 101, ']')


'Epoch [102] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 967,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008924992499433322,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 4, 5, 6, 7, 9, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 102,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 102, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205832.999577)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205868.939005)
('Worker processing elapsed time: ', 35.939427852630615, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[102]', 'EPOCH LOSS:', 0.066345612888742356, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 102, ']')


'Epoch [103] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 352,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00014350318033156198,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 7, 9, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 103,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 103, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205868.945004)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205903.119564)
('Worker processing elapsed time: ', 34.17456007003784, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[103]', 'EPOCH LOSS:', 1484.4724458417559, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 103, ']')


'Epoch [104] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 284,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008918898059105596,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 104,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 104, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205903.124474)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205937.763919)
('Worker processing elapsed time: ', 34.639445066452026, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[104]', 'EPOCH LOSS:', 0.027788297223245478, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 104, ']')


'Epoch [105] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 767,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009854879256862954,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 9, 4, 5, 7, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 105,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 105, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205937.770073)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494205972.489873)
('Worker processing elapsed time: ', 34.71979999542236, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[105]', 'EPOCH LOSS:', 0.034097520201706352, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 105, ']')


'Epoch [106] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 348,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006113728231676354,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 5, 9, 9, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 106,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 106, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494205972.494883)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206006.444013)
('Worker processing elapsed time: ', 33.949130058288574, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[106]', 'EPOCH LOSS:', 0.021588731592024221, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 106, ']')


'Epoch [107] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 892,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002081948358400394,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 7, 7, 6, 7, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 107,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 107, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206006.449367)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206040.632282)
('Worker processing elapsed time: ', 34.1829149723053, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[107]', 'EPOCH LOSS:', 0.023611516283363811, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 107, ']')


'Epoch [108] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 139,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005935568974935828,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 108,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 108, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206040.638133)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206074.258493)
('Worker processing elapsed time: ', 33.620359897613525, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[108]', 'EPOCH LOSS:', 0.023664091335648851, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 108, ']')


'Epoch [109] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1236,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008294176263605681,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 6, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 109,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 109, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206074.264077)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206108.012189)
('Worker processing elapsed time: ', 33.748111963272095, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[109]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 109, ']')


'Epoch [110] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1361,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00010936525453951582,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 8, 9, 7, 9, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 110,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 110, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206108.017884)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206142.907831)
('Worker processing elapsed time: ', 34.889946937561035, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[110]', 'EPOCH LOSS:', 8995.6369294527867, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 110, ']')


'Epoch [111] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 182,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003898927316917356,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 5, 8, 4, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 111,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 111, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206142.913859)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206178.180455)
('Worker processing elapsed time: ', 35.26659607887268, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[111]', 'EPOCH LOSS:', 46.063861876826373, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 111, ']')


'Epoch [112] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 709,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005235925571361395,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 5, 8, 9, 9, 5, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 112,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 112, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206178.185521)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206213.066824)
('Worker processing elapsed time: ', 34.88130307197571, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[112]', 'EPOCH LOSS:', 1.7732758876245145, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 112, ']')


'Epoch [113] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 569,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006858179986449431,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 5, 6, 5, 8, 4, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 113,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 113, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206213.071993)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206248.054782)
('Worker processing elapsed time: ', 34.98278880119324, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[113]', 'EPOCH LOSS:', 0.14847249506024823, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 113, ']')


'Epoch [114] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 366,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007151529122712971,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 9, 7, 9, 4, 9, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 114,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 114, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206248.061042)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206282.363129)
('Worker processing elapsed time: ', 34.30208683013916, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[114]', 'EPOCH LOSS:', 0.022122122243524518, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 114, ']')


'Epoch [115] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 803,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003960728802398129,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 115,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 115, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206282.369269)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206315.964334)
('Worker processing elapsed time: ', 33.595065116882324, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[115]', 'EPOCH LOSS:', 0.024128552846925482, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 115, ']')


'Epoch [116] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1805,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002041118401150319,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 9, 4, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 116,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 116, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206315.970093)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206350.909314)
('Worker processing elapsed time: ', 34.939220905303955, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[116]', 'EPOCH LOSS:', 20.60559754549833, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 116, ']')


'Epoch [117] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 345,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00013985694594564392,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 5, 5, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 117,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 117, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206350.914349)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206384.776611)
('Worker processing elapsed time: ', 33.86226201057434, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[117]', 'EPOCH LOSS:', 0.024101883529417951, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 117, ']')


'Epoch [118] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1209,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023813595243478414,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 6, 5, 6, 7, 7, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 118,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 118, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206384.781934)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206419.722235)
('Worker processing elapsed time: ', 34.940300941467285, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[118]', 'EPOCH LOSS:', 81.17274612523147, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 118, ']')


'Epoch [119] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1209,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00054552346497345,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 4, 6, 8, 4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 119,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 119, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206419.7273)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206454.39646)
('Worker processing elapsed time: ', 34.66916012763977, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[119]', 'EPOCH LOSS:', 1.6391758807893313, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 119, ']')


'Epoch [120] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 788,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003431748005334773,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 9, 4, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 120,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 120, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206454.401404)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206489.450454)
('Worker processing elapsed time: ', 35.049050092697144, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[120]', 'EPOCH LOSS:', 0.084722202540814834, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 120, ']')


'Epoch [121] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1458,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006086850390588207,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 121,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 121, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206489.455743)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206523.286062)
('Worker processing elapsed time: ', 33.83031892776489, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[121]', 'EPOCH LOSS:', 0.8734332257786267, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 121, ']')


'Epoch [122] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1380,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00048721925780506495,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 122,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 122, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206523.292041)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206557.661216)
('Worker processing elapsed time: ', 34.36917495727539, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[122]', 'EPOCH LOSS:', 0.063829197118835848, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 122, ']')


'Epoch [123] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1311,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007583587625793462,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 8, 7, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 123,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 123, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206557.667414)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206591.95313)
('Worker processing elapsed time: ', 34.28571605682373, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[123]', 'EPOCH LOSS:', 4914.1809102364177, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 123, ']')


'Epoch [124] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1498,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008787269967482302,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 7, 8, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 124,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 124, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206591.958696)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206625.802375)
('Worker processing elapsed time: ', 33.84367918968201, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[124]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 124, ']')


'Epoch [125] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 758,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006751504882719383,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 5, 7, 5, 9, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 125,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 125, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206625.80751)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206661.42652)
('Worker processing elapsed time: ', 35.61901021003723, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[125]', 'EPOCH LOSS:', 3.3816043824270294, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 125, ']')


'Epoch [126] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 545,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006628515775729658,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 7, 6, 6, 5, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 126,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 126, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206661.431577)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206696.985678)
('Worker processing elapsed time: ', 35.55410099029541, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[126]', 'EPOCH LOSS:', 0.060429472298802051, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 126, ']')


'Epoch [127] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1665,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00014550301095231344,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 127,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 127, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206696.991684)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206730.553375)
('Worker processing elapsed time: ', 33.56169104576111, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[127]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 127, ']')


'Epoch [128] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1217,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007498431399065035,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 4, 7, 8, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 128,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 128, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206730.55941)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206764.997266)
('Worker processing elapsed time: ', 34.4378559589386, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[128]', 'EPOCH LOSS:', 0.26750647695843394, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 128, ']')


'Epoch [129] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 492,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00015928881451578343,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 129,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 129, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206765.002347)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206798.83066)
('Worker processing elapsed time: ', 33.82831311225891, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[129]', 'EPOCH LOSS:', 1013.7196570154481, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 129, ']')


'Epoch [130] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 713,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00039968607776943077,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 4, 7, 9, 6, 4, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 130,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 130, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206798.835869)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206833.137064)
('Worker processing elapsed time: ', 34.30119490623474, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[130]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 130, ']')


'Epoch [131] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1028,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009032934425131063,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 7, 9, 5, 8, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 131,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 131, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206833.142277)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206868.754856)
('Worker processing elapsed time: ', 35.612579107284546, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[131]', 'EPOCH LOSS:', 0.27736062892223246, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 131, ']')


'Epoch [132] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1303,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008882752831495822,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 132,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 132, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206868.760132)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206902.35984)
('Worker processing elapsed time: ', 33.59970784187317, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[132]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 132, ']')


'Epoch [133] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1469,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006432688503431464,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 133,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 133, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206902.365359)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206936.210884)
('Worker processing elapsed time: ', 33.84552502632141, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[133]', 'EPOCH LOSS:', 6.6163585148996438, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 133, ']')


'Epoch [134] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 865,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005027631493768097,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 134,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 134, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206936.21688)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494206970.670604)
('Worker processing elapsed time: ', 34.4537239074707, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[134]', 'EPOCH LOSS:', 0.26084570807974872, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 134, ']')


'Epoch [135] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1374,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003405086252265833,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 9, 8, 8, 5, 6, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 135,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 135, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494206970.676012)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207005.601344)
('Worker processing elapsed time: ', 34.92533206939697, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[135]', 'EPOCH LOSS:', 1153.2775300497108, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 135, ']')


'Epoch [136] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1246,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000345007764804614,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 8, 7, 4, 9, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 136,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 136, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207005.607142)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207039.728933)
('Worker processing elapsed time: ', 34.12179112434387, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[136]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 136, ']')


'Epoch [137] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1478,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006057251047693969,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 9, 7, 8, 7, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 137,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 137, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207039.734276)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207073.857927)
('Worker processing elapsed time: ', 34.12365102767944, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[137]', 'EPOCH LOSS:', 0.024625739161099988, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 137, ']')


'Epoch [138] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 818,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006697448881176739,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 8, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 138,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 138, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207073.86351)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207107.905859)
('Worker processing elapsed time: ', 34.042349100112915, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[138]', 'EPOCH LOSS:', 2.4496327302487049, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 138, ']')


'Epoch [139] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 456,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007093979096866302,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 5, 8, 4, 7, 8, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 139,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 139, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207107.911378)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207142.196713)
('Worker processing elapsed time: ', 34.285335063934326, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[139]', 'EPOCH LOSS:', 0.024072532002242613, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 139, ']')


'Epoch [140] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 179,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00042739250873795255,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 9, 5, 9, 7, 4, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 140,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 140, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207142.20216)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207177.084654)
('Worker processing elapsed time: ', 34.8824942111969, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[140]', 'EPOCH LOSS:', 0.11038621625366528, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 140, ']')


'Epoch [141] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1640,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009468517332796002,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 8, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 141,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 141, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207177.090253)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207211.803264)
('Worker processing elapsed time: ', 34.71301078796387, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[141]', 'EPOCH LOSS:', 1.022477188297358, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 141, ']')


'Epoch [142] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1452,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00014650162910331463,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 9, 7, 4, 9, 9, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 142,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 142, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207211.808546)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207246.764302)
('Worker processing elapsed time: ', 34.955755949020386, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[142]', 'EPOCH LOSS:', 0.66539079450731675, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 142, ']')


'Epoch [143] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1287,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008278644735376764,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 5, 6, 5, 6, 8, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 143,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 143, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207246.770298)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207282.721494)
('Worker processing elapsed time: ', 35.95119595527649, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[143]', 'EPOCH LOSS:', 0.083454418827671059, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 143, ']')


'Epoch [144] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1859,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000528026303091282,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 7, 9, 7, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 144,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 144, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207282.726438)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207317.262913)
('Worker processing elapsed time: ', 34.53647494316101, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[144]', 'EPOCH LOSS:', 0.57572798512220646, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 144, ']')


'Epoch [145] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 780,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00010354735983284283,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 145,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 145, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207317.267711)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207351.601775)
('Worker processing elapsed time: ', 34.33406400680542, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[145]', 'EPOCH LOSS:', 0.94816634281469614, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 145, ']')


'Epoch [146] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 699,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005645220335609327,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 5, 5, 8, 4, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 146,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 146, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207351.606877)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207386.235396)
('Worker processing elapsed time: ', 34.62851881980896, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[146]', 'EPOCH LOSS:', 0.67162418717467653, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 146, ']')


'Epoch [147] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1943,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004931146306580973,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 6, 6, 7, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 147,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 147, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207386.240394)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207420.199985)
('Worker processing elapsed time: ', 33.959590911865234, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[147]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 147, ']')


'Epoch [148] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1526,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023192110508334952,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 6, 9, 9, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 148,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 148, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207420.204693)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207454.164814)
('Worker processing elapsed time: ', 33.96012091636658, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[148]', 'EPOCH LOSS:', 0.024345812177452405, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 148, ']')


'Epoch [149] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 409,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005021049501114171,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 5, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 149,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 149, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207454.170117)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207488.843553)
('Worker processing elapsed time: ', 34.67343616485596, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[149]', 'EPOCH LOSS:', 0.085467501515431019, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 149, ']')


'Epoch [150] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1758,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005619835344315377,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 6, 6, 7, 8, 6, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 150,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 150, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207488.849653)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207523.104192)
('Worker processing elapsed time: ', 34.254539012908936, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[150]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 150, ']')


'Epoch [151] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 466,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005306619794035197,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 7, 6, 6, 4, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 151,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 151, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207523.10979)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207557.235047)
('Worker processing elapsed time: ', 34.12525701522827, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[151]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 151, ']')


'Epoch [152] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1953,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006544994528200198,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 152,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 152, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207557.241073)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207590.78191)
('Worker processing elapsed time: ', 33.54083704948425, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[152]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 152, ']')


'Epoch [153] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 945,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007810728195200159,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 4, 7, 7, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 153,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 153, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207590.789258)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207626.080734)
('Worker processing elapsed time: ', 35.291476011276245, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[153]', 'EPOCH LOSS:', 0.039900764837995262, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 153, ']')


'Epoch [154] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 603,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005435187135583398,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 8, 4, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 154,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 154, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207626.087186)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207659.943447)
('Worker processing elapsed time: ', 33.856261014938354, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[154]', 'EPOCH LOSS:', 0.02597243708211016, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 154, ']')


'Epoch [155] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 315,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004279056588406765,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 155,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 155, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207659.94857)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207694.340715)
('Worker processing elapsed time: ', 34.39214491844177, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[155]', 'EPOCH LOSS:', 0.02661544438215268, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 155, ']')


'Epoch [156] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 808,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00044711155136017543,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 4, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 156,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 156, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207694.346709)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207728.041249)
('Worker processing elapsed time: ', 33.69454002380371, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[156]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 156, ']')


'Epoch [157] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 820,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004462819247118737,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 6, 4, 5, 8, 9, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 157,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 157, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207728.047047)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207763.014773)
('Worker processing elapsed time: ', 34.96772599220276, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[157]', 'EPOCH LOSS:', 0.27997270338276686, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 157, ']')


'Epoch [158] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 358,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007959133155046074,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 158,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 158, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207763.019902)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207796.555667)
('Worker processing elapsed time: ', 33.535764932632446, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[158]', 'EPOCH LOSS:', 0.023367975053739616, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 158, ']')


'Epoch [159] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1492,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006860505668301211,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 5, 8, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 159,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 159, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207796.561909)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207831.516681)
('Worker processing elapsed time: ', 34.954771995544434, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[159]', 'EPOCH LOSS:', 0.35609655041671379, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 159, ']')


'Epoch [160] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 745,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003632692226480128,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 8, 5, 6, 6, 8, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 160,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 160, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207831.522578)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207865.735406)
('Worker processing elapsed time: ', 34.212827920913696, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[160]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 160, ']')


'Epoch [161] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1359,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006094518346430136,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 4, 8, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 161,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 161, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207865.740717)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207899.567445)
('Worker processing elapsed time: ', 33.826728105545044, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[161]', 'EPOCH LOSS:', 0.024698769724454433, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 161, ']')


'Epoch [162] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 524,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00036109723881004086,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 9, 4, 7, 5, 9, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 162,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 162, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207899.572646)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207935.540653)
('Worker processing elapsed time: ', 35.96800708770752, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[162]', 'EPOCH LOSS:', 1.570788940416425, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 162, ']')


'Epoch [163] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1100,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007024998653894118,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 7, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 163,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 163, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207935.545883)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494207970.228802)
('Worker processing elapsed time: ', 34.68291902542114, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[163]', 'EPOCH LOSS:', 0.051429569623676043, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 163, ']')


'Epoch [164] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1066,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002808406232039939,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 6, 4, 6, 7, 7, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 164,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 164, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494207970.23416)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208006.088317)
('Worker processing elapsed time: ', 35.85415697097778, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[164]', 'EPOCH LOSS:', 0.039095727305561656, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 164, ']')


'Epoch [165] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1811,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009847636780606133,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 165,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 165, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208006.093954)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208039.958618)
('Worker processing elapsed time: ', 33.86466383934021, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[165]', 'EPOCH LOSS:', 0.17121973549242606, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 165, ']')


'Epoch [166] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 953,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005405048179461483,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 7, 7, 8, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 166,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 166, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208039.964995)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208075.611693)
('Worker processing elapsed time: ', 35.646697998046875, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[166]', 'EPOCH LOSS:', 0.036570397734699708, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 166, ']')


'Epoch [167] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1967,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004734759234692347,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 5, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 167,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 167, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208075.617276)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208109.282199)
('Worker processing elapsed time: ', 33.66492295265198, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[167]', 'EPOCH LOSS:', 0.023564691375465165, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 167, ']')


'Epoch [168] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 290,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009989632931679162,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 8, 8, 6, 4, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 168,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 168, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208109.287745)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208144.022478)
('Worker processing elapsed time: ', 34.73473310470581, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[168]', 'EPOCH LOSS:', 0.057380276889946215, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 168, ']')


'Epoch [169] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1683,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000210874702175895,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 169,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 169, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208144.027878)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208177.536351)
('Worker processing elapsed time: ', 33.50847291946411, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[169]', 'EPOCH LOSS:', 0.023760938332183363, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 169, ']')


'Epoch [170] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 331,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016916395709940869,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 5, 7, 6, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 170,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 170, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208177.541807)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208212.837592)
('Worker processing elapsed time: ', 35.29578495025635, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[170]', 'EPOCH LOSS:', 11.664415309804435, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 170, ']')


'Epoch [171] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 301,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007340795898751903,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 9, 6, 4, 8, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 171,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 171, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208212.842884)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208246.905627)
('Worker processing elapsed time: ', 34.062742948532104, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[171]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 171, ']')


'Epoch [172] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1689,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005258263243423389,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 5, 8, 6, 7, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 172,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 172, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208246.911438)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208281.62867)
('Worker processing elapsed time: ', 34.71723198890686, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[172]', 'EPOCH LOSS:', 0.96264535124572859, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 172, ']')


'Epoch [173] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1439,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003740130269202787,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 173,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 173, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208281.634276)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208316.053054)
('Worker processing elapsed time: ', 34.41877818107605, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[173]', 'EPOCH LOSS:', 0.066951216238089847, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 173, ']')


'Epoch [174] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1644,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00036173330483497707,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 6, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 174,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 174, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208316.058506)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208350.131916)
('Worker processing elapsed time: ', 34.07341003417969, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[174]', 'EPOCH LOSS:', 750.02322157757021, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 174, ']')


'Epoch [175] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 387,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00011081194608484181,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 175,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 175, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208350.137088)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208383.67367)
('Worker processing elapsed time: ', 33.53658199310303, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[175]', 'EPOCH LOSS:', 0.023628392009070107, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 175, ']')


'Epoch [176] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 631,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023307052438813224,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 6, 6, 9, 8, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 176,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 176, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208383.678956)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208419.320258)
('Worker processing elapsed time: ', 35.64130187034607, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[176]', 'EPOCH LOSS:', 0.055783853638308373, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 176, ']')


'Epoch [177] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 406,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005379159491113639,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 5, 9, 6, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 177,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 177, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208419.326266)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208454.698529)
('Worker processing elapsed time: ', 35.372262954711914, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[177]', 'EPOCH LOSS:', 0.02640700869629212, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 177, ']')


'Epoch [178] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1340,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005879740824284402,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 4, 9, 7, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 178,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 178, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208454.703666)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208489.136571)
('Worker processing elapsed time: ', 34.432904958724976, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[178]', 'EPOCH LOSS:', 0.22090181317235089, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 178, ']')


'Epoch [179] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 857,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007653840122374619,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 9, 6, 4, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 179,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 179, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208489.142387)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208524.409406)
('Worker processing elapsed time: ', 35.26701903343201, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[179]', 'EPOCH LOSS:', 0.063795842126974645, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 179, ']')


'Epoch [180] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1049,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009181282977144987,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 180,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 180, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208524.415171)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208558.279424)
('Worker processing elapsed time: ', 33.86425304412842, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[180]', 'EPOCH LOSS:', 13.412290454016855, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 180, ']')


'Epoch [181] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 198,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006067220990827834,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 8, 9, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 181,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 181, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208558.284899)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208592.533149)
('Worker processing elapsed time: ', 34.248250007629395, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[181]', 'EPOCH LOSS:', 2.2651324674388236, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 181, ']')


'Epoch [182] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1154,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001979427785979151,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 7, 8, 9, 9, 7, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 182,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 182, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208592.538704)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208626.720636)
('Worker processing elapsed time: ', 34.18193197250366, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[182]', 'EPOCH LOSS:', 0.025779607481579852, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 182, ']')


'Epoch [183] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 618,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00047681102294093165,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 4, 8, 4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 183,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 183, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208626.726729)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208660.647075)
('Worker processing elapsed time: ', 33.92034602165222, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[183]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 183, ']')


'Epoch [184] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 856,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005902106829454842,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 6, 9, 9, 9, 7, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 184,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 184, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208660.65251)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208694.928265)
('Worker processing elapsed time: ', 34.275755167007446, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[184]', 'EPOCH LOSS:', 0.024759592974962021, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 184, ']')


'Epoch [185] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1496,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00024713571217479114,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 4, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 185,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 185, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208694.934381)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208729.022536)
('Worker processing elapsed time: ', 34.088155031204224, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[185]', 'EPOCH LOSS:', 4.8527793261168686, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 185, ']')


'Epoch [186] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1647,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004642566662846544,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 4, 8, 8, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 186,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 186, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208729.028443)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208762.998832)
('Worker processing elapsed time: ', 33.970388889312744, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[186]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 186, ']')


'Epoch [187] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 825,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00045005348108022444,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 7, 5, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 187,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 187, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208763.005249)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208797.249683)
('Worker processing elapsed time: ', 34.244433879852295, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[187]', 'EPOCH LOSS:', 1327.9613144884281, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 187, ']')


'Epoch [188] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 155,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00021590633480316845,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 8, 8, 6, 8, 5, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 188,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 188, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208797.255174)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208832.202203)
('Worker processing elapsed time: ', 34.94702911376953, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[188]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 188, ']')


'Epoch [189] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 371,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008851954709695634,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 189,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 189, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208832.207571)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208865.814264)
('Worker processing elapsed time: ', 33.60669302940369, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[189]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 189, ']')


'Epoch [190] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1373,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002537252084777291,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 4, 7, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 190,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 190, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208865.820007)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208899.659541)
('Worker processing elapsed time: ', 33.83953380584717, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[190]', 'EPOCH LOSS:', 0.024511465308652523, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 190, ']')


'Epoch [191] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1677,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00046160784862346973,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 8, 5, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 191,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 191, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208899.665088)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208934.647901)
('Worker processing elapsed time: ', 34.982813119888306, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[191]', 'EPOCH LOSS:', 0.053332422358958326, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 191, ']')


'Epoch [192] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1590,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00017531115316858571,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 4, 5, 4, 7, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 192,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 192, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208934.654062)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494208970.349104)
('Worker processing elapsed time: ', 35.69504189491272, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[192]', 'EPOCH LOSS:', 0.032387235245910473, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 192, ']')


'Epoch [193] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1950,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005101310040906816,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 5, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 193,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 193, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494208970.355813)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209004.388471)
('Worker processing elapsed time: ', 34.032657861709595, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[193]', 'EPOCH LOSS:', 6.9147640502250534, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 193, ']')


'Epoch [194] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1145,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00014711457766166454,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 6, 7, 6, 7, 8, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 194,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 194, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209004.393833)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209040.33728)
('Worker processing elapsed time: ', 35.94344711303711, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[194]', 'EPOCH LOSS:', 0.13311144558448806, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 194, ']')


'Epoch [195] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1155,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007344449482146054,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 4, 4, 5, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 195,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 195, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209040.342962)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209074.275262)
('Worker processing elapsed time: ', 33.932300090789795, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[195]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 195, ']')


'Epoch [196] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1746,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00044013480154390765,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 5, 6, 8, 6, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 196,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 196, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209074.281861)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209109.951322)
('Worker processing elapsed time: ', 35.6694610118866, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[196]', 'EPOCH LOSS:', 4.4570308496185342, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 196, ']')


'Epoch [197] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 651,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006380147946165815,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 9, 4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 197,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 197, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209109.956382)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209143.787258)
('Worker processing elapsed time: ', 33.830875873565674, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[197]', 'EPOCH LOSS:', 0.024805621260344756, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 197, ']')


'Epoch [198] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1654,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006527276797366262,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 198,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 198, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209143.792559)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209177.667265)
('Worker processing elapsed time: ', 33.87470602989197, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[198]', 'EPOCH LOSS:', 2.7048488615583866, 'BEST LOSS:', 0.019290518209551695)
('END OF Optimizer EPOCH =====================>>[', 198, ']')


'Epoch [199] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 758,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004512998116255665,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 6, 9, 9, 6, 7, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 199,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 199, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209177.673744)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209213.480095)
('Worker processing elapsed time: ', 35.80635094642639, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[199]', 'EPOCH LOSS:', 0.015198407588926106, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 199, ']')


'Epoch [200] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1480,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000892611486006045,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 4, 7, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 200,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 200, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209213.486903)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209248.483327)
('Worker processing elapsed time: ', 34.996423959732056, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[200]', 'EPOCH LOSS:', 0.067626182882109101, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 200, ']')


'Epoch [201] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 168,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004577903689331732,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 201,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 201, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209248.488822)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209282.742499)
('Worker processing elapsed time: ', 34.25367712974548, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[201]', 'EPOCH LOSS:', 0.033387897759130876, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 201, ']')


'Epoch [202] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 237,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00043013761602605604,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 9, 6, 8, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 202,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 202, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209282.747818)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209316.743171)
('Worker processing elapsed time: ', 33.99535298347473, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[202]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 202, ']')


'Epoch [203] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1927,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007722842060971965,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 5, 8, 8, 7, 5, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 203,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 203, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209316.749385)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209350.9463)
('Worker processing elapsed time: ', 34.19691491127014, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[203]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 203, ']')


'Epoch [204] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1493,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005638034939291949,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 6, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 204,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 204, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209350.951965)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209384.632068)
('Worker processing elapsed time: ', 33.680102825164795, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[204]', 'EPOCH LOSS:', 0.024568437033923283, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 204, ']')


'Epoch [205] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 384,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00010330857114269291,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 7, 8, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 205,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 205, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209384.637976)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209418.518892)
('Worker processing elapsed time: ', 33.880916118621826, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[205]', 'EPOCH LOSS:', 0.022934611719709338, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 205, ']')


'Epoch [206] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 517,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00010671248579506489,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 7, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 206,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 206, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209418.52553)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209453.247356)
('Worker processing elapsed time: ', 34.72182583808899, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[206]', 'EPOCH LOSS:', 0.56531350504268907, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 206, ']')


'Epoch [207] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1613,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006305668980668797,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 6, 8, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 207,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 207, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209453.253272)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209488.3116)
('Worker processing elapsed time: ', 35.0583279132843, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[207]', 'EPOCH LOSS:', 0.031672880655676493, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 207, ']')


'Epoch [208] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 682,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005113006394322422,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 5, 8, 6, 5, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 208,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 208, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209488.317585)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209523.937585)
('Worker processing elapsed time: ', 35.62000012397766, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[208]', 'EPOCH LOSS:', 0.023989347272088658, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 208, ']')


'Epoch [209] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 293,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008815890825742221,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 5, 5, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 209,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 209, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209523.943225)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209558.221949)
('Worker processing elapsed time: ', 34.278724193573, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[209]', 'EPOCH LOSS:', 2.7188814132239201, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 209, ']')


'Epoch [210] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1534,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007511112707730376,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 9, 6, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 210,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 210, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209558.228029)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209593.262096)
('Worker processing elapsed time: ', 35.034066915512085, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[210]', 'EPOCH LOSS:', 0.034636717333572639, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 210, ']')


'Epoch [211] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1705,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00011875093717056287,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 6, 8, 4, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 211,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 211, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209593.267519)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209627.243319)
('Worker processing elapsed time: ', 33.97580003738403, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[211]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 211, ']')


'Epoch [212] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1620,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00014319769337229758,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 8, 6, 9, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 212,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 212, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209627.248773)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209662.60003)
('Worker processing elapsed time: ', 35.35125684738159, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[212]', 'EPOCH LOSS:', 33.008096463843735, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 212, ']')


'Epoch [213] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1122,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004145540682857036,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 213,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 213, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209662.606286)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209696.640986)
('Worker processing elapsed time: ', 34.0346999168396, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[213]', 'EPOCH LOSS:', 29.06671699651497, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 213, ']')


'Epoch [214] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1515,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009945336993622755,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 8, 8, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 214,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 214, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209696.646902)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209730.91619)
('Worker processing elapsed time: ', 34.26928782463074, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[214]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 214, ']')


'Epoch [215] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1899,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00014482442036018047,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 7, 6, 8, 6, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 215,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 215, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209730.92246)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209765.582464)
('Worker processing elapsed time: ', 34.660003900527954, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[215]', 'EPOCH LOSS:', 84.269682854077615, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 215, ']')


'Epoch [216] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 292,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005215973896112109,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 5, 9, 6, 6, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 216,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 216, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209765.588862)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209801.184753)
('Worker processing elapsed time: ', 35.59589099884033, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[216]', 'EPOCH LOSS:', 0.19802017833689986, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 216, ']')


'Epoch [217] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 785,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00020741688246605713,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 4, 4, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 217,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 217, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209801.190308)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209835.162955)
('Worker processing elapsed time: ', 33.972646951675415, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[217]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 217, ']')


'Epoch [218] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1224,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001026409838884001,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 7, 5, 6, 7, 7, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 218,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 218, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209835.168312)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209870.074669)
('Worker processing elapsed time: ', 34.90635681152344, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[218]', 'EPOCH LOSS:', 1.7373706531803472, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 218, ']')


'Epoch [219] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1487,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006300438402266049,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 8, 5, 9, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 219,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 219, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209870.081206)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209905.357801)
('Worker processing elapsed time: ', 35.27659487724304, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[219]', 'EPOCH LOSS:', 0.20002198338611715, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 219, ']')


'Epoch [220] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 849,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003467255656752912,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 220,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 220, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209905.363243)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209938.942173)
('Worker processing elapsed time: ', 33.57892990112305, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[220]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 220, ']')


'Epoch [221] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 364,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023272485873311003,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 8, 9, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 221,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 221, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209938.948589)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494209973.169641)
('Worker processing elapsed time: ', 34.221051931381226, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[221]', 'EPOCH LOSS:', 7.3582951273261061, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 221, ']')


'Epoch [222] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 313,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002545493450360562,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 6, 7, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 222,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 222, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494209973.175023)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210007.063479)
('Worker processing elapsed time: ', 33.888455867767334, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[222]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 222, ']')


'Epoch [223] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1556,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007005172377285015,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 9, 5, 8, 6, 6, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 223,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 223, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210007.069426)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210041.381163)
('Worker processing elapsed time: ', 34.311736822128296, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[223]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 223, ']')


'Epoch [224] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 163,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005007254767101139,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 6, 8, 8, 9, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 224,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 224, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210041.387568)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210076.107578)
('Worker processing elapsed time: ', 34.72001004219055, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[224]', 'EPOCH LOSS:', 0.71743474965793763, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 224, ']')


'Epoch [225] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 227,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008983102286785707,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 8, 9, 5, 5, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 225,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 225, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210076.113803)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210110.250692)
('Worker processing elapsed time: ', 34.13688898086548, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[225]', 'EPOCH LOSS:', 0.025328909400832381, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 225, ']')


'Epoch [226] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1754,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009549824375644974,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 6, 5, 6, 7, 9, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 226,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 226, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210110.25729)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210146.125951)
('Worker processing elapsed time: ', 35.86866116523743, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[226]', 'EPOCH LOSS:', 0.038136497941985523, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 226, ']')


'Epoch [227] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1631,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006604749126615664,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 9, 7, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 227,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 227, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210146.132421)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210180.007442)
('Worker processing elapsed time: ', 33.87502098083496, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[227]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 227, ']')


'Epoch [228] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 373,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005363278032774881,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 6, 6, 9, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 228,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 228, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210180.013088)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210214.036426)
('Worker processing elapsed time: ', 34.023338079452515, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[228]', 'EPOCH LOSS:', 0.022915976427440889, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 228, ']')


'Epoch [229] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1186,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008592995624299325,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 229,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 229, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210214.041796)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210247.577186)
('Worker processing elapsed time: ', 33.5353901386261, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[229]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 229, ']')


'Epoch [230] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 859,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000700644772167789,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 8, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 230,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 230, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210247.582897)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210282.217081)
('Worker processing elapsed time: ', 34.63418412208557, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[230]', 'EPOCH LOSS:', 1.4309742785193758, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 230, ']')


'Epoch [231] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1754,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005494763982680282,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 7, 6, 4, 4, 9, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 231,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 231, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210282.222729)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210318.118186)
('Worker processing elapsed time: ', 35.89545702934265, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[231]', 'EPOCH LOSS:', 22.722488657279271, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 231, ']')


'Epoch [232] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 576,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00037284786321420426,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 232,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 232, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210318.124303)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210352.52553)
('Worker processing elapsed time: ', 34.40122699737549, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[232]', 'EPOCH LOSS:', 0.066971739444558143, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 232, ']')


'Epoch [233] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 245,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023530781778573835,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 6, 5, 8, 9, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 233,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 233, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210352.531302)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210388.198958)
('Worker processing elapsed time: ', 35.66765594482422, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[233]', 'EPOCH LOSS:', 8.2077896917321045, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 233, ']')


'Epoch [234] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1930,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000885117530438653,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 4, 4, 7, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 234,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 234, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210388.20508)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210422.725942)
('Worker processing elapsed time: ', 34.520861864089966, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[234]', 'EPOCH LOSS:', 614.58630738231011, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 234, ']')


'Epoch [235] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1689,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008757400431806035,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 7, 6, 9, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 235,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 235, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210422.731802)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210456.834201)
('Worker processing elapsed time: ', 34.10239911079407, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[235]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 235, ']')


'Epoch [236] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1346,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007559624854690945,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 5, 7, 4, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 236,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 236, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210456.840421)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210491.278759)
('Worker processing elapsed time: ', 34.43833804130554, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[236]', 'EPOCH LOSS:', 2469.1484924264264, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 236, ']')


'Epoch [237] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1572,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005496129988888966,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 7, 9, 4, 7, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 237,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 237, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210491.283968)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210525.937224)
('Worker processing elapsed time: ', 34.65325593948364, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[237]', 'EPOCH LOSS:', 196.22915945087331, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 237, ']')


'Epoch [238] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1208,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006280793076582738,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 8, 8, 8, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 238,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 238, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210525.942601)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210561.290359)
('Worker processing elapsed time: ', 35.347758054733276, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[238]', 'EPOCH LOSS:', 1.9686172579573908, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 238, ']')


'Epoch [239] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 923,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009656737967723543,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 8, 8, 6, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 239,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 239, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210561.296637)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210596.72265)
('Worker processing elapsed time: ', 35.42601299285889, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[239]', 'EPOCH LOSS:', 0.023753993371215781, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 239, ']')


'Epoch [240] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 142,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00014735323976356374,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 5, 6, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 240,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 240, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210596.728711)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210630.538525)
('Worker processing elapsed time: ', 33.80981421470642, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[240]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 240, ']')


'Epoch [241] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1442,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00010692518840441578,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 6, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 241,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 241, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210630.544091)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210664.641006)
('Worker processing elapsed time: ', 34.09691500663757, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[241]', 'EPOCH LOSS:', 209.53094844832614, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 241, ']')


'Epoch [242] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1880,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023540802009940933,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 9, 9, 4, 6, 8, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 242,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 242, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210664.646592)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210699.592631)
('Worker processing elapsed time: ', 34.9460391998291, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[242]', 'EPOCH LOSS:', 1.8912138012971798, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 242, ']')


'Epoch [243] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1675,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008127784754489934,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 9, 8, 8, 6, 9, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 243,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 243, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210699.597872)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210733.887309)
('Worker processing elapsed time: ', 34.28943705558777, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[243]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 243, ']')


'Epoch [244] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1431,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008990219704656513,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 6, 6, 5, 8, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 244,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 244, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210733.893638)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210768.499813)
('Worker processing elapsed time: ', 34.60617518424988, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[244]', 'EPOCH LOSS:', 0.1327886038366764, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 244, ']')


'Epoch [245] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 763,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005581675713694703,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 9, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 245,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 245, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210768.50582)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210803.135188)
('Worker processing elapsed time: ', 34.62936806678772, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[245]', 'EPOCH LOSS:', 0.031451177349412392, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 245, ']')


'Epoch [246] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 409,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023879976568800902,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 7, 7, 4, 8, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 246,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 246, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210803.141201)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210837.754276)
('Worker processing elapsed time: ', 34.61307501792908, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[246]', 'EPOCH LOSS:', 3.9383247027939179, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 246, ']')


'Epoch [247] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 294,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004585642105910156,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 247,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 247, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210837.759716)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210871.341694)
('Worker processing elapsed time: ', 33.58197808265686, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[247]', 'EPOCH LOSS:', 0.030044009729928947, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 247, ']')


'Epoch [248] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1937,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000359167551471387,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 4, 9, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 248,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 248, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210871.347384)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210906.352585)
('Worker processing elapsed time: ', 35.0052011013031, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[248]', 'EPOCH LOSS:', 89.138362972601769, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 248, ']')


'Epoch [249] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 947,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006621785154876891,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 6, 7, 4, 4, 7, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 249,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 249, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210906.358226)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210940.612183)
('Worker processing elapsed time: ', 34.25395703315735, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[249]', 'EPOCH LOSS:', 0.025497730695757372, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 249, ']')


'Epoch [250] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1136,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008621525365115001,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 4, 9, 7, 4, 7, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 250,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 250, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210940.618457)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494210975.68436)
('Worker processing elapsed time: ', 35.06590294837952, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[250]', 'EPOCH LOSS:', 8.247243638823285, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 250, ']')


'Epoch [251] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 310,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008547604556789068,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 8, 7, 9, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 251,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 251, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494210975.689971)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211010.973325)
('Worker processing elapsed time: ', 35.28335404396057, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[251]', 'EPOCH LOSS:', 1.0901459640751918, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 251, ']')


'Epoch [252] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 723,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016931971166462385,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 6, 4, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 252,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 252, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211010.979514)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211044.871737)
('Worker processing elapsed time: ', 33.89222311973572, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[252]', 'EPOCH LOSS:', 0.023803716379690105, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 252, ']')


'Epoch [253] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 181,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00019662354432024734,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 6, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 253,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 253, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211044.87748)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211078.619211)
('Worker processing elapsed time: ', 33.74173092842102, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[253]', 'EPOCH LOSS:', 0.024157442330814429, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 253, ']')


'Epoch [254] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 491,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004123344406508232,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 5, 7, 9, 9, 8, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 254,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 254, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211078.62467)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211113.563415)
('Worker processing elapsed time: ', 34.93874502182007, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[254]', 'EPOCH LOSS:', 2142.598690131269, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 254, ']')


'Epoch [255] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 818,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00030563036608947773,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 6, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 255,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 255, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211113.569631)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211148.232584)
('Worker processing elapsed time: ', 34.66295289993286, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[255]', 'EPOCH LOSS:', 1.7738288316388189, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 255, ']')


'Epoch [256] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1845,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005536438215660261,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 4, 6, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 256,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 256, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211148.237829)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211182.528526)
('Worker processing elapsed time: ', 34.29069709777832, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[256]', 'EPOCH LOSS:', 896.59245544864939, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 256, ']')


'Epoch [257] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 617,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00048421901207396437,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 8, 9, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 257,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 257, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211182.533982)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211217.580118)
('Worker processing elapsed time: ', 35.046135902404785, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[257]', 'EPOCH LOSS:', 31.84263716834657, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 257, ']')


'Epoch [258] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 769,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008465674839132062,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 258,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 258, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211217.586335)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211251.54842)
('Worker processing elapsed time: ', 33.962085008621216, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[258]', 'EPOCH LOSS:', 288.3558845008609, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 258, ']')


'Epoch [259] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 243,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009873368733517407,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 8, 7, 7, 5, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 259,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 259, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211251.554115)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211285.604716)
('Worker processing elapsed time: ', 34.0506010055542, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[259]', 'EPOCH LOSS:', 0.025204512120522781, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 259, ']')


'Epoch [260] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1748,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006852643367701416,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 260,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 260, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211285.610472)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211319.995309)
('Worker processing elapsed time: ', 34.38483715057373, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[260]', 'EPOCH LOSS:', 0.35298469588409642, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 260, ']')


'Epoch [261] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 994,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009966691449744436,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 5, 7, 9, 4, 9, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 261,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 261, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211320.001625)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211355.052122)
('Worker processing elapsed time: ', 35.05049705505371, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[261]', 'EPOCH LOSS:', 0.67194609077580891, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 261, ']')


'Epoch [262] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 701,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023669406370464819,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 8, 7, 4, 9, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 262,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 262, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211355.058468)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211389.761959)
('Worker processing elapsed time: ', 34.70349097251892, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[262]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 262, ']')


'Epoch [263] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 627,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00021802880416567285,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 9, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 263,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 263, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211389.767929)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211423.585386)
('Worker processing elapsed time: ', 33.8174569606781, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[263]', 'EPOCH LOSS:', 0.025263519423335951, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 263, ']')


'Epoch [264] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1338,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006653053848854138,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 264,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 264, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211423.590744)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211457.940993)
('Worker processing elapsed time: ', 34.35024905204773, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[264]', 'EPOCH LOSS:', 0.040665302660221073, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 264, ']')


'Epoch [265] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1022,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00044615573747784176,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 5, 7, 7, 6, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 265,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 265, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211457.947334)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211492.904563)
('Worker processing elapsed time: ', 34.957228899002075, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[265]', 'EPOCH LOSS:', 3933.4912371197202, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 265, ']')


'Epoch [266] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1903,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006381026685258341,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 6, 4, 5, 7, 7, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 266,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 266, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211492.910783)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211527.292591)
('Worker processing elapsed time: ', 34.381808042526245, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[266]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 266, ']')


'Epoch [267] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 686,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008430285424988347,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 9, 6, 4, 7, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 267,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 267, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211527.29819)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211561.981684)
('Worker processing elapsed time: ', 34.683493852615356, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[267]', 'EPOCH LOSS:', 3.6624677617288861, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 267, ']')


'Epoch [268] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1065,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00042784811641178623,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 268,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 268, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211561.987147)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211596.334776)
('Worker processing elapsed time: ', 34.3476288318634, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[268]', 'EPOCH LOSS:', 0.073064147603749879, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 268, ']')


'Epoch [269] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1629,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00045433139918249466,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 8, 6, 9, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 269,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 269, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211596.340927)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211630.320821)
('Worker processing elapsed time: ', 33.979894161224365, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[269]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 269, ']')


'Epoch [270] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 888,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007052115243545662,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 270,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 270, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211630.32713)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211663.839931)
('Worker processing elapsed time: ', 33.51280093193054, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[270]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 270, ']')


'Epoch [271] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 577,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000864240127660376,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 8, 7, 7, 9, 7, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 271,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 271, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211663.846202)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211699.862717)
('Worker processing elapsed time: ', 36.016515016555786, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[271]', 'EPOCH LOSS:', 1.343874846123196, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 271, ']')


'Epoch [272] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1942,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009357098206760663,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 5, 8, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 272,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 272, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211699.869166)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211734.089527)
('Worker processing elapsed time: ', 34.22036099433899, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[272]', 'EPOCH LOSS:', 14.023226897517331, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 272, ']')


'Epoch [273] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 359,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008691845566131544,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 5, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 273,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 273, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211734.095891)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211768.792549)
('Worker processing elapsed time: ', 34.69665789604187, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[273]', 'EPOCH LOSS:', 0.036732677830188805, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 273, ']')


'Epoch [274] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 719,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00046279839689094757,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 9, 7, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 274,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 274, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211768.798033)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211803.069841)
('Worker processing elapsed time: ', 34.27180790901184, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[274]', 'EPOCH LOSS:', 4.9094187420363316, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 274, ']')


'Epoch [275] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1686,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00035705032692343584,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 9, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 275,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 275, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211803.075644)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211836.792755)
('Worker processing elapsed time: ', 33.71711087226868, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[275]', 'EPOCH LOSS:', 0.023727897962034904, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 275, ']')


'Epoch [276] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1173,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008323126768744872,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 9, 6, 4, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 276,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 276, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211836.798781)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211870.80599)
('Worker processing elapsed time: ', 34.007209062576294, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[276]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 276, ']')


'Epoch [277] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 801,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006895950422180193,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 5, 6, 4, 7, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 277,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 277, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211870.811749)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211904.99553)
('Worker processing elapsed time: ', 34.183780908584595, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[277]', 'EPOCH LOSS:', 0.024168249885845938, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 277, ']')


'Epoch [278] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1300,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007069800610083679,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 5, 5, 9, 5, 5, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 278,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 278, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211905.002142)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211939.94291)
('Worker processing elapsed time: ', 34.940768003463745, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[278]', 'EPOCH LOSS:', 0.59964645380729742, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 278, ']')


'Epoch [279] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 255,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00030890105297522424,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 6, 6, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 279,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 279, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211939.94948)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494211974.303656)
('Worker processing elapsed time: ', 34.35417604446411, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[279]', 'EPOCH LOSS:', 8.831234303813579, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 279, ']')


'Epoch [280] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 250,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007853590144013523,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 280,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 280, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494211974.309103)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212008.751211)
('Worker processing elapsed time: ', 34.442107915878296, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[280]', 'EPOCH LOSS:', 0.032939995695388156, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 280, ']')


'Epoch [281] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 456,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008183914584718997,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 4, 7, 9, 4, 8, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 281,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 281, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212008.757282)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212043.702578)
('Worker processing elapsed time: ', 34.94529604911804, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[281]', 'EPOCH LOSS:', 0.024057286017467126, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 281, ']')


'Epoch [282] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 828,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004018123871754885,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 5, 9, 7, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 282,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 282, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212043.709053)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212078.201368)
('Worker processing elapsed time: ', 34.49231505393982, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[282]', 'EPOCH LOSS:', 36.647681854557703, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 282, ']')


'Epoch [283] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1868,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006326633197044839,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 4, 8, 7, 4, 7, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 283,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 283, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212078.206969)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212112.53179)
('Worker processing elapsed time: ', 34.32482099533081, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[283]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 283, ']')


'Epoch [284] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1788,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00047190333829335033,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 5, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 284,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 284, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212112.537693)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212146.587946)
('Worker processing elapsed time: ', 34.05025291442871, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[284]', 'EPOCH LOSS:', 257.33953913422562, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 284, ']')


'Epoch [285] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1107,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007591899675790626,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 285,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 285, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212146.594021)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212180.233536)
('Worker processing elapsed time: ', 33.6395149230957, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[285]', 'EPOCH LOSS:', 0.025518358535619543, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 285, ']')


'Epoch [286] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 127,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009264167458196989,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 7, 6, 9, 9, 6, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 286,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 286, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212180.239325)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212215.152387)
('Worker processing elapsed time: ', 34.91306185722351, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[286]', 'EPOCH LOSS:', 1.8407507492169435, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 286, ']')


'Epoch [287] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 103,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007145859684645895,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 287,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 287, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212215.158441)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212249.5536)
('Worker processing elapsed time: ', 34.395159006118774, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[287]', 'EPOCH LOSS:', 1.0031890519549611, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 287, ']')


'Epoch [288] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1026,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005500888674832753,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 288,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 288, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212249.562331)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212283.450746)
('Worker processing elapsed time: ', 33.88841509819031, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[288]', 'EPOCH LOSS:', 2.6157394105971674, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 288, ']')


'Epoch [289] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 255,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007229726476003191,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 4, 6, 9, 7, 8, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 289,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 289, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212283.456404)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212317.668198)
('Worker processing elapsed time: ', 34.21179413795471, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[289]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 289, ']')


'Epoch [290] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 936,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003934978814563266,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 290,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 290, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212317.674124)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212352.082294)
('Worker processing elapsed time: ', 34.408169984817505, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[290]', 'EPOCH LOSS:', 0.037511692608909381, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 290, ']')


'Epoch [291] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 191,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003098361188471103,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 5, 5, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 291,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 291, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212352.088521)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212386.495532)
('Worker processing elapsed time: ', 34.40701103210449, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[291]', 'EPOCH LOSS:', 48.860242762165406, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 291, ']')


'Epoch [292] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 945,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006277944965307562,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 9, 4, 9, 6, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 292,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 292, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212386.501762)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212421.225913)
('Worker processing elapsed time: ', 34.72415113449097, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[292]', 'EPOCH LOSS:', 46.694008616086954, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 292, ']')


'Epoch [293] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 380,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001693728386128907,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 293,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 293, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212421.23243)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212455.58788)
('Worker processing elapsed time: ', 34.35544991493225, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[293]', 'EPOCH LOSS:', 4.9440977510065496, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 293, ']')


'Epoch [294] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 782,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005154487836528895,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 8, 8, 4, 4, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 294,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 294, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212455.593833)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212489.793423)
('Worker processing elapsed time: ', 34.19958996772766, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[294]', 'EPOCH LOSS:', 0.024249798210535738, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 294, ']')


'Epoch [295] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 436,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007876876465870194,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 295,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 295, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212489.799212)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212523.669096)
('Worker processing elapsed time: ', 33.86988401412964, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[295]', 'EPOCH LOSS:', 45.569752436170191, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 295, ']')


'Epoch [296] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 698,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00010366058244726467,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 296,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 296, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212523.675199)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212558.099374)
('Worker processing elapsed time: ', 34.42417502403259, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[296]', 'EPOCH LOSS:', 13.562442176827888, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 296, ']')


'Epoch [297] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 950,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005058893856724372,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 4, 4, 6, 6, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 297,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 297, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212558.105515)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212593.769077)
('Worker processing elapsed time: ', 35.663562059402466, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[297]', 'EPOCH LOSS:', 0.027495053554675665, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 297, ']')


'Epoch [298] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1083,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006186166650678209,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 6, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 298,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 298, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212593.774329)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212627.458181)
('Worker processing elapsed time: ', 33.68385195732117, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[298]', 'EPOCH LOSS:', 0.025468723492699366, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 298, ']')


'Epoch [299] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1680,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002769141369488193,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 7, 8, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 299,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 299, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212627.463535)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212661.696931)
('Worker processing elapsed time: ', 34.23339581489563, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[299]', 'EPOCH LOSS:', 492.24428254876955, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 299, ']')


'Epoch [300] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1072,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00021173080966395842,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 6, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 300,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 300, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212661.703342)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212695.800194)
('Worker processing elapsed time: ', 34.09685206413269, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[300]', 'EPOCH LOSS:', 2726.6894949375892, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 300, ']')


'Epoch [301] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 891,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005085250759494659,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 7, 9, 9, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 301,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 301, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212695.806446)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212729.849668)
('Worker processing elapsed time: ', 34.043221950531006, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[301]', 'EPOCH LOSS:', 1.7973264563394231, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 301, ']')


'Epoch [302] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 604,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002760573664140468,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 7, 4, 7, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 302,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 302, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212729.855536)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212763.943844)
('Worker processing elapsed time: ', 34.08830809593201, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[302]', 'EPOCH LOSS:', 0.02499992581229021, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 302, ']')


'Epoch [303] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 230,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008510396743986267,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 303,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 303, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212763.949391)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212797.840671)
('Worker processing elapsed time: ', 33.89128017425537, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[303]', 'EPOCH LOSS:', 139.99283042410437, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 303, ']')


'Epoch [304] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1915,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008101487197212103,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 5, 6, 7, 5, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 304,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 304, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212797.847074)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212833.878424)
('Worker processing elapsed time: ', 36.031349897384644, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[304]', 'EPOCH LOSS:', 0.023500548870159552, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 304, ']')


'Epoch [305] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 723,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004013999960867399,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 8, 8, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 305,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 305, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212833.885161)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212868.097853)
('Worker processing elapsed time: ', 34.21269202232361, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[305]', 'EPOCH LOSS:', 18.025394979519842, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 305, ']')


'Epoch [306] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 395,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003124152889344453,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 9, 7, 9, 7, 6, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 306,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 306, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212868.103416)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212903.024649)
('Worker processing elapsed time: ', 34.92123293876648, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[306]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 306, ']')


'Epoch [307] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 914,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007273180946695575,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 307,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 307, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212903.041008)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212937.403689)
('Worker processing elapsed time: ', 34.36268091201782, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[307]', 'EPOCH LOSS:', 0.0538149849694168, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 307, ']')


'Epoch [308] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1808,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008031012828906651,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 8, 9, 8, 5, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 308,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 308, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212937.410199)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494212971.685741)
('Worker processing elapsed time: ', 34.27554202079773, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[308]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 308, ']')


'Epoch [309] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 380,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008057561008589257,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 309,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 309, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494212971.691228)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213005.592463)
('Worker processing elapsed time: ', 33.90123510360718, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[309]', 'EPOCH LOSS:', 523.63558320681784, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 309, ']')


'Epoch [310] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 202,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006863764845945694,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 7, 8, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 310,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 310, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213005.598918)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213039.431476)
('Worker processing elapsed time: ', 33.832558155059814, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[310]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 310, ']')


'Epoch [311] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1499,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005165498812518864,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 6, 7, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 311,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 311, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213039.437979)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213073.254902)
('Worker processing elapsed time: ', 33.81692290306091, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[311]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 311, ']')


'Epoch [312] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1205,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00037597454727953793,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 312,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 312, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213073.260629)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213107.180284)
('Worker processing elapsed time: ', 33.919655084609985, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[312]', 'EPOCH LOSS:', 6.7953807974329905, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 312, ']')


'Epoch [313] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1890,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004933970412041836,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 5, 9, 9, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 313,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 313, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213107.186088)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213141.219136)
('Worker processing elapsed time: ', 34.033047914505005, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[313]', 'EPOCH LOSS:', 0.023240669642110492, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 313, ']')


'Epoch [314] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 371,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007288389987529043,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 314,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 314, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213141.225011)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213175.061503)
('Worker processing elapsed time: ', 33.83649182319641, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[314]', 'EPOCH LOSS:', 0.42958830301285561, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 314, ']')


'Epoch [315] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1498,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016927200659007998,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 315,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 315, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213175.067034)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213209.211063)
('Worker processing elapsed time: ', 34.14402890205383, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[315]', 'EPOCH LOSS:', 8.8638660528870563, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 315, ']')


'Epoch [316] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 273,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005706488558029899,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 6, 9, 8, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 316,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 316, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213209.216874)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213243.239247)
('Worker processing elapsed time: ', 34.02237319946289, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[316]', 'EPOCH LOSS:', 0.024742922670062156, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 316, ']')


'Epoch [317] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1953,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007951221224424381,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 5, 6, 6, 8, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 317,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 317, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213243.246062)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213277.94924)
('Worker processing elapsed time: ', 34.70317792892456, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[317]', 'EPOCH LOSS:', 2.7507876821150457, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 317, ']')


'Epoch [318] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 936,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009932858459026088,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 9, 6, 7, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 318,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 318, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213277.955692)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213313.187572)
('Worker processing elapsed time: ', 35.2318799495697, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[318]', 'EPOCH LOSS:', 0.033077247765850734, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 318, ']')


'Epoch [319] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 857,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00042111372002197334,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 8, 4, 6, 7, 4, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 319,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 319, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213313.193973)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213349.171469)
('Worker processing elapsed time: ', 35.97749590873718, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[319]', 'EPOCH LOSS:', 0.025181143499475415, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 319, ']')


'Epoch [320] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1132,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007619581285298533,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 9, 4, 5, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 320,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 320, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213349.176955)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213384.487284)
('Worker processing elapsed time: ', 35.3103289604187, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[320]', 'EPOCH LOSS:', 0.040059664960478907, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 320, ']')


'Epoch [321] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 440,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008394690606632984,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 8, 8, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 321,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 321, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213384.493466)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213418.863669)
('Worker processing elapsed time: ', 34.37020301818848, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[321]', 'EPOCH LOSS:', 0.49206608439150645, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 321, ']')


'Epoch [322] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 256,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016063703758510315,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 4, 8, 7, 4, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 322,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 322, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213418.870444)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213453.590705)
('Worker processing elapsed time: ', 34.72026085853577, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[322]', 'EPOCH LOSS:', 251.44277827062152, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 322, ']')


'Epoch [323] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 919,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00042213816129297536,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 5, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 323,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 323, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213453.596707)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213488.290029)
('Worker processing elapsed time: ', 34.69332194328308, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[323]', 'EPOCH LOSS:', 0.15616548846990147, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 323, ']')


'Epoch [324] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 773,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000957049923292035,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 9, 9, 7, 8, 9, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 324,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 324, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213488.295502)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213523.257754)
('Worker processing elapsed time: ', 34.962252140045166, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[324]', 'EPOCH LOSS:', 5695.4528423287748, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 324, ']')


'Epoch [325] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1623,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008628485910245398,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 8, 5, 5, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 325,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 325, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213523.26358)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213558.608714)
('Worker processing elapsed time: ', 35.345134019851685, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[325]', 'EPOCH LOSS:', 2.4000820784272565, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 325, ']')


'Epoch [326] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1339,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006007843441036133,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 9, 4, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 326,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 326, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213558.614525)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213592.647638)
('Worker processing elapsed time: ', 34.0331130027771, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[326]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 326, ']')


'Epoch [327] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 828,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00017248636993993287,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 5, 9, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 327,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 327, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213592.654254)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213626.494024)
('Worker processing elapsed time: ', 33.83977007865906, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[327]', 'EPOCH LOSS:', 0.030309732304281866, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 327, ']')


'Epoch [328] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1191,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007886489479695561,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 4, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 328,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 328, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213626.499726)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213661.182184)
('Worker processing elapsed time: ', 34.68245792388916, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[328]', 'EPOCH LOSS:', 2.664520308035434, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 328, ']')


'Epoch [329] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1786,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004965900200960526,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 8, 5, 4, 7, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 329,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 329, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213661.187752)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213695.971532)
('Worker processing elapsed time: ', 34.783780097961426, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[329]', 'EPOCH LOSS:', 644.75668101017629, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 329, ']')


'Epoch [330] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1460,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003052335882131891,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 8, 4, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 330,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 330, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213695.977025)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213729.829289)
('Worker processing elapsed time: ', 33.85226392745972, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[330]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 330, ']')


'Epoch [331] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1775,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00011537443812444353,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 9, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 331,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 331, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213729.834819)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213763.908151)
('Worker processing elapsed time: ', 34.07333183288574, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[331]', 'EPOCH LOSS:', 5682.0157825250108, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 331, ']')


'Epoch [332] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 563,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00039505399634620555,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 332,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 332, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213763.914283)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213797.395701)
('Worker processing elapsed time: ', 33.4814178943634, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[332]', 'EPOCH LOSS:', 4339.2674101830207, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 332, ']')


'Epoch [333] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1366,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00027067503244666597,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 6, 6, 4, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 333,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 333, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213797.40209)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213831.813928)
('Worker processing elapsed time: ', 34.4118378162384, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[333]', 'EPOCH LOSS:', 1343.2486954398375, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 333, ']')


'Epoch [334] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1211,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005698069082640194,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 7, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 334,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 334, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213831.819417)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213865.473839)
('Worker processing elapsed time: ', 33.65442204475403, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[334]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 334, ']')


'Epoch [335] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 582,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000390552582207955,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 6, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 335,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 335, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213865.479488)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213900.207091)
('Worker processing elapsed time: ', 34.72760319709778, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[335]', 'EPOCH LOSS:', 13.516795264110813, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 335, ']')


'Epoch [336] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1246,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006637391506176297,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 8, 8, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 336,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 336, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213900.213041)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213935.334722)
('Worker processing elapsed time: ', 35.12168097496033, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[336]', 'EPOCH LOSS:', 0.046950018111019245, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 336, ']')


'Epoch [337] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 958,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007427548253426549,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 337,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 337, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213935.34048)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494213968.972013)
('Worker processing elapsed time: ', 33.63153290748596, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[337]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 337, ']')


'Epoch [338] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1680,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00011935569132800084,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 8, 6, 6, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 338,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 338, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494213968.978675)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214002.952921)
('Worker processing elapsed time: ', 33.97424602508545, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[338]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 338, ']')


'Epoch [339] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1033,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008317903260392685,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 8, 7, 6, 7, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 339,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 339, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214002.95923)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214037.608875)
('Worker processing elapsed time: ', 34.64964509010315, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[339]', 'EPOCH LOSS:', 2.4888918582037562, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 339, ']')


'Epoch [340] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 126,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001257509101014933,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 5, 7, 8, 6, 9, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 340,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 340, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214037.616011)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214073.55623)
('Worker processing elapsed time: ', 35.94021916389465, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[340]', 'EPOCH LOSS:', 247.3641176623909, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 340, ']')


'Epoch [341] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1742,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008536263441509538,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 341,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 341, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214073.561681)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214107.998495)
('Worker processing elapsed time: ', 34.436814069747925, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[341]', 'EPOCH LOSS:', 0.9042043383341043, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 341, ']')


'Epoch [342] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 637,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005743938817497626,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 5, 8, 7, 5, 5, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 342,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 342, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214108.004656)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214142.934653)
('Worker processing elapsed time: ', 34.929996967315674, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[342]', 'EPOCH LOSS:', 4.7547711489865963, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 342, ']')


'Epoch [343] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 834,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008966595440443907,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 8, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 343,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 343, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214142.940697)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214177.000592)
('Worker processing elapsed time: ', 34.059895038604736, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[343]', 'EPOCH LOSS:', 3232.9733585915455, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 343, ']')


'Epoch [344] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1747,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000405555880159505,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 6, 9, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 344,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 344, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214177.006356)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214210.927107)
('Worker processing elapsed time: ', 33.920751094818115, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[344]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 344, ']')


'Epoch [345] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 364,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00038096481928512414,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 7, 4, 8, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 345,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 345, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214210.933132)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214246.194267)
('Worker processing elapsed time: ', 35.26113510131836, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[345]', 'EPOCH LOSS:', 0.027899690598087095, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 345, ']')


'Epoch [346] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1048,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00014573374193401975,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 5, 4, 5, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 346,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 346, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214246.199819)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214281.506282)
('Worker processing elapsed time: ', 35.30646300315857, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[346]', 'EPOCH LOSS:', 4.4486861188353188, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 346, ']')


'Epoch [347] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 529,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00020933012901103133,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 347,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 347, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214281.512803)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214315.798561)
('Worker processing elapsed time: ', 34.28575801849365, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[347]', 'EPOCH LOSS:', 0.5227405816641767, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 347, ']')


'Epoch [348] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 616,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008877110002178522,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 9, 5, 8, 5, 8, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 348,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 348, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214315.805483)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214350.124524)
('Worker processing elapsed time: ', 34.31904101371765, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[348]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 348, ']')


'Epoch [349] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 770,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000775593451069288,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 349,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 349, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214350.130236)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214384.051786)
('Worker processing elapsed time: ', 33.921550035476685, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[349]', 'EPOCH LOSS:', 1.3434747061290917, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 349, ']')


'Epoch [350] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 950,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004549217888490478,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 7, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 350,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 350, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214384.05743)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214418.676747)
('Worker processing elapsed time: ', 34.619317054748535, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[350]', 'EPOCH LOSS:', 1.9634199435304438, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 350, ']')


'Epoch [351] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1078,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00035825016847183496,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 6, 5, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 351,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 351, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214418.682469)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214452.901884)
('Worker processing elapsed time: ', 34.21941518783569, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[351]', 'EPOCH LOSS:', 8819.1869214169692, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 351, ']')


'Epoch [352] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1114,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006426590443039182,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 5, 4, 9, 6, 5, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 352,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 352, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214452.907958)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214487.807503)
('Worker processing elapsed time: ', 34.89954495429993, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[352]', 'EPOCH LOSS:', 760.36408827631283, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 352, ']')


'Epoch [353] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1242,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006962279338277684,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 8, 5, 6, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 353,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 353, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214487.813608)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214523.127571)
('Worker processing elapsed time: ', 35.313963174819946, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[353]', 'EPOCH LOSS:', 0.19257336667624997, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 353, ']')


'Epoch [354] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 765,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005776790965503545,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 7, 4, 8, 4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 354,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 354, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214523.133682)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214558.812651)
('Worker processing elapsed time: ', 35.67896890640259, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[354]', 'EPOCH LOSS:', 0.027421044675380521, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 354, ']')


'Epoch [355] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1070,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008439469405792138,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 6, 6, 8, 9, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 355,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 355, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214558.818701)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214594.49911)
('Worker processing elapsed time: ', 35.68040895462036, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[355]', 'EPOCH LOSS:', 0.023300644261027983, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 355, ']')


'Epoch [356] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1187,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009461521008693897,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 4, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 356,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 356, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214594.505568)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214628.203699)
('Worker processing elapsed time: ', 33.69813108444214, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[356]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 356, ']')


'Epoch [357] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1898,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007201976933436257,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 8, 8, 4, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 357,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 357, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214628.2094)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214662.250595)
('Worker processing elapsed time: ', 34.04119515419006, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[357]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 357, ']')


'Epoch [358] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 175,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005167136225971415,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 358,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 358, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214662.257335)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214696.690278)
('Worker processing elapsed time: ', 34.43294310569763, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[358]', 'EPOCH LOSS:', 0.40111703661357923, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 358, ']')


'Epoch [359] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1885,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008250682644495211,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 9, 5, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 359,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 359, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214696.696563)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214730.991161)
('Worker processing elapsed time: ', 34.29459810256958, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[359]', 'EPOCH LOSS:', 1033.2914630153189, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 359, ']')


'Epoch [360] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1041,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00011219258303654255,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 360,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 360, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214730.997193)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214764.62213)
('Worker processing elapsed time: ', 33.62493681907654, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[360]', 'EPOCH LOSS:', 0.02443575833008764, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 360, ']')


'Epoch [361] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 577,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005180230288138147,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 4, 4, 7, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 361,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 361, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214764.628782)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214798.638658)
('Worker processing elapsed time: ', 34.009876012802124, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[361]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 361, ']')


'Epoch [362] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 452,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005211724109462669,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 8, 7, 7, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 362,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 362, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214798.645662)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214833.878518)
('Worker processing elapsed time: ', 35.232856035232544, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[362]', 'EPOCH LOSS:', 0.061070608086677101, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 362, ']')


'Epoch [363] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1055,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008429201748353715,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 363,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 363, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214833.884123)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214867.675289)
('Worker processing elapsed time: ', 33.791165828704834, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[363]', 'EPOCH LOSS:', 0.90696342623312765, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 363, ']')


'Epoch [364] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1062,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006840829503093179,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 8, 7, 6, 7, 9, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 364,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 364, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214867.682072)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214903.579215)
('Worker processing elapsed time: ', 35.89714312553406, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[364]', 'EPOCH LOSS:', 2.7884204481748824, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 364, ']')


'Epoch [365] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 753,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005827118034148822,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 9, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 365,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 365, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214903.58487)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214937.272352)
('Worker processing elapsed time: ', 33.68748188018799, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[365]', 'EPOCH LOSS:', 0.023919542828663671, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 365, ']')


'Epoch [366] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 316,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002567979019647902,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 8, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 366,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 366, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214937.277974)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494214971.000672)
('Worker processing elapsed time: ', 33.72269821166992, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[366]', 'EPOCH LOSS:', 0.02500465735408933, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 366, ']')


'Epoch [367] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 316,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001908142995607796,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 367,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 367, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494214971.00641)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215004.927278)
('Worker processing elapsed time: ', 33.920868158340454, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[367]', 'EPOCH LOSS:', 33.940971169183953, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 367, ']')


'Epoch [368] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1492,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000469505204171717,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 9, 5, 5, 6, 7, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 368,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 368, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215004.933581)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215039.875529)
('Worker processing elapsed time: ', 34.94194793701172, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[368]', 'EPOCH LOSS:', 0.14178060072629076, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 368, ']')


'Epoch [369] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1502,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005990191204460858,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 4, 9, 9, 5, 7, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 369,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 369, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215039.882468)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215074.836716)
('Worker processing elapsed time: ', 34.95424795150757, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[369]', 'EPOCH LOSS:', 0.19492377817024209, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 369, ']')


'Epoch [370] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 122,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005504402196231681,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 9, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 370,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 370, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215074.842718)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215108.543532)
('Worker processing elapsed time: ', 33.70081400871277, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[370]', 'EPOCH LOSS:', 0.023914548967613917, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 370, ']')


'Epoch [371] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 419,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007571949661743284,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 4, 7, 7, 7, 7, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 371,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 371, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215108.550566)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215142.858749)
('Worker processing elapsed time: ', 34.30818295478821, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[371]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 371, ']')


'Epoch [372] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 605,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004085256720749537,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 5, 4, 7, 5, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 372,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 372, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215142.865345)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215178.533845)
('Worker processing elapsed time: ', 35.66849994659424, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[372]', 'EPOCH LOSS:', 0.025669482816661621, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 372, ']')


'Epoch [373] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1475,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00037027272529490275,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 9, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 373,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 373, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215178.539455)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215212.905588)
('Worker processing elapsed time: ', 34.366132974624634, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[373]', 'EPOCH LOSS:', 3.4990758661679546, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 373, ']')


'Epoch [374] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1350,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009985841349678798,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 374,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 374, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215212.912148)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215247.288575)
('Worker processing elapsed time: ', 34.37642693519592, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[374]', 'EPOCH LOSS:', 0.038310513999062872, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 374, ']')


'Epoch [375] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1514,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002668300127378017,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 8, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 375,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 375, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215247.297369)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215281.935701)
('Worker processing elapsed time: ', 34.6383318901062, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[375]', 'EPOCH LOSS:', 1.737388970567167, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 375, ']')


'Epoch [376] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 540,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006880703411101731,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 4, 6, 6, 6, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 376,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 376, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215281.942683)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215317.539943)
('Worker processing elapsed time: ', 35.59725999832153, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[376]', 'EPOCH LOSS:', 0.023921524591815787, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 376, ']')


'Epoch [377] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 384,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004691048140431865,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 6, 5, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 377,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 377, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215317.546257)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215351.82192)
('Worker processing elapsed time: ', 34.275662899017334, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[377]', 'EPOCH LOSS:', 26.48220250372486, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 377, ']')


'Epoch [378] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1509,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00030204635143831664,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 7, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 378,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 378, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215351.827495)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215385.913463)
('Worker processing elapsed time: ', 34.085968017578125, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[378]', 'EPOCH LOSS:', 0.29035582161348966, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 378, ']')


'Epoch [379] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1064,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008868034680615543,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 4, 9, 4, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 379,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 379, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215385.919619)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215419.955157)
('Worker processing elapsed time: ', 34.03553795814514, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[379]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 379, ']')


'Epoch [380] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 343,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007777917539733849,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 380,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 380, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215419.960796)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215454.356749)
('Worker processing elapsed time: ', 34.39595293998718, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[380]', 'EPOCH LOSS:', 0.33917664060912767, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 380, ']')


'Epoch [381] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1217,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004926488909915236,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 5, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 381,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 381, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215454.362136)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215488.449635)
('Worker processing elapsed time: ', 34.087499141693115, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[381]', 'EPOCH LOSS:', 0.35870206810063671, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 381, ']')


'Epoch [382] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1935,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00039086490390492713,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 8, 8, 6, 7, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 382,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 382, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215488.455184)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215524.026366)
('Worker processing elapsed time: ', 35.57118201255798, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[382]', 'EPOCH LOSS:', 0.69765234282512423, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 382, ']')


'Epoch [383] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1846,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007816531929075781,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 5, 6, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 383,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 383, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215524.032027)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215558.212029)
('Worker processing elapsed time: ', 34.180001974105835, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[383]', 'EPOCH LOSS:', 4.2521913657753352, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 383, ']')


'Epoch [384] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 345,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004410967605497592,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 7, 8, 9, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 384,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 384, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215558.217937)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215592.23314)
('Worker processing elapsed time: ', 34.01520299911499, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[384]', 'EPOCH LOSS:', 0.023815848138465254, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 384, ']')


'Epoch [385] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 197,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006748128114123226,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 5, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 385,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 385, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215592.239733)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215625.864853)
('Worker processing elapsed time: ', 33.62511992454529, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[385]', 'EPOCH LOSS:', 0.024699235613209875, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 385, ']')


'Epoch [386] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 994,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00027602035397881065,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 7, 7, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 386,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 386, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215625.870632)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215659.690927)
('Worker processing elapsed time: ', 33.820295095443726, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[386]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 386, ']')


'Epoch [387] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1439,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00039198932829267725,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 5, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 387,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 387, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215659.697745)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215694.351717)
('Worker processing elapsed time: ', 34.653971910476685, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[387]', 'EPOCH LOSS:', 0.018008175649539541, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 387, ']')


'Epoch [388] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1374,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00031682021790165396,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 6, 8, 8, 9, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 388,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 388, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215694.358417)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215728.538518)
('Worker processing elapsed time: ', 34.18010091781616, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[388]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 388, ']')


'Epoch [389] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1301,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007017686257868433,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 6, 9, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 389,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 389, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215728.544163)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215762.422626)
('Worker processing elapsed time: ', 33.87846302986145, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[389]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 389, ']')


'Epoch [390] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1176,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009627917704461306,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 5, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 390,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 390, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215762.429522)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215796.778606)
('Worker processing elapsed time: ', 34.34908390045166, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[390]', 'EPOCH LOSS:', 0.484925869965811, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 390, ']')


'Epoch [391] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1512,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00031544205325036254,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 5, 8, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 391,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 391, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215796.784282)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215830.52156)
('Worker processing elapsed time: ', 33.73727798461914, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[391]', 'EPOCH LOSS:', 0.024395938534859364, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 391, ']')


'Epoch [392] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1195,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002411391191130222,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 9, 7, 5, 6, 6, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 392,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 392, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215830.52777)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215866.459895)
('Worker processing elapsed time: ', 35.932124853134155, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[392]', 'EPOCH LOSS:', 0.16102317620341106, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 392, ']')


'Epoch [393] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1651,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008820204059128681,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 8, 6, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 393,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 393, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215866.465194)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215900.612065)
('Worker processing elapsed time: ', 34.1468710899353, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[393]', 'EPOCH LOSS:', 6236.1569844797004, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 393, ']')


'Epoch [394] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 655,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00015441733896054985,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 5, 6, 6, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 394,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 394, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215900.618546)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215935.05474)
('Worker processing elapsed time: ', 34.43619394302368, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[394]', 'EPOCH LOSS:', 475.9522469047331, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 394, ']')


'Epoch [395] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1557,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003894772984560168,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 8, 5, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 395,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 395, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215935.061426)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494215968.845511)
('Worker processing elapsed time: ', 33.7840850353241, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[395]', 'EPOCH LOSS:', 0.024016137942606978, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 395, ']')


'Epoch [396] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1620,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007993668304748723,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 6, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 396,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 396, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494215968.851965)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216002.92409)
('Worker processing elapsed time: ', 34.07212495803833, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[396]', 'EPOCH LOSS:', 2.9441034116325042, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 396, ']')


'Epoch [397] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 847,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00025329534502854453,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 9, 4, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 397,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 397, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216002.930149)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216037.839386)
('Worker processing elapsed time: ', 34.909236907958984, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[397]', 'EPOCH LOSS:', 9.6491501014924275, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 397, ']')


'Epoch [398] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1355,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008437798861201099,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 7, 7, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 398,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 398, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216037.845363)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216071.690375)
('Worker processing elapsed time: ', 33.845012187957764, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[398]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 398, ']')


'Epoch [399] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 301,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00019861578787562994,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 8, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 399,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 399, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216071.697434)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216105.754494)
('Worker processing elapsed time: ', 34.05706000328064, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[399]', 'EPOCH LOSS:', 138.9931316535486, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 399, ']')


'Epoch [400] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 988,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00032532244391878667,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 400,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 400, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216105.761359)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216140.126874)
('Worker processing elapsed time: ', 34.3655149936676, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[400]', 'EPOCH LOSS:', 0.055335023850062399, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 400, ']')


'Epoch [401] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 757,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007956928771527126,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 6, 7, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 401,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 401, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216140.132591)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216173.983519)
('Worker processing elapsed time: ', 33.85092806816101, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[401]', 'EPOCH LOSS:', 0.023914211007867808, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 401, ']')


'Epoch [402] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1460,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00048117219919054663,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 9, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 402,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 402, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216173.990032)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216207.699425)
('Worker processing elapsed time: ', 33.70939302444458, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[402]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 402, ']')


'Epoch [403] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 704,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001242931944797173,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 403,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 403, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216207.70584)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216241.513006)
('Worker processing elapsed time: ', 33.80716586112976, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[403]', 'EPOCH LOSS:', 394.15957462582315, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 403, ']')


'Epoch [404] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 596,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008974834563743765,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 4, 7, 6, 9, 8, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 404,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 404, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216241.519515)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216277.429222)
('Worker processing elapsed time: ', 35.90970706939697, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[404]', 'EPOCH LOSS:', 0.1260620708129106, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 404, ']')


'Epoch [405] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1640,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00039555325010854486,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 4, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 405,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 405, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216277.435857)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216311.516061)
('Worker processing elapsed time: ', 34.080204010009766, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[405]', 'EPOCH LOSS:', 0.15242812210953816, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 405, ']')


'Epoch [406] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 101,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008680873285225385,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 5, 4, 7, 7, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 406,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 406, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216311.522264)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216347.141594)
('Worker processing elapsed time: ', 35.61932992935181, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[406]', 'EPOCH LOSS:', 0.02569552086313261, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 406, ']')


'Epoch [407] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1316,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00017510384761607858,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 407,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 407, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216347.147423)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216380.988357)
('Worker processing elapsed time: ', 33.84093403816223, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[407]', 'EPOCH LOSS:', 5.7695679184551381, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 407, ']')


'Epoch [408] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1440,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00043838123543745247,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 5, 8, 4, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 408,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 408, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216380.994084)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216416.346941)
('Worker processing elapsed time: ', 35.35285711288452, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[408]', 'EPOCH LOSS:', 15.632860054201469, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 408, ']')


'Epoch [409] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 893,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005830123446246194,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 6, 5, 7, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 409,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 409, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216416.352897)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216450.338646)
('Worker processing elapsed time: ', 33.98574900627136, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[409]', 'EPOCH LOSS:', 0.024699310024825397, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 409, ']')


'Epoch [410] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1996,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006644347866139004,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 9, 4, 4, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 410,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 410, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216450.346133)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216484.76979)
('Worker processing elapsed time: ', 34.423656940460205, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[410]', 'EPOCH LOSS:', 206.009522665161, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 410, ']')


'Epoch [411] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1280,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00015865206930870083,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 8, 9, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 411,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 411, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216484.776116)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216518.677397)
('Worker processing elapsed time: ', 33.901281118392944, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[411]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 411, ']')


'Epoch [412] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1063,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005047029663484717,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 412,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 412, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216518.683777)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216552.265306)
('Worker processing elapsed time: ', 33.58152890205383, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[412]', 'EPOCH LOSS:', 0.02533983228385666, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 412, ']')


'Epoch [413] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1363,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00019133188527851205,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 8, 6, 4, 7, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 413,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 413, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216552.270966)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216586.325026)
('Worker processing elapsed time: ', 34.054059982299805, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[413]', 'EPOCH LOSS:', 0.024643463216498528, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 413, ']')


'Epoch [414] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1809,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006078212527821597,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 414,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 414, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216586.330573)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216620.660579)
('Worker processing elapsed time: ', 34.33000588417053, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[414]', 'EPOCH LOSS:', 0.058179100461272081, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 414, ']')


'Epoch [415] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1002,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006382326464135937,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 6, 9, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 415,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 415, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216620.66667)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216654.906472)
('Worker processing elapsed time: ', 34.23980188369751, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[415]', 'EPOCH LOSS:', 0.72053195345408627, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 415, ']')


'Epoch [416] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1988,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009037882146910176,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 4, 5, 6, 7, 7, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 416,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 416, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216654.912079)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216690.777977)
('Worker processing elapsed time: ', 35.86589789390564, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[416]', 'EPOCH LOSS:', 0.34734993911450712, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 416, ']')


'Epoch [417] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1568,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00024309154274060275,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 5, 4, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 417,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 417, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216690.783872)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216724.562321)
('Worker processing elapsed time: ', 33.778449058532715, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[417]', 'EPOCH LOSS:', 0.023910274760426273, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 417, ']')


'Epoch [418] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 161,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009111698891143615,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 8, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 418,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 418, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216724.569094)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216758.293517)
('Worker processing elapsed time: ', 33.72442317008972, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[418]', 'EPOCH LOSS:', 0.024347935749081808, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 418, ']')


'Epoch [419] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 700,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008210548304684765,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 6, 5, 9, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 419,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 419, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216758.299646)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216792.76819)
('Worker processing elapsed time: ', 34.468544006347656, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[419]', 'EPOCH LOSS:', 3.0458159154300599, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 419, ']')


'Epoch [420] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 278,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00031808548055317165,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 8, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 420,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 420, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216792.774012)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216826.791559)
('Worker processing elapsed time: ', 34.01754689216614, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[420]', 'EPOCH LOSS:', 105.65795827872788, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 420, ']')


'Epoch [421] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1383,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00039258154476803204,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 421,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 421, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216826.797008)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216860.648686)
('Worker processing elapsed time: ', 33.851677894592285, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[421]', 'EPOCH LOSS:', 1.0579174388669437, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 421, ']')


'Epoch [422] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1966,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005652272206720301,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 422,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 422, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216860.654594)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216894.933611)
('Worker processing elapsed time: ', 34.279016971588135, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[422]', 'EPOCH LOSS:', 0.19121014608920997, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 422, ']')


'Epoch [423] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 401,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006997797591534198,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 5, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 423,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 423, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216894.939179)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216928.620771)
('Worker processing elapsed time: ', 33.68159198760986, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[423]', 'EPOCH LOSS:', 0.02446735616720094, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 423, ']')


'Epoch [424] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1172,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00043753924542621985,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 4, 6, 6, 4, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 424,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 424, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216928.627636)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216964.214581)
('Worker processing elapsed time: ', 35.58694505691528, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[424]', 'EPOCH LOSS:', 0.28774980065850148, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 424, ']')


'Epoch [425] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1342,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008133987651869227,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 8, 4, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 425,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 425, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216964.220035)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494216998.485535)
('Worker processing elapsed time: ', 34.26549983024597, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[425]', 'EPOCH LOSS:', 4.77965599963846, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 425, ']')


'Epoch [426] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 749,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00013787732380637614,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 9, 4, 5, 6, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 426,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 426, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494216998.491778)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217034.082999)
('Worker processing elapsed time: ', 35.59122109413147, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[426]', 'EPOCH LOSS:', 0.17806640631104373, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 426, ']')


'Epoch [427] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 968,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009022937976170295,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 427,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 427, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217034.088407)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217067.88314)
('Worker processing elapsed time: ', 33.79473304748535, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[427]', 'EPOCH LOSS:', 0.14396214111790051, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 427, ']')


'Epoch [428] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1195,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006544831017554645,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 9, 5, 4, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 428,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 428, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217067.889014)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217101.904167)
('Worker processing elapsed time: ', 34.01515293121338, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[428]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 428, ']')


'Epoch [429] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 149,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00013351146818804107,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 429,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 429, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217101.909917)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217136.201487)
('Worker processing elapsed time: ', 34.29156994819641, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[429]', 'EPOCH LOSS:', 0.12750149534664346, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 429, ']')


'Epoch [430] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 575,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007167899592438355,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 430,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 430, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217136.207958)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217170.322566)
('Worker processing elapsed time: ', 34.1146080493927, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[430]', 'EPOCH LOSS:', 2.2263819378989291, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 430, ']')


'Epoch [431] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1572,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00044817648051477543,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 6, 4, 6, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 431,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 431, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217170.328709)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217204.730776)
('Worker processing elapsed time: ', 34.40206718444824, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[431]', 'EPOCH LOSS:', 0.79001808360647596, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 431, ']')


'Epoch [432] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1193,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008786267250447757,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 5, 4, 8, 4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 432,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 432, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217204.736479)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217240.377211)
('Worker processing elapsed time: ', 35.64073204994202, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[432]', 'EPOCH LOSS:', 0.024172267743650649, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 432, ']')


'Epoch [433] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1867,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004851666881893323,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 433,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 433, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217240.38201)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217274.772272)
('Worker processing elapsed time: ', 34.39026212692261, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[433]', 'EPOCH LOSS:', 0.05730463027274621, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 433, ']')


'Epoch [434] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 427,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002481441937541097,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 434,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 434, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217274.778137)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217309.152076)
('Worker processing elapsed time: ', 34.373939037323, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[434]', 'EPOCH LOSS:', 1.4994249269102602, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 434, ']')


'Epoch [435] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1668,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006864150484774971,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 9, 8, 7, 9, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 435,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 435, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217309.158098)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217344.842097)
('Worker processing elapsed time: ', 35.68399906158447, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[435]', 'EPOCH LOSS:', 0.023558449107473921, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 435, ']')


'Epoch [436] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 903,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006313004433835263,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 436,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 436, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217344.846883)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217378.414777)
('Worker processing elapsed time: ', 33.567893981933594, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[436]', 'EPOCH LOSS:', 0.024498904439441645, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 436, ']')


'Epoch [437] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1225,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00010950343699383429,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 7, 7, 9, 7, 7, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 437,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 437, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217378.421783)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217414.371415)
('Worker processing elapsed time: ', 35.94963192939758, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[437]', 'EPOCH LOSS:', 0.10260070427279917, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 437, ']')


'Epoch [438] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1623,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007197976127777286,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 7, 5, 5, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 438,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 438, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217414.376827)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217449.683446)
('Worker processing elapsed time: ', 35.3066189289093, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[438]', 'EPOCH LOSS:', 0.047359559165476552, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 438, ']')


'Epoch [439] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1277,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002507376647640675,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 439,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 439, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217449.689887)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217483.579687)
('Worker processing elapsed time: ', 33.88980007171631, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[439]', 'EPOCH LOSS:', 1.8378022394049378, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 439, ']')


'Epoch [440] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1076,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00055168717425692,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 440,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 440, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217483.585459)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217517.365361)
('Worker processing elapsed time: ', 33.77990198135376, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[440]', 'EPOCH LOSS:', 1.5065625606704669, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 440, ']')


'Epoch [441] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1841,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000702523815661078,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 7, 5, 6, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 441,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 441, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217517.371244)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217552.714302)
('Worker processing elapsed time: ', 35.34305810928345, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[441]', 'EPOCH LOSS:', 1.5873649201673772, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 441, ']')


'Epoch [442] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1838,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001804752658791642,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 9, 9, 6, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 442,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 442, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217552.720607)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217587.087354)
('Worker processing elapsed time: ', 34.36674690246582, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[442]', 'EPOCH LOSS:', 1.0492293600115792, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 442, ']')


'Epoch [443] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1828,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00042289124163736344,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 5, 8, 6, 8, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 443,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 443, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217587.093747)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217622.672555)
('Worker processing elapsed time: ', 35.578808069229126, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[443]', 'EPOCH LOSS:', 0.23073692647457181, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 443, ']')


'Epoch [444] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 126,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000280414696813604,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 9, 4, 8, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 444,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 444, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217622.677964)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217656.662726)
('Worker processing elapsed time: ', 33.98476195335388, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[444]', 'EPOCH LOSS:', 0.023288649642270003, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 444, ']')


'Epoch [445] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 162,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007280195917194678,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 445,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 445, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217656.669102)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217690.994986)
('Worker processing elapsed time: ', 34.325884103775024, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[445]', 'EPOCH LOSS:', 0.025831874396346856, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 445, ']')


'Epoch [446] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1310,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008046918455413968,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 4, 8, 6, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 446,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 446, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217691.001816)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217725.375945)
('Worker processing elapsed time: ', 34.37412905693054, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[446]', 'EPOCH LOSS:', 20.474124743920548, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 446, ']')


'Epoch [447] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 178,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005064176680463593,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 4, 4, 4, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 447,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 447, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217725.381562)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217759.291347)
('Worker processing elapsed time: ', 33.90978503227234, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[447]', 'EPOCH LOSS:', 0.020895372562137089, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 447, ']')


'Epoch [448] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1965,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002701438058380026,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 5, 4, 4, 7, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 448,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 448, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217759.29814)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217793.970373)
('Worker processing elapsed time: ', 34.67223286628723, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[448]', 'EPOCH LOSS:', 8.8742054404102699, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 448, ']')


'Epoch [449] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1302,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007577723972653985,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 7, 9, 5, 8, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 449,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 449, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217793.977376)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217828.718873)
('Worker processing elapsed time: ', 34.74149703979492, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[449]', 'EPOCH LOSS:', 0.059523740957965698, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 449, ']')


'Epoch [450] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 639,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006442798699610977,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 4, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 450,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 450, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217828.725368)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217862.718785)
('Worker processing elapsed time: ', 33.99341702461243, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[450]', 'EPOCH LOSS:', 1.1632768082198657, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 450, ']')


'Epoch [451] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1317,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002786381902969038,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 4, 9, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 451,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 451, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217862.724478)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217897.612046)
('Worker processing elapsed time: ', 34.88756799697876, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[451]', 'EPOCH LOSS:', 0.078363289750267531, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 451, ']')


'Epoch [452] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1250,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007591948084260756,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 452,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 452, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217897.618061)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217931.191642)
('Worker processing elapsed time: ', 33.5735809803009, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[452]', 'EPOCH LOSS:', 0.026575398804888285, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 452, ']')


'Epoch [453] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 310,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009893771897009724,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 8, 8, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 453,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 453, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217931.198357)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494217965.056373)
('Worker processing elapsed time: ', 33.85801577568054, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[453]', 'EPOCH LOSS:', 0.024209107219268611, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 453, ']')


'Epoch [454] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 871,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004716080431737765,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 9, 9, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 454,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 454, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494217965.062656)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218000.060714)
('Worker processing elapsed time: ', 34.99805808067322, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[454]', 'EPOCH LOSS:', 99.859992346791614, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 454, ']')


'Epoch [455] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1733,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008315359605038236,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 5, 8, 9, 7, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 455,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 455, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218000.067395)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218035.668374)
('Worker processing elapsed time: ', 35.60097908973694, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[455]', 'EPOCH LOSS:', 0.047407304928349076, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 455, ']')


'Epoch [456] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1059,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003996272081333818,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 9, 7, 6, 4, 7, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 456,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 456, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218035.674922)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218070.596579)
('Worker processing elapsed time: ', 34.9216570854187, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[456]', 'EPOCH LOSS:', 106.36199063913048, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 456, ']')


'Epoch [457] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1961,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007789069856221719,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 6, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 457,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 457, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218070.60324)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218104.646634)
('Worker processing elapsed time: ', 34.04339408874512, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[457]', 'EPOCH LOSS:', 3.7820284865058835, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 457, ']')


'Epoch [458] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 931,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00019960722310603959,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 8, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 458,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 458, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218104.652259)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218138.285645)
('Worker processing elapsed time: ', 33.63338589668274, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[458]', 'EPOCH LOSS:', 0.02507522144399699, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 458, ']')


'Epoch [459] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1920,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000623490726681288,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 6, 5, 6, 8, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 459,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 459, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218138.291769)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218173.010355)
('Worker processing elapsed time: ', 34.71858596801758, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[459]', 'EPOCH LOSS:', 2.1907810699082675, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 459, ']')


'Epoch [460] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 501,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007122469880949858,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 7, 6, 7, 7, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 460,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 460, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218173.016324)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218207.020894)
('Worker processing elapsed time: ', 34.00457000732422, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[460]', 'EPOCH LOSS:', 0.024523813932880251, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 460, ']')


'Epoch [461] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 404,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006031015526988189,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 5, 8, 8, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 461,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 461, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218207.027666)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218241.030239)
('Worker processing elapsed time: ', 34.002573013305664, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[461]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 461, ']')


'Epoch [462] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1154,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00018237088092305515,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 7, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 462,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 462, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218241.035899)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218275.640524)
('Worker processing elapsed time: ', 34.60462498664856, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[462]', 'EPOCH LOSS:', 0.31485802455230388, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 462, ']')


'Epoch [463] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1380,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008899954956784541,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 9, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 463,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 463, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218275.647304)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218309.699547)
('Worker processing elapsed time: ', 34.05224299430847, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[463]', 'EPOCH LOSS:', 2.0069893115700603, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 463, ']')


'Epoch [464] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1058,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008600731656530869,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 464,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 464, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218309.70598)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218343.593109)
('Worker processing elapsed time: ', 33.887128829956055, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[464]', 'EPOCH LOSS:', 0.15714815638722734, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 464, ']')


'Epoch [465] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1199,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000609079264160423,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 5, 9, 6, 5, 5, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 465,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 465, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218343.598994)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218378.539374)
('Worker processing elapsed time: ', 34.94038009643555, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[465]', 'EPOCH LOSS:', 9.6784850723694227, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 465, ']')


'Epoch [466] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 828,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001925276622257345,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 6, 4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 466,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 466, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218378.545961)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218412.400033)
('Worker processing elapsed time: ', 33.85407209396362, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[466]', 'EPOCH LOSS:', 0.025111874308569534, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 466, ']')


'Epoch [467] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1616,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00020119436203654974,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 9, 9, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 467,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 467, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218412.406268)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218446.670936)
('Worker processing elapsed time: ', 34.264668226242065, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[467]', 'EPOCH LOSS:', 3667.386566899695, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 467, ']')


'Epoch [468] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 800,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006887843963004793,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 5, 5, 8, 9, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 468,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 468, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218446.677365)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218481.313951)
('Worker processing elapsed time: ', 34.63658595085144, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[468]', 'EPOCH LOSS:', 0.027436472195299818, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 468, ']')


'Epoch [469] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 809,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005772764672275716,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 4, 7, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 469,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 469, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218481.319729)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218516.312203)
('Worker processing elapsed time: ', 34.9924738407135, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[469]', 'EPOCH LOSS:', 4.7924266882960742, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 469, ']')


'Epoch [470] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1573,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008469260906527208,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 5, 4, 8, 9, 7, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 470,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 470, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218516.318049)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218550.503468)
('Worker processing elapsed time: ', 34.1854190826416, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[470]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 470, ']')


'Epoch [471] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1695,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00011217412521384563,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 8, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 471,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 471, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218550.510384)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218584.2745)
('Worker processing elapsed time: ', 33.76411581039429, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[471]', 'EPOCH LOSS:', 0.023634013409650226, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 471, ']')


'Epoch [472] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 817,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000928701141065494,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 5, 6, 7, 6, 4, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 472,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 472, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218584.280353)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218618.529359)
('Worker processing elapsed time: ', 34.249006032943726, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[472]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 472, ']')


'Epoch [473] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 302,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002213573825630581,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 5, 9, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 473,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 473, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218618.535484)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218653.534321)
('Worker processing elapsed time: ', 34.99883699417114, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[473]', 'EPOCH LOSS:', 576.89580836841981, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 473, ']')


'Epoch [474] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1387,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001440756109605557,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 8, 4, 6, 4, 5, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 474,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 474, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218653.54122)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218687.750682)
('Worker processing elapsed time: ', 34.20946216583252, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[474]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 474, ']')


'Epoch [475] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 891,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005871161892305561,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 475,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 475, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218687.756605)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218722.116999)
('Worker processing elapsed time: ', 34.36039400100708, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[475]', 'EPOCH LOSS:', 0.042032707106289771, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 475, ']')


'Epoch [476] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 894,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005775475652285882,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 5, 6, 4, 5, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 476,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 476, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218722.123366)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218756.225074)
('Worker processing elapsed time: ', 34.10170793533325, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[476]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 476, ']')


'Epoch [477] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1483,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004472896576606588,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 4, 7, 6, 9, 4, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 477,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 477, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218756.231326)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218790.36289)
('Worker processing elapsed time: ', 34.131563901901245, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[477]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 477, ']')


'Epoch [478] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 431,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00032468303446903763,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 6, 9, 5, 9, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 478,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 478, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218790.368642)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218824.451243)
('Worker processing elapsed time: ', 34.082600831985474, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[478]', 'EPOCH LOSS:', 0.024283165866607795, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 478, ']')


'Epoch [479] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1175,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005319306317081292,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 479,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 479, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218824.456938)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218858.303163)
('Worker processing elapsed time: ', 33.84622502326965, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[479]', 'EPOCH LOSS:', 7.035473774829951, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 479, ']')


'Epoch [480] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 243,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009969797787931405,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 4, 8, 9, 6, 8, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 480,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 480, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218858.309707)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218892.512857)
('Worker processing elapsed time: ', 34.203150033950806, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[480]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 480, ']')


'Epoch [481] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1807,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004924545299610445,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 6, 5, 8, 9, 4, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 481,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 481, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218892.518956)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218926.839347)
('Worker processing elapsed time: ', 34.320390939712524, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[481]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 481, ']')


'Epoch [482] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1176,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007166216359560505,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 4, 7, 8, 8, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 482,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 482, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218926.845825)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218962.464893)
('Worker processing elapsed time: ', 35.61906814575195, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[482]', 'EPOCH LOSS:', 0.025348434211007898, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 482, ']')


'Epoch [483] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 135,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008054100227150958,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 7, 4, 9, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 483,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 483, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218962.471644)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494218996.451718)
('Worker processing elapsed time: ', 33.98007416725159, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[483]', 'EPOCH LOSS:', 0.023120115968778384, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 483, ']')


'Epoch [484] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1588,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009988527641191663,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 4, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 484,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 484, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494218996.457981)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219030.15065)
('Worker processing elapsed time: ', 33.69266891479492, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[484]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 484, ']')


'Epoch [485] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1715,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005138473025864542,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 6, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 485,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 485, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219030.156776)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219064.848168)
('Worker processing elapsed time: ', 34.691391944885254, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[485]', 'EPOCH LOSS:', 0.55578272770491777, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 485, ']')


'Epoch [486] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 436,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005624921640517194,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 8, 7, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 486,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 486, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219064.853983)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219099.121705)
('Worker processing elapsed time: ', 34.26772212982178, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[486]', 'EPOCH LOSS:', 3292.8274648495253, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 486, ']')


'Epoch [487] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 957,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00039356979244567253,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 5, 4, 4, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 487,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 487, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219099.127449)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219133.071103)
('Worker processing elapsed time: ', 33.94365406036377, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[487]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 487, ']')


'Epoch [488] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 508,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00026689169742176086,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 8, 6, 7, 8, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 488,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 488, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219133.076863)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219167.722756)
('Worker processing elapsed time: ', 34.64589285850525, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[488]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 488, ']')


'Epoch [489] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 831,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009432100075631209,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 7, 9, 6, 8, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 489,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 489, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219167.729678)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219201.947328)
('Worker processing elapsed time: ', 34.217650175094604, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[489]', 'EPOCH LOSS:', 0.025085047612029711, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 489, ']')


'Epoch [490] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1865,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004121459905839117,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 4, 5, 5, 7, 9, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 490,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 490, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219201.953675)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219236.104358)
('Worker processing elapsed time: ', 34.15068292617798, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[490]', 'EPOCH LOSS:', 0.02341822948368669, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 490, ']')


'Epoch [491] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 774,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00026271427578133943,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 5, 5, 4, 9, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 491,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 491, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219236.110378)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219271.708774)
('Worker processing elapsed time: ', 35.59839606285095, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[491]', 'EPOCH LOSS:', 0.023775963993839487, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 491, ']')


'Epoch [492] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1271,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006851231715730443,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 4, 9, 8, 4, 7, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 492,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 492, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219271.715474)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219307.555601)
('Worker processing elapsed time: ', 35.84012699127197, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[492]', 'EPOCH LOSS:', 0.02531349659697852, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 492, ']')


'Epoch [493] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1317,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008948904770854955,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 5, 8, 4, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 493,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 493, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219307.561582)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219342.050886)
('Worker processing elapsed time: ', 34.48930382728577, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[493]', 'EPOCH LOSS:', 4.1341981240935946, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 493, ']')


'Epoch [494] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1611,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004956315411313942,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 9, 7, 4, 7, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 494,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 494, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219342.056925)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219376.149548)
('Worker processing elapsed time: ', 34.09262299537659, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[494]', 'EPOCH LOSS:', 0.023965568058029057, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 494, ']')


'Epoch [495] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1723,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005080591974877534,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 4, 7, 7, 7, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 495,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 495, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219376.156249)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219410.848469)
('Worker processing elapsed time: ', 34.692219972610474, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[495]', 'EPOCH LOSS:', 0.30833616678377657, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 495, ']')


'Epoch [496] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1228,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016220197753909042,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 6, 4, 7, 5, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 496,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 496, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219410.854129)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219446.447275)
('Worker processing elapsed time: ', 35.59314584732056, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[496]', 'EPOCH LOSS:', 0.025829960053047121, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 496, ']')


'Epoch [497] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 110,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009143076004781075,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 4, 4, 6, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 497,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 497, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219446.453785)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219480.4795)
('Worker processing elapsed time: ', 34.02571511268616, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[497]', 'EPOCH LOSS:', 0.021641654859505491, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 497, ']')


'Epoch [498] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1108,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006067265976551581,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 9, 7, 7, 8, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 498,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 498, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219480.485482)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219515.122641)
('Worker processing elapsed time: ', 34.6371591091156, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[498]', 'EPOCH LOSS:', 963.51950807596677, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 498, ']')


'Epoch [499] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1968,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009642318807696841,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 6, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 499,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 499, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219515.128489)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219549.784931)
('Worker processing elapsed time: ', 34.65644192695618, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[499]', 'EPOCH LOSS:', 0.057141155367666556, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 499, ']')


'Epoch [500] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1126,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004475083875223572,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 500,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 500, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219549.790768)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219584.169705)
('Worker processing elapsed time: ', 34.378937005996704, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[500]', 'EPOCH LOSS:', 0.057542308280334131, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 500, ']')


'Epoch [501] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1699,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008564681632157323,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 8, 8, 6, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 501,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 501, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219584.176254)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219619.430198)
('Worker processing elapsed time: ', 35.2539439201355, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[501]', 'EPOCH LOSS:', 0.06234253054602306, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 501, ']')


'Epoch [502] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1995,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000710618384357611,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 8, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 502,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 502, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219619.436162)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219654.077802)
('Worker processing elapsed time: ', 34.641639947891235, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[502]', 'EPOCH LOSS:', 1.7253670808463639, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 502, ']')


'Epoch [503] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 874,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006621119824927117,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 8, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 503,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 503, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219654.083517)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219688.114396)
('Worker processing elapsed time: ', 34.03087902069092, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[503]', 'EPOCH LOSS:', 0.63722071267676395, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 503, ']')


'Epoch [504] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1981,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009581325820223533,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 7, 6, 9, 8, 8, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 504,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 504, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219688.121438)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219722.364546)
('Worker processing elapsed time: ', 34.24310803413391, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[504]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 504, ']')


'Epoch [505] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1183,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00045863213399411674,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 505,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 505, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219722.370567)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219756.799429)
('Worker processing elapsed time: ', 34.42886185646057, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[505]', 'EPOCH LOSS:', 0.3501449379649289, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 505, ']')


'Epoch [506] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1477,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009211904003176009,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 506,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 506, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219756.805228)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219790.36998)
('Worker processing elapsed time: ', 33.56475210189819, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[506]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 506, ']')


'Epoch [507] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1313,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00010363879978418612,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 4, 9, 7, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 507,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 507, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219790.375913)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219824.871158)
('Worker processing elapsed time: ', 34.4952449798584, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[507]', 'EPOCH LOSS:', 187.67867510504593, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 507, ']')


'Epoch [508] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 135,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009541884155098932,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 8, 5, 5, 9, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 508,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 508, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219824.877772)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219859.586621)
('Worker processing elapsed time: ', 34.70884895324707, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[508]', 'EPOCH LOSS:', 0.41606962430608413, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 508, ']')


'Epoch [509] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 197,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005931088036176047,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 6, 6, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 509,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 509, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219859.59247)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219894.576949)
('Worker processing elapsed time: ', 34.98447895050049, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[509]', 'EPOCH LOSS:', 48.376028959522777, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 509, ']')


'Epoch [510] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1832,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005879502861636268,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 6, 9, 4, 5, 8, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 510,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 510, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219894.582836)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219930.537868)
('Worker processing elapsed time: ', 35.95503211021423, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[510]', 'EPOCH LOSS:', 0.78736241374054883, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 510, ']')


'Epoch [511] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 503,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009530051035571414,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 9, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 511,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 511, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219930.544009)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219965.145506)
('Worker processing elapsed time: ', 34.60149693489075, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[511]', 'EPOCH LOSS:', 0.027248649440270546, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 511, ']')


'Epoch [512] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 885,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002682128851521537,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 6, 7, 9, 9, 8, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 512,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 512, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219965.151856)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494219999.43419)
('Worker processing elapsed time: ', 34.282334089279175, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[512]', 'EPOCH LOSS:', 0.024874190265303926, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 512, ']')


'Epoch [513] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1431,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005385794151259648,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 7, 9, 7, 9, 6, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 513,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 513, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494219999.440943)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220034.485014)
('Worker processing elapsed time: ', 35.04407095909119, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[513]', 'EPOCH LOSS:', 1251.2893349785425, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 513, ']')


'Epoch [514] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 470,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004172060114169011,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 9, 8, 9, 4, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 514,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 514, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220034.490848)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220068.589163)
('Worker processing elapsed time: ', 34.09831500053406, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[514]', 'EPOCH LOSS:', 0.023042311971326255, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 514, ']')


'Epoch [515] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1452,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009580139440111618,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 6, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 515,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 515, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220068.595303)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220103.249379)
('Worker processing elapsed time: ', 34.65407586097717, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[515]', 'EPOCH LOSS:', 0.055823450854875133, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 515, ']')


'Epoch [516] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1704,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007171104104451661,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 9, 9, 4, 6, 6, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 516,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 516, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220103.255173)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220138.247946)
('Worker processing elapsed time: ', 34.99277305603027, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[516]', 'EPOCH LOSS:', 2914.1124422932344, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 516, ']')


'Epoch [517] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 181,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009014577757225218,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 5, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 517,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 517, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220138.254165)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220172.347097)
('Worker processing elapsed time: ', 34.0929319858551, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[517]', 'EPOCH LOSS:', 10.434243748700403, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 517, ']')


'Epoch [518] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1362,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009303680262353602,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 8, 4, 7, 5, 4, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 518,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 518, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220172.352993)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220208.219375)
('Worker processing elapsed time: ', 35.866381883621216, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[518]', 'EPOCH LOSS:', 0.024888070550422633, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 518, ']')


'Epoch [519] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1304,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00022252940523952494,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 7, 7, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 519,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 519, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220208.225771)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220242.636253)
('Worker processing elapsed time: ', 34.41048216819763, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[519]', 'EPOCH LOSS:', 0.3930846353048898, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 519, ']')


'Epoch [520] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1632,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005760937022389284,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 7, 6, 5, 9, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 520,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 520, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220242.643099)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220278.613435)
('Worker processing elapsed time: ', 35.970335960388184, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[520]', 'EPOCH LOSS:', 0.41044244411470276, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 520, ']')


'Epoch [521] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 730,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00010252002541719497,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 8, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 521,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 521, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220278.619218)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220312.292267)
('Worker processing elapsed time: ', 33.673048973083496, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[521]', 'EPOCH LOSS:', 0.024112879044436163, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 521, ']')


'Epoch [522] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 389,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00015952430540855497,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 7, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 522,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 522, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220312.298084)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220346.354527)
('Worker processing elapsed time: ', 34.056442975997925, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[522]', 'EPOCH LOSS:', 4.08621862336192, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 522, ']')


'Epoch [523] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1449,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000195209032854645,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 5, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 523,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 523, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220346.365163)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220380.442085)
('Worker processing elapsed time: ', 34.07692193984985, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[523]', 'EPOCH LOSS:', 163.59754488153078, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 523, ']')


'Epoch [524] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1640,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006861844982324057,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 8, 8, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 524,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 524, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220380.448558)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220414.699229)
('Worker processing elapsed time: ', 34.25067090988159, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[524]', 'EPOCH LOSS:', 136.85911494906966, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 524, ']')


'Epoch [525] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 964,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006660693352313278,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 5, 5, 5, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 525,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 525, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220414.705155)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220448.757268)
('Worker processing elapsed time: ', 34.05211305618286, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[525]', 'EPOCH LOSS:', 0.025200263361949044, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 525, ']')


'Epoch [526] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 157,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000612020007297391,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 7, 9, 8, 8, 4, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 526,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 526, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220448.763696)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220483.001385)
('Worker processing elapsed time: ', 34.23768901824951, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[526]', 'EPOCH LOSS:', 0.018253156381065259, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 526, ']')


'Epoch [527] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1360,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00017074714428994844,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 4, 7, 7, 9, 7, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 527,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 527, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220483.007418)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220519.001758)
('Worker processing elapsed time: ', 35.99434018135071, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[527]', 'EPOCH LOSS:', 137.64407474364668, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 527, ']')


'Epoch [528] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 667,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008368217683761175,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 7, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 528,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 528, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220519.007773)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220552.753185)
('Worker processing elapsed time: ', 33.74541211128235, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[528]', 'EPOCH LOSS:', 0.024836615964249742, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 528, ']')


'Epoch [529] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 926,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005826916373111272,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 9, 8, 5, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 529,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 529, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220552.759701)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220586.612695)
('Worker processing elapsed time: ', 33.852993965148926, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[529]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 529, ']')


'Epoch [530] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1285,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007127428131286136,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 5, 9, 5, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 530,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 530, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220586.618949)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220621.047025)
('Worker processing elapsed time: ', 34.42807602882385, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[530]', 'EPOCH LOSS:', 90.358412616009119, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 530, ']')


'Epoch [531] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 628,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006024993223308628,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 4, 9, 7, 4, 9, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 531,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 531, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220621.053684)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220655.314275)
('Worker processing elapsed time: ', 34.26059103012085, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[531]', 'EPOCH LOSS:', 0.025176939231337039, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 531, ']')


'Epoch [532] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 342,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008835363791303752,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 4, 7, 8, 6, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 532,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 532, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220655.320245)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220689.949915)
('Worker processing elapsed time: ', 34.62966990470886, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[532]', 'EPOCH LOSS:', 1.4441933251377745, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 532, ']')


'Epoch [533] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 826,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003477401532488187,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 7, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 533,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 533, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220689.955841)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220723.995933)
('Worker processing elapsed time: ', 34.04009199142456, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[533]', 'EPOCH LOSS:', 314.64790776519084, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 533, ']')


'Epoch [534] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1519,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008445595188485821,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 6, 9, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 534,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 534, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220724.001899)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220757.86261)
('Worker processing elapsed time: ', 33.860711097717285, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[534]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 534, ']')


'Epoch [535] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 644,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00038816157920492094,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 8, 6, 8, 9, 4, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 535,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 535, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220757.868113)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220792.782689)
('Worker processing elapsed time: ', 34.914576053619385, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[535]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 535, ']')


'Epoch [536] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1615,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006910957691132497,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 5, 7, 4, 4, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 536,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 536, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220792.788848)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220828.287721)
('Worker processing elapsed time: ', 35.49887299537659, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[536]', 'EPOCH LOSS:', 0.56934014052094972, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 536, ']')


'Epoch [537] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 858,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008968086690931465,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 5, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 537,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 537, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220828.293286)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220862.365105)
('Worker processing elapsed time: ', 34.071818828582764, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[537]', 'EPOCH LOSS:', 15.753203320550286, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 537, ']')


'Epoch [538] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1084,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00039183480043784727,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 6, 8, 4, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 538,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 538, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220862.369908)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220896.299244)
('Worker processing elapsed time: ', 33.929335832595825, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[538]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 538, ']')


'Epoch [539] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 719,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006103056574754229,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 7, 9, 4, 8, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 539,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 539, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220896.304082)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220931.934358)
('Worker processing elapsed time: ', 35.63027596473694, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[539]', 'EPOCH LOSS:', 1.8731738398408289, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 539, ']')


'Epoch [540] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1705,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006064788156008218,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 6, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 540,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 540, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220931.939245)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494220965.966958)
('Worker processing elapsed time: ', 34.02771306037903, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[540]', 'EPOCH LOSS:', 279.77583460413911, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 540, ']')


'Epoch [541] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 884,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000988142608533635,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 4, 8, 7, 4, 6, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 541,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 541, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494220965.973217)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221001.90732)
('Worker processing elapsed time: ', 35.93410301208496, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[541]', 'EPOCH LOSS:', 0.024894693176821317, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 541, ']')


'Epoch [542] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1796,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006952680771285893,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 8, 7, 4, 8, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 542,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 542, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221001.915234)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221036.656672)
('Worker processing elapsed time: ', 34.741437911987305, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[542]', 'EPOCH LOSS:', 0.15391071702753858, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 542, ']')


'Epoch [543] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1263,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005908195402744691,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 9, 5, 4, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 543,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 543, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221036.6623)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221070.645105)
('Worker processing elapsed time: ', 33.98280477523804, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[543]', 'EPOCH LOSS:', 0.025766841400926752, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 543, ']')


'Epoch [544] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 691,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008328492507389726,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 9, 9, 8, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 544,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 544, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221070.6503)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221106.004617)
('Worker processing elapsed time: ', 35.35431694984436, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[544]', 'EPOCH LOSS:', 0.024151677166922098, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 544, ']')


'Epoch [545] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1019,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006881744765953406,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 6, 8, 8, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 545,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 545, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221106.010348)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221140.411689)
('Worker processing elapsed time: ', 34.4013409614563, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[545]', 'EPOCH LOSS:', 0.12045800680505676, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 545, ']')


'Epoch [546] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1534,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003006465047788869,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 8, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 546,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 546, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221140.416937)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221174.43297)
('Worker processing elapsed time: ', 34.01603293418884, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[546]', 'EPOCH LOSS:', 367.36880806173826, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 546, ']')


'Epoch [547] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1089,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000336113228549027,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 8, 9, 9, 5, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 547,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 547, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221174.437935)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221208.543895)
('Worker processing elapsed time: ', 34.10595989227295, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[547]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 547, ']')


'Epoch [548] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1709,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009448906578287591,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 5, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 548,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 548, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221208.549641)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221243.154256)
('Worker processing elapsed time: ', 34.604615211486816, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[548]', 'EPOCH LOSS:', 0.021736278017446334, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 548, ']')


'Epoch [549] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1120,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005296395191098958,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 6, 5, 4, 7, 6, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 549,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 549, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221243.160417)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221279.083964)
('Worker processing elapsed time: ', 35.92354702949524, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[549]', 'EPOCH LOSS:', 45.200970204027378, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 549, ']')


'Epoch [550] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1926,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007441445435456439,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 5, 5, 5, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 550,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 550, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221279.089611)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221313.515729)
('Worker processing elapsed time: ', 34.42611789703369, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[550]', 'EPOCH LOSS:', 36.651562949086859, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 550, ']')


'Epoch [551] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 207,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006799356566077764,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 9, 9, 5, 6, 4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 551,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 551, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221313.520626)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221349.408705)
('Worker processing elapsed time: ', 35.888078927993774, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[551]', 'EPOCH LOSS:', 1.7912536700454806, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 551, ']')


'Epoch [552] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 211,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00042592560076199816,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 8, 7, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 552,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 552, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221349.41445)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221383.304281)
('Worker processing elapsed time: ', 33.88983106613159, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[552]', 'EPOCH LOSS:', 0.024729528417382688, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 552, ']')


'Epoch [553] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1606,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007645180360851271,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 4, 7, 7, 5, 8, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 553,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 553, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221383.309292)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221418.282618)
('Worker processing elapsed time: ', 34.973325967788696, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[553]', 'EPOCH LOSS:', 651.8010753620166, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 553, ']')


'Epoch [554] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1645,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003149189926254475,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 5, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 554,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 554, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221418.288772)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221451.917502)
('Worker processing elapsed time: ', 33.628729820251465, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[554]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 554, ']')


'Epoch [555] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1172,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00021841869058132743,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 4, 4, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 555,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 555, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221451.923485)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221486.926388)
('Worker processing elapsed time: ', 35.00290298461914, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[555]', 'EPOCH LOSS:', 79.163794387032297, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 555, ']')


'Epoch [556] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1928,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004015312389557187,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 8, 8, 4, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 556,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 556, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221486.931615)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221520.901565)
('Worker processing elapsed time: ', 33.96994996070862, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[556]', 'EPOCH LOSS:', 0.02315934005272767, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 556, ']')


'Epoch [557] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1301,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009336970565996847,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 6, 7, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 557,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 557, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221520.906745)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221554.66167)
('Worker processing elapsed time: ', 33.7549250125885, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[557]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 557, ']')


'Epoch [558] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1424,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009645101780802986,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 9, 8, 9, 6, 5, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 558,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 558, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221554.667777)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221589.578989)
('Worker processing elapsed time: ', 34.91121196746826, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[558]', 'EPOCH LOSS:', 0.39091215140117641, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 558, ']')


'Epoch [559] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1658,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007899728729714353,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 559,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 559, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221589.584968)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221624.162203)
('Worker processing elapsed time: ', 34.577234983444214, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[559]', 'EPOCH LOSS:', 0.031734697457250528, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 559, ']')


'Epoch [560] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 369,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009658545903936888,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 560,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 560, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221624.167304)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221657.804238)
('Worker processing elapsed time: ', 33.63693404197693, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[560]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 560, ']')


'Epoch [561] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1156,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009068231819718297,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 9, 9, 6, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 561,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 561, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221657.809919)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221692.190845)
('Worker processing elapsed time: ', 34.38092589378357, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[561]', 'EPOCH LOSS:', 872.61576042200602, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 561, ']')


'Epoch [562] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1782,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007802417525804764,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 8, 7, 6, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 562,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 562, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221692.196289)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221727.391749)
('Worker processing elapsed time: ', 35.195459842681885, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[562]', 'EPOCH LOSS:', 0.1257531773898827, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 562, ']')


'Epoch [563] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1179,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009228135414592042,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 8, 5, 9, 5, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 563,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 563, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221727.396745)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221761.510577)
('Worker processing elapsed time: ', 34.113831996917725, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[563]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 563, ']')


'Epoch [564] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 153,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006958283780825813,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 4, 7, 9, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 564,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 564, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221761.515661)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221796.863967)
('Worker processing elapsed time: ', 35.34830594062805, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[564]', 'EPOCH LOSS:', 0.033230071014009613, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 564, ']')


'Epoch [565] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1251,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000973715282112188,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 565,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 565, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221796.869976)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221831.3107)
('Worker processing elapsed time: ', 34.44072389602661, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[565]', 'EPOCH LOSS:', 0.40309323995975538, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 565, ']')


'Epoch [566] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1336,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009955394888730418,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 5, 4, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 566,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 566, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221831.31563)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221865.560706)
('Worker processing elapsed time: ', 34.245075941085815, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[566]', 'EPOCH LOSS:', 0.65534453891856315, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 566, ']')


'Epoch [567] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1736,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009972602223559382,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 567,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 567, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221865.566598)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221899.069834)
('Worker processing elapsed time: ', 33.503236055374146, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[567]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 567, ']')


'Epoch [568] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 291,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006890511556492282,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 5, 5, 4, 6, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 568,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 568, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221899.075799)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221933.652782)
('Worker processing elapsed time: ', 34.5769829750061, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[568]', 'EPOCH LOSS:', 0.17581015302245226, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 568, ']')


'Epoch [569] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1925,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001520481706676817,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 5, 4, 5, 4, 5, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 569,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 569, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221933.659275)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494221968.538404)
('Worker processing elapsed time: ', 34.87912893295288, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[569]', 'EPOCH LOSS:', 0.89315096927940951, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 569, ']')


'Epoch [570] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 230,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008617220663215462,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 5, 8, 8, 8, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 570,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 570, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494221968.543543)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222004.111824)
('Worker processing elapsed time: ', 35.568280935287476, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[570]', 'EPOCH LOSS:', 0.025624447648153314, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 570, ']')


'Epoch [571] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 321,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008521172447006987,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 571,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 571, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222004.117265)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222038.464772)
('Worker processing elapsed time: ', 34.34750699996948, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[571]', 'EPOCH LOSS:', 0.14473186932821419, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 571, ']')


'Epoch [572] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1483,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000248397131420069,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 572,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 572, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222038.469857)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222072.057611)
('Worker processing elapsed time: ', 33.587754011154175, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[572]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 572, ']')


'Epoch [573] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 183,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006050750542537333,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 7, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 573,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 573, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222072.062579)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222106.650543)
('Worker processing elapsed time: ', 34.58796405792236, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[573]', 'EPOCH LOSS:', 0.072637197906030349, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 573, ']')


'Epoch [574] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 202,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00013412223004639976,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 8, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 574,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 574, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222106.655421)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222140.59264)
('Worker processing elapsed time: ', 33.93721890449524, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[574]', 'EPOCH LOSS:', 187.07287729935265, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 574, ']')


'Epoch [575] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1135,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008252419272941777,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 4, 5, 7, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 575,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 575, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222140.598489)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222174.613261)
('Worker processing elapsed time: ', 34.014771938323975, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[575]', 'EPOCH LOSS:', 0.025642958206107781, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 575, ']')


'Epoch [576] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1959,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003954616984389955,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 9, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 576,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 576, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222174.618518)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222208.251803)
('Worker processing elapsed time: ', 33.6332848072052, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[576]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 576, ']')


'Epoch [577] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 845,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009991705294407625,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 5, 5, 5, 8, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 577,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 577, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222208.256853)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222243.843463)
('Worker processing elapsed time: ', 35.586609840393066, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[577]', 'EPOCH LOSS:', 0.023649050611391893, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 577, ']')


'Epoch [578] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1192,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007581220088234745,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 5, 6, 5, 8, 5, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 578,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 578, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222243.84907)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222279.769556)
('Worker processing elapsed time: ', 35.920485973358154, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[578]', 'EPOCH LOSS:', 0.028265045702590436, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 578, ']')


'Epoch [579] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1983,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00025156917817629836,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 8, 6, 5, 9, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 579,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 579, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222279.774884)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222315.395906)
('Worker processing elapsed time: ', 35.62102198600769, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[579]', 'EPOCH LOSS:', 0.042338925178318512, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 579, ']')


'Epoch [580] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 716,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005559497318886609,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 7, 5, 4, 5, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 580,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 580, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222315.401593)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222349.931953)
('Worker processing elapsed time: ', 34.530359983444214, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[580]', 'EPOCH LOSS:', 0.049445950117817765, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 580, ']')


'Epoch [581] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 396,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006983394767443325,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 6, 4, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 581,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 581, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222349.938023)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222384.192085)
('Worker processing elapsed time: ', 34.25406193733215, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[581]', 'EPOCH LOSS:', 98.132734155287508, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 581, ']')


'Epoch [582] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 456,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009501277362296061,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 7, 8, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 582,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 582, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222384.197981)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222419.173573)
('Worker processing elapsed time: ', 34.97559189796448, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[582]', 'EPOCH LOSS:', 0.034989008823244953, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 582, ']')


'Epoch [583] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 901,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000179199242687638,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 8, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 583,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 583, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222419.179297)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222452.800123)
('Worker processing elapsed time: ', 33.62082600593567, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[583]', 'EPOCH LOSS:', 0.024559295058163576, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 583, ']')


'Epoch [584] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1086,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016324723665085976,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 8, 6, 8, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 584,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 584, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222452.806066)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222487.306807)
('Worker processing elapsed time: ', 34.50074100494385, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[584]', 'EPOCH LOSS:', 1.7718376430077634, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 584, ']')


'Epoch [585] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1688,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003092714719850454,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 7, 9, 5, 8, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 585,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 585, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222487.312619)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222521.685906)
('Worker processing elapsed time: ', 34.373286962509155, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[585]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 585, ']')


'Epoch [586] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 432,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00032266069018777716,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 4, 9, 8, 4, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 586,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 586, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222521.69199)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222556.360172)
('Worker processing elapsed time: ', 34.668182134628296, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[586]', 'EPOCH LOSS:', 2738.2893372403137, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 586, ']')


'Epoch [587] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1984,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00044855474932777473,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 587,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 587, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222556.364891)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222590.122075)
('Worker processing elapsed time: ', 33.75718402862549, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[587]', 'EPOCH LOSS:', 5.7040810025763484, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 587, ']')


'Epoch [588] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1166,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001419669096026936,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 5, 6, 6, 5, 5, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 588,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 588, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222590.126917)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222625.070687)
('Worker processing elapsed time: ', 34.94377017021179, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[588]', 'EPOCH LOSS:', 7548.0225298232117, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 588, ']')


'Epoch [589] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1415,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005082403574958308,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 4, 8, 7, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 589,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 589, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222625.075965)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222659.529437)
('Worker processing elapsed time: ', 34.45347213745117, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[589]', 'EPOCH LOSS:', 9.0388403324038773, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 589, ']')


'Epoch [590] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 180,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009869693939488174,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 590,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 590, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222659.534587)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222693.431505)
('Worker processing elapsed time: ', 33.896918058395386, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[590]', 'EPOCH LOSS:', 29.046246861532762, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 590, ']')


'Epoch [591] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1255,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005219353734213005,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 6, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 591,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 591, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222693.437329)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222727.558667)
('Worker processing elapsed time: ', 34.121337890625, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[591]', 'EPOCH LOSS:', 127.8112059378343, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 591, ']')


'Epoch [592] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1674,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007172250804729908,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 6, 9, 8, 9, 8, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 592,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 592, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222727.563775)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222762.53581)
('Worker processing elapsed time: ', 34.97203493118286, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[592]', 'EPOCH LOSS:', 0.037477699044677674, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 592, ']')


'Epoch [593] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1869,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005898618210041359,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 593,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 593, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222762.541648)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222796.91763)
('Worker processing elapsed time: ', 34.37598204612732, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[593]', 'EPOCH LOSS:', 0.025535216248920869, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 593, ']')


'Epoch [594] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1779,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00036268812085405507,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 6, 4, 8, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 594,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 594, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222796.922795)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222830.877014)
('Worker processing elapsed time: ', 33.95421886444092, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[594]', 'EPOCH LOSS:', 0.023817962946586994, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 594, ']')


'Epoch [595] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 314,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008059638576497707,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 9, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 595,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 595, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222830.882078)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222864.935144)
('Worker processing elapsed time: ', 34.05306601524353, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[595]', 'EPOCH LOSS:', 1155.7186456621982, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 595, ']')


'Epoch [596] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1239,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00046030230597283615,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 8, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 596,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 596, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222864.940074)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222898.961082)
('Worker processing elapsed time: ', 34.021008014678955, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[596]', 'EPOCH LOSS:', 120.2832854199017, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 596, ']')


'Epoch [597] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 945,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000638246696301475,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 5, 7, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 597,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 597, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222898.966035)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222933.157527)
('Worker processing elapsed time: ', 34.19149208068848, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[597]', 'EPOCH LOSS:', 410.94503947439597, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 597, ']')


'Epoch [598] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 997,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006980187668991706,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 6, 4, 9, 7, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 598,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 598, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222933.163052)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494222967.880375)
('Worker processing elapsed time: ', 34.7173228263855, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[598]', 'EPOCH LOSS:', 0.15130863382602297, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 598, ']')


'Epoch [599] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1231,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009782972618357691,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 5, 5, 8, 9, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 599,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 599, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494222967.886661)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223002.582192)
('Worker processing elapsed time: ', 34.69553089141846, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[599]', 'EPOCH LOSS:', 1639.7735149601551, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 599, ']')


'Epoch [600] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1683,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005775691766766352,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 8, 7, 9, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 600,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 600, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223002.587176)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223037.819438)
('Worker processing elapsed time: ', 35.23226189613342, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[600]', 'EPOCH LOSS:', 0.021433857503619012, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 600, ']')


'Epoch [601] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 129,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007201554795898583,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 9, 7, 4, 6, 8, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 601,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 601, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223037.824482)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223072.036314)
('Worker processing elapsed time: ', 34.21183204650879, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[601]', 'EPOCH LOSS:', 0.022922115950600987, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 601, ']')


'Epoch [602] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 330,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000869006390802054,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 5, 8, 5, 8, 4, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 602,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 602, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223072.042102)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223107.90127)
('Worker processing elapsed time: ', 35.85916781425476, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[602]', 'EPOCH LOSS:', 0.024255369212927091, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 602, ']')


'Epoch [603] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1660,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023874597584965852,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 5, 5, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 603,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 603, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223107.906584)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223142.950697)
('Worker processing elapsed time: ', 35.04411292076111, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[603]', 'EPOCH LOSS:', 1.9976683606456989, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 603, ']')


'Epoch [604] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1382,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002740826778412185,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 8, 6, 7, 9, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 604,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 604, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223142.955754)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223178.599717)
('Worker processing elapsed time: ', 35.64396286010742, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[604]', 'EPOCH LOSS:', 0.023888019963507208, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 604, ']')


'Epoch [605] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1421,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004067778620172658,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 7, 6, 8, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 605,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 605, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223178.606238)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223212.53324)
('Worker processing elapsed time: ', 33.92700219154358, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[605]', 'EPOCH LOSS:', 0.024530717165721128, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 605, ']')


'Epoch [606] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 128,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00031712920691260194,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 9, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 606,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 606, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223212.538196)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223246.531689)
('Worker processing elapsed time: ', 33.99349284172058, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[606]', 'EPOCH LOSS:', 55.888673590703959, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 606, ']')


'Epoch [607] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 695,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002639693628691728,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 4, 7, 4, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 607,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 607, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223246.536686)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223280.550462)
('Worker processing elapsed time: ', 34.01377606391907, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[607]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 607, ']')


'Epoch [608] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1134,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007206218731450143,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 6, 6, 6, 8, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 608,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 608, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223280.556571)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223314.78722)
('Worker processing elapsed time: ', 34.2306489944458, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[608]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 608, ']')


'Epoch [609] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 263,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006755713896035773,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 9, 9, 8, 7, 4, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 609,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 609, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223314.793365)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223349.651211)
('Worker processing elapsed time: ', 34.85784602165222, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[609]', 'EPOCH LOSS:', 0.62374715310445394, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 609, ']')


'Epoch [610] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 671,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005879890599013053,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 610,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 610, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223349.656045)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223383.530486)
('Worker processing elapsed time: ', 33.874441146850586, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[610]', 'EPOCH LOSS:', 56.909193258092522, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 610, ']')


'Epoch [611] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1423,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009329150040222856,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 7, 7, 9, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 611,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 611, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223383.53574)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223417.938045)
('Worker processing elapsed time: ', 34.402305126190186, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[611]', 'EPOCH LOSS:', 0.5135027976907226, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 611, ']')


'Epoch [612] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 235,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00047792633082092213,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 4, 7, 5, 5, 4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 612,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 612, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223417.94391)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223452.825412)
('Worker processing elapsed time: ', 34.88150215148926, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[612]', 'EPOCH LOSS:', 0.10054800987974838, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 612, ']')


'Epoch [613] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 994,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006966471134178101,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 4, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 613,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 613, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223452.830591)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223486.511739)
('Worker processing elapsed time: ', 33.681148052215576, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[613]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 613, ']')


'Epoch [614] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 610,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007094018297299906,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 6, 4, 7, 4, 6, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 614,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 614, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223486.518139)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223522.331915)
('Worker processing elapsed time: ', 35.81377601623535, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[614]', 'EPOCH LOSS:', 6.9751399542089576, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 614, ']')


'Epoch [615] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1721,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005907635288605027,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 9, 5, 9, 7, 9, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 615,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 615, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223522.337403)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223556.615179)
('Worker processing elapsed time: ', 34.27777600288391, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[615]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 615, ']')


'Epoch [616] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1622,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00041562801736992193,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 8, 4, 6, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 616,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 616, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223556.621275)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223590.564162)
('Worker processing elapsed time: ', 33.9428870677948, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[616]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 616, ']')


'Epoch [617] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 180,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00031958201335039447,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 4, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 617,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 617, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223590.569739)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223624.367957)
('Worker processing elapsed time: ', 33.79821801185608, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[617]', 'EPOCH LOSS:', 0.025592807824642967, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 617, ']')


'Epoch [618] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1153,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004440728434562854,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 8, 8, 7, 8, 9, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 618,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 618, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223624.37404)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223658.610879)
('Worker processing elapsed time: ', 34.236839056015015, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[618]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 618, ']')


'Epoch [619] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1896,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00041176355989532823,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 9, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 619,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 619, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223658.616873)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223693.261898)
('Worker processing elapsed time: ', 34.64502501487732, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[619]', 'EPOCH LOSS:', 0.051812934195311931, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 619, ']')


'Epoch [620] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1218,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006311733478070332,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 8, 5, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 620,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 620, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223693.267448)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223728.211212)
('Worker processing elapsed time: ', 34.943763971328735, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[620]', 'EPOCH LOSS:', 0.039312064411004599, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 620, ']')


'Epoch [621] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1173,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00059769765432381,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 4, 5, 6, 4, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 621,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 621, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223728.217513)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223762.210906)
('Worker processing elapsed time: ', 33.99339294433594, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[621]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 621, ']')


'Epoch [622] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 494,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000808641325251589,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 4, 4, 7, 7, 6, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 622,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 622, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223762.216417)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223798.180677)
('Worker processing elapsed time: ', 35.96425986289978, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[622]', 'EPOCH LOSS:', 0.024067249772520966, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 622, ']')


'Epoch [623] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 352,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009096522294506676,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 5, 4, 4, 8, 9, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 623,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 623, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223798.186257)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223832.38898)
('Worker processing elapsed time: ', 34.202723026275635, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[623]', 'EPOCH LOSS:', 0.023674330203467944, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 623, ']')


'Epoch [624] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1397,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00038787746815842307,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 8, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 624,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 624, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223832.393917)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223867.000752)
('Worker processing elapsed time: ', 34.60683488845825, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[624]', 'EPOCH LOSS:', 1.4038555215657162, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 624, ']')


'Epoch [625] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 824,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00036894629029037625,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 4, 6, 9, 6, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 625,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 625, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223867.005686)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223901.179229)
('Worker processing elapsed time: ', 34.173542976379395, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[625]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 625, ']')


'Epoch [626] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1858,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006010279564588853,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 6, 8, 4, 8, 6, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 626,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 626, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223901.184625)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223935.432644)
('Worker processing elapsed time: ', 34.248018980026245, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[626]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 626, ']')


'Epoch [627] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1926,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00014567922288258512,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 5, 6, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 627,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 627, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223935.43778)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494223970.438778)
('Worker processing elapsed time: ', 35.00099802017212, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[627]', 'EPOCH LOSS:', 0.78263982021987888, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 627, ']')


'Epoch [628] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1034,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007863595122695761,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 5, 4, 7, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 628,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 628, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494223970.444967)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224004.441936)
('Worker processing elapsed time: ', 33.99696898460388, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[628]', 'EPOCH LOSS:', 0.025157087275428498, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 628, ']')


'Epoch [629] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1249,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007033165301219783,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 5, 7, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 629,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 629, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224004.447597)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224038.300226)
('Worker processing elapsed time: ', 33.85262894630432, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[629]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 629, ']')


'Epoch [630] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 694,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00020402278495160067,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 4, 9, 6, 5, 9, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 630,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 630, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224038.305111)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224072.555232)
('Worker processing elapsed time: ', 34.250121116638184, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[630]', 'EPOCH LOSS:', 0.024276845568719958, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 630, ']')


'Epoch [631] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 865,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00019465408097569116,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 7, 6, 6, 7, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 631,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 631, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224072.560321)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224108.216808)
('Worker processing elapsed time: ', 35.65648698806763, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[631]', 'EPOCH LOSS:', 4.198692941111327, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 631, ']')


'Epoch [632] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1977,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002510940132967054,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 6, 8, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 632,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 632, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224108.222302)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224142.541505)
('Worker processing elapsed time: ', 34.31920313835144, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[632]', 'EPOCH LOSS:', 4906.0480172205635, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 632, ']')


'Epoch [633] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 401,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00036681190468817646,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 5, 9, 9, 6, 4, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 633,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 633, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224142.547734)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224178.408649)
('Worker processing elapsed time: ', 35.86091494560242, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[633]', 'EPOCH LOSS:', 0.32633652780605205, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 633, ']')


'Epoch [634] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1345,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007430584081819691,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 9, 6, 9, 6, 8, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 634,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 634, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224178.41455)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224213.437191)
('Worker processing elapsed time: ', 35.02264094352722, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[634]', 'EPOCH LOSS:', 0.7528766068772047, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 634, ']')


'Epoch [635] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 721,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007694201965739909,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 5, 8, 9, 7, 7, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 635,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 635, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224213.4426)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224249.357284)
('Worker processing elapsed time: ', 35.91468405723572, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[635]', 'EPOCH LOSS:', 0.024542294739621308, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 635, ']')


'Epoch [636] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1941,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008076665680028434,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 5, 7, 8, 9, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 636,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 636, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224249.362539)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224284.001384)
('Worker processing elapsed time: ', 34.63884496688843, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[636]', 'EPOCH LOSS:', 0.23774496092831829, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 636, ']')


'Epoch [637] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 321,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00021290753777734493,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 637,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 637, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224284.007121)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224317.822801)
('Worker processing elapsed time: ', 33.81568002700806, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[637]', 'EPOCH LOSS:', 60.789176930938119, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 637, ']')


'Epoch [638] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1800,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007309340869143231,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 6, 4, 8, 8, 4, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 638,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 638, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224317.828287)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224352.076032)
('Worker processing elapsed time: ', 34.24774503707886, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[638]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 638, ']')


'Epoch [639] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1740,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002447744960736841,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 639,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 639, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224352.08119)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224385.591806)
('Worker processing elapsed time: ', 33.510615825653076, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[639]', 'EPOCH LOSS:', 0.023945412336271284, 'BEST LOSS:', 0.015198407588926106)
('END OF Optimizer EPOCH =====================>>[', 639, ']')


'Epoch [640] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 415,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005254742094714339,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 8, 8, 9, 6, 4, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 640,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 640, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224385.597395)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224421.573027)
('Worker processing elapsed time: ', 35.97563195228577, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[640]', 'EPOCH LOSS:', 0.010035713448830577, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 640, ']')


'Epoch [641] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 155,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00036294201773688314,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 5, 8, 8, 7, 9, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 641,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 641, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224421.578932)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224457.462907)
('Worker processing elapsed time: ', 35.8839750289917, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[641]', 'EPOCH LOSS:', 3.9673847999711671, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 641, ']')


'Epoch [642] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1462,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000529863389013166,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 8, 4, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 642,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 642, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224457.468945)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224491.344868)
('Worker processing elapsed time: ', 33.8759229183197, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[642]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 642, ']')


'Epoch [643] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1980,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004563784380322715,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 8, 4, 6, 7, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 643,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 643, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224491.349608)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224525.515278)
('Worker processing elapsed time: ', 34.16567015647888, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[643]', 'EPOCH LOSS:', 0.023574894841056133, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 643, ']')


'Epoch [644] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1240,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007834295226677376,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 6, 4, 9, 5, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 644,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 644, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224525.521436)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224561.044779)
('Worker processing elapsed time: ', 35.523343086242676, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[644]', 'EPOCH LOSS:', 0.026547164508928554, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 644, ']')


'Epoch [645] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 745,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004933046513059939,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 9, 7, 6, 4, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 645,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 645, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224561.050571)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224595.727216)
('Worker processing elapsed time: ', 34.676645040512085, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[645]', 'EPOCH LOSS:', 2.8568618512192181, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 645, ']')


'Epoch [646] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1199,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006288326195148814,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 6, 5, 6, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 646,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 646, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224595.732311)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224630.983527)
('Worker processing elapsed time: ', 35.25121593475342, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[646]', 'EPOCH LOSS:', 0.026227514533066553, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 646, ']')


'Epoch [647] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1538,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000489286355897042,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 9, 7, 8, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 647,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 647, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224630.988897)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224666.333987)
('Worker processing elapsed time: ', 35.34508991241455, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[647]', 'EPOCH LOSS:', 0.39880795313511364, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 647, ']')


'Epoch [648] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 437,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005579859321820739,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 648,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 648, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224666.339462)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224700.238493)
('Worker processing elapsed time: ', 33.899030923843384, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[648]', 'EPOCH LOSS:', 52.984403183295903, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 648, ']')


'Epoch [649] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 913,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006322683166204911,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 5, 7, 4, 9, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 649,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 649, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224700.243474)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224734.459284)
('Worker processing elapsed time: ', 34.2158100605011, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[649]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 649, ']')


'Epoch [650] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1065,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00015677142039796665,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 7, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 650,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 650, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224734.464336)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224768.548836)
('Worker processing elapsed time: ', 34.0845000743866, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[650]', 'EPOCH LOSS:', 47.119325746586824, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 650, ']')


'Epoch [651] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1488,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000697217189961006,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 651,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 651, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224768.554076)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224802.832821)
('Worker processing elapsed time: ', 34.27874493598938, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[651]', 'EPOCH LOSS:', 0.085582551822204866, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 651, ']')


'Epoch [652] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 386,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009531182166935823,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 5, 8, 9, 4, 4, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 652,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 652, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224802.839)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224838.759229)
('Worker processing elapsed time: ', 35.92022895812988, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[652]', 'EPOCH LOSS:', 0.034668025326640511, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 652, ']')


'Epoch [653] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 658,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009054386781002416,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 653,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 653, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224838.76517)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224872.65004)
('Worker processing elapsed time: ', 33.88486981391907, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[653]', 'EPOCH LOSS:', 0.19700672323859567, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 653, ']')


'Epoch [654] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1297,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00034876466533468654,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 9, 4, 4, 7, 7, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 654,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 654, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224872.656035)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224906.863518)
('Worker processing elapsed time: ', 34.2074830532074, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[654]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 654, ']')


'Epoch [655] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 283,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007061167186855101,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 5, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 655,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 655, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224906.868703)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224940.614407)
('Worker processing elapsed time: ', 33.74570417404175, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[655]', 'EPOCH LOSS:', 0.1610035571510389, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 655, ']')


'Epoch [656] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 117,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00047431456945929156,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 6, 8, 4, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 656,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 656, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224940.619597)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494224974.595492)
('Worker processing elapsed time: ', 33.975894927978516, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[656]', 'EPOCH LOSS:', 0.020313495991164957, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 656, ']')


'Epoch [657] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 910,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002769281708537311,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 4, 9, 4, 4, 9, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 657,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 657, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494224974.600483)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225010.510491)
('Worker processing elapsed time: ', 35.9100079536438, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[657]', 'EPOCH LOSS:', 35.438551047046744, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 657, ']')


'Epoch [658] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1642,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007189989237956279,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 8, 4, 6, 8, 8, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 658,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 658, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225010.516518)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225045.508312)
('Worker processing elapsed time: ', 34.9917938709259, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[658]', 'EPOCH LOSS:', 36.144051683559518, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 658, ']')


'Epoch [659] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 493,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004092021185377008,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 9, 9, 8, 7, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 659,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 659, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225045.51443)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225080.224487)
('Worker processing elapsed time: ', 34.71005702018738, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[659]', 'EPOCH LOSS:', 3.3136981824839702, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 659, ']')


'Epoch [660] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1672,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007407886991601972,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 4, 8, 5, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 660,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 660, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225080.229884)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225114.282432)
('Worker processing elapsed time: ', 34.05254817008972, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[660]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 660, ']')


'Epoch [661] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1484,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004926796197018589,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 661,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 661, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225114.287557)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225148.142369)
('Worker processing elapsed time: ', 33.854812145233154, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[661]', 'EPOCH LOSS:', 1.3191661420217136, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 661, ']')


'Epoch [662] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 178,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003723088596770623,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 662,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 662, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225148.14794)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225182.527315)
('Worker processing elapsed time: ', 34.379374980926514, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[662]', 'EPOCH LOSS:', 0.062581964814782357, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 662, ']')


'Epoch [663] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 209,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005619890548320775,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 7, 6, 6, 7, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 663,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 663, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225182.532796)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225217.206658)
('Worker processing elapsed time: ', 34.67386198043823, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[663]', 'EPOCH LOSS:', 8466.387276938267, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 663, ']')


'Epoch [664] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 206,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006927536071160692,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 9, 5, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 664,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 664, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225217.211921)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225252.252937)
('Worker processing elapsed time: ', 35.04101610183716, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[664]', 'EPOCH LOSS:', 26.52699906626982, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 664, ']')


'Epoch [665] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1277,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005665769706956684,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 7, 8, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 665,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 665, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225252.258507)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225286.531142)
('Worker processing elapsed time: ', 34.272634983062744, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[665]', 'EPOCH LOSS:', 1.3015251832205079, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 665, ']')


'Epoch [666] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 408,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000863892531982288,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 9, 7, 4, 7, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 666,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 666, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225286.535951)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225321.13176)
('Worker processing elapsed time: ', 34.59580898284912, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[666]', 'EPOCH LOSS:', 1.1394803067434187, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 666, ']')


'Epoch [667] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 481,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00025029242033113117,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 6, 8, 4, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 667,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 667, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225321.137565)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225355.645034)
('Worker processing elapsed time: ', 34.507469177246094, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[667]', 'EPOCH LOSS:', 0.22692898933776243, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 667, ']')


'Epoch [668] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1402,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005276415634393402,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 5, 7, 4, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 668,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 668, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225355.650266)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225390.953988)
('Worker processing elapsed time: ', 35.30372214317322, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[668]', 'EPOCH LOSS:', 0.29564520176934528, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 668, ']')


'Epoch [669] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 684,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00018085615099294706,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 9, 8, 8, 7, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 669,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 669, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225390.959228)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225425.051362)
('Worker processing elapsed time: ', 34.09213399887085, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[669]', 'EPOCH LOSS:', 0.023984217019765882, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 669, ']')


'Epoch [670] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1017,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00027173137175804155,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 4, 8, 9, 6, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 670,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 670, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225425.056551)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225460.638779)
('Worker processing elapsed time: ', 35.58222794532776, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[670]', 'EPOCH LOSS:', 0.055442470140663794, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 670, ']')


'Epoch [671] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 532,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005001884482682199,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 671,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 671, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225460.644677)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225495.055)
('Worker processing elapsed time: ', 34.41032314300537, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[671]', 'EPOCH LOSS:', 0.034435365879761565, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 671, ']')


'Epoch [672] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1214,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003982627071268375,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 9, 6, 6, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 672,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 672, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225495.061031)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225530.342331)
('Worker processing elapsed time: ', 35.28129982948303, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[672]', 'EPOCH LOSS:', 0.026871555940266061, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 672, ']')


'Epoch [673] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 404,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003026881083394682,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 9, 9, 8, 9, 8, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 673,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 673, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225530.348637)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225566.306424)
('Worker processing elapsed time: ', 35.95778679847717, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[673]', 'EPOCH LOSS:', 0.024613535068047331, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 673, ']')


'Epoch [674] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 288,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006284536010484537,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 8, 5, 6, 6, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 674,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 674, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225566.312476)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225601.787805)
('Worker processing elapsed time: ', 35.47532916069031, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[674]', 'EPOCH LOSS:', 0.023763071915715181, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 674, ']')


'Epoch [675] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1475,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001334708427334079,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 4, 8, 6, 4, 8, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 675,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 675, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225601.792773)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225636.818587)
('Worker processing elapsed time: ', 35.025814056396484, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[675]', 'EPOCH LOSS:', 0.35967141138645831, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 675, ']')


'Epoch [676] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 111,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002626370423531928,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 7, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 676,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 676, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225636.823711)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225670.873032)
('Worker processing elapsed time: ', 34.04932117462158, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[676]', 'EPOCH LOSS:', 5768.1010940502929, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 676, ']')


'Epoch [677] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 151,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006381007873804111,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 9, 8, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 677,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 677, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225670.877929)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225704.722448)
('Worker processing elapsed time: ', 33.84451913833618, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[677]', 'EPOCH LOSS:', 0.024689156099073434, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 677, ']')


'Epoch [678] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 283,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008574323120278695,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 7, 5, 8, 9, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 678,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 678, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225704.728675)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225738.832903)
('Worker processing elapsed time: ', 34.104228019714355, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[678]', 'EPOCH LOSS:', 0.023043712947544058, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 678, ']')


'Epoch [679] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1124,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006738036577073683,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 9, 6, 5, 5, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 679,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 679, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225738.838368)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225772.96286)
('Worker processing elapsed time: ', 34.124492168426514, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[679]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 679, ']')


'Epoch [680] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 200,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006841423881853035,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 680,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 680, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225772.967804)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225776.703859)
('Worker processing elapsed time: ', 3.7360551357269287, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[680]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 680, ']')


'Epoch [681] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 457,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007090462148715284,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 6, 4, 9, 5, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 681,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 681, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225776.709151)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225810.837602)
('Worker processing elapsed time: ', 34.128450870513916, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[681]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 681, ']')


'Epoch [682] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 947,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006852715520426834,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 4, 7, 5, 4, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 682,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 682, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225810.843561)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225846.474307)
('Worker processing elapsed time: ', 35.63074612617493, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[682]', 'EPOCH LOSS:', 0.028599216502302641, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 682, ']')


'Epoch [683] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1780,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008475622993385478,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 683,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 683, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225846.479418)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225880.846148)
('Worker processing elapsed time: ', 34.366729974746704, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[683]', 'EPOCH LOSS:', 0.29514665597527134, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 683, ']')


'Epoch [684] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1379,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002748696964386642,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 7, 9, 4, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 684,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 684, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225880.851888)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225916.411538)
('Worker processing elapsed time: ', 35.55964994430542, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[684]', 'EPOCH LOSS:', 0.040198125119355041, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 684, ']')


'Epoch [685] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1759,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005733329827835487,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 4, 7, 7, 9, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 685,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 685, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225916.416643)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225951.244316)
('Worker processing elapsed time: ', 34.8276731967926, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[685]', 'EPOCH LOSS:', 0.98675053297804194, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 685, ']')


'Epoch [686] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1005,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000495829983258206,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 686,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 686, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225951.249801)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494225985.111827)
('Worker processing elapsed time: ', 33.86202597618103, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[686]', 'EPOCH LOSS:', 4.4776080861238139, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 686, ']')


'Epoch [687] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 913,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009350499803659345,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 687,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 687, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494225985.117055)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226018.658759)
('Worker processing elapsed time: ', 33.541704177856445, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[687]', 'EPOCH LOSS:', 0.024337864437331672, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 687, ']')


'Epoch [688] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1949,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004765942119177995,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 4, 4, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 688,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 688, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226018.663757)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226052.582708)
('Worker processing elapsed time: ', 33.91895079612732, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[688]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 688, ']')


'Epoch [689] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 126,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00030661894054590357,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 7, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 689,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 689, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226052.588465)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226086.219088)
('Worker processing elapsed time: ', 33.63062310218811, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[689]', 'EPOCH LOSS:', 0.023395671051824125, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 689, ']')


'Epoch [690] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 401,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000849303905721578,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 4, 5, 7, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 690,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 690, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226086.224107)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226120.246797)
('Worker processing elapsed time: ', 34.02269005775452, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[690]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 690, ']')


'Epoch [691] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1469,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000963220640176818,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 8, 9, 5, 8, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 691,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 691, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226120.252667)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226155.870426)
('Worker processing elapsed time: ', 35.617758989334106, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[691]', 'EPOCH LOSS:', 0.040460148679609786, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 691, ']')


'Epoch [692] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 312,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004986812581541914,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 9, 8, 9, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 692,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 692, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226155.876415)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226190.269965)
('Worker processing elapsed time: ', 34.39354991912842, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[692]', 'EPOCH LOSS:', 0.11357657155040683, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 692, ']')


'Epoch [693] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1270,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009215140160102023,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 9, 7, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 693,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 693, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226190.27484)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226224.545919)
('Worker processing elapsed time: ', 34.27107882499695, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[693]', 'EPOCH LOSS:', 13.493815194320058, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 693, ']')


'Epoch [694] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 967,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007256679435893371,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 8, 8, 8, 9, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 694,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 694, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226224.551031)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226258.574642)
('Worker processing elapsed time: ', 34.02361083030701, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[694]', 'EPOCH LOSS:', 0.025143052854210552, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 694, ']')


'Epoch [695] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1622,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006996093864102369,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 6, 4, 7, 4, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 695,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 695, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226258.579584)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226292.7381)
('Worker processing elapsed time: ', 34.15851616859436, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[695]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 695, ']')


'Epoch [696] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 957,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008860503852237616,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 5, 6, 9, 8, 4, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 696,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 696, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226292.743338)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226326.989505)
('Worker processing elapsed time: ', 34.246166944503784, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[696]', 'EPOCH LOSS:', 0.025345386383174573, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 696, ']')


'Epoch [697] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 367,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006043328764018968,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 5, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 697,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 697, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226326.994537)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226360.659213)
('Worker processing elapsed time: ', 33.66467595100403, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[697]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 697, ']')


'Epoch [698] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 908,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00030911764863204806,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 6, 9, 8, 9, 7, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 698,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 698, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226360.664409)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226396.541634)
('Worker processing elapsed time: ', 35.877225160598755, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[698]', 'EPOCH LOSS:', 0.053506317570313822, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 698, ']')


'Epoch [699] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 457,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00018655660086139064,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 699,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 699, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226396.546616)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226430.873165)
('Worker processing elapsed time: ', 34.32654881477356, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[699]', 'EPOCH LOSS:', 1.5700477343904939, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 699, ']')


'Epoch [700] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 252,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004100575583213231,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 8, 7, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 700,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 700, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226430.878239)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226464.644943)
('Worker processing elapsed time: ', 33.766704082489014, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[700]', 'EPOCH LOSS:', 0.023421614870912367, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 700, ']')


'Epoch [701] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1471,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008104482863108026,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 8, 4, 9, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 701,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 701, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226464.649678)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226499.997449)
('Worker processing elapsed time: ', 35.34777092933655, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[701]', 'EPOCH LOSS:', 0.024128671069926427, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 701, ']')


'Epoch [702] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 836,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000168123946202034,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 702,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 702, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226500.002735)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226534.365472)
('Worker processing elapsed time: ', 34.36273717880249, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[702]', 'EPOCH LOSS:', 24.936999357707005, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 702, ']')


'Epoch [703] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 790,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009404484147316184,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 4, 6, 4, 5, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 703,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 703, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226534.370611)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226569.0034)
('Worker processing elapsed time: ', 34.63278913497925, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[703]', 'EPOCH LOSS:', 16.246766238870062, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 703, ']')


'Epoch [704] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 950,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000646818933538082,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 7, 8, 8, 8, 4, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 704,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 704, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226569.008826)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226604.863323)
('Worker processing elapsed time: ', 35.85449695587158, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[704]', 'EPOCH LOSS:', 0.37217635419528949, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 704, ']')


'Epoch [705] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1031,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007846249899227577,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 6, 6, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 705,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 705, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226604.86866)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226638.708292)
('Worker processing elapsed time: ', 33.83963203430176, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[705]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 705, ']')


'Epoch [706] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 605,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009761575044427082,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 6, 8, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 706,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 706, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226638.713568)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226672.983615)
('Worker processing elapsed time: ', 34.2700469493866, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[706]', 'EPOCH LOSS:', 199.84300727079713, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 706, ']')


'Epoch [707] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1757,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007844682952735316,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 4, 4, 6, 9, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 707,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 707, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226672.988738)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226708.618718)
('Worker processing elapsed time: ', 35.629979848861694, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[707]', 'EPOCH LOSS:', 0.023896536003255635, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 707, ']')


'Epoch [708] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1773,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008403913609997776,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 9, 6, 8, 7, 5, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 708,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 708, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226708.623562)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226742.873667)
('Worker processing elapsed time: ', 34.250104904174805, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[708]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 708, ']')


'Epoch [709] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1056,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005835420134059693,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 7, 8, 7, 8, 6, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 709,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 709, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226742.87853)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226777.784502)
('Worker processing elapsed time: ', 34.90597200393677, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[709]', 'EPOCH LOSS:', 3.4564801045595179, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 709, ']')


'Epoch [710] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 519,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000943812631676243,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 8, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 710,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 710, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226777.78988)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226811.558729)
('Worker processing elapsed time: ', 33.76884889602661, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[710]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 710, ']')


'Epoch [711] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 187,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006332670363376181,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 4, 4, 6, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 711,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 711, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226811.564815)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226845.516157)
('Worker processing elapsed time: ', 33.9513418674469, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[711]', 'EPOCH LOSS:', 0.026243939485980489, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 711, ']')


'Epoch [712] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1168,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016787049157126815,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 5, 7, 9, 7, 6, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 712,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 712, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226845.521314)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226881.466938)
('Worker processing elapsed time: ', 35.945624113082886, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[712]', 'EPOCH LOSS:', 0.039545149032772488, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 712, ']')


'Epoch [713] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 758,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006664544666988532,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 4, 7, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 713,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 713, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226881.473015)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226915.318126)
('Worker processing elapsed time: ', 33.84511089324951, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[713]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 713, ']')


'Epoch [714] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 634,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009725376050855272,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 8, 4, 7, 7, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 714,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 714, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226915.324741)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226949.912935)
('Worker processing elapsed time: ', 34.588194131851196, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[714]', 'EPOCH LOSS:', 0.041832196074723119, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 714, ']')


'Epoch [715] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1452,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00011454074233128411,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 5, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 715,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 715, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226949.918985)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494226983.75008)
('Worker processing elapsed time: ', 33.83109521865845, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[715]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 715, ']')


'Epoch [716] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1854,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002542031063611876,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 7, 9, 8, 7, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 716,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 716, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494226983.755564)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227019.384431)
('Worker processing elapsed time: ', 35.62886691093445, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[716]', 'EPOCH LOSS:', 0.3439742529638678, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 716, ']')


'Epoch [717] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1350,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009198125775935338,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 717,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 717, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227019.389343)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227052.881648)
('Worker processing elapsed time: ', 33.4923050403595, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[717]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 717, ']')


'Epoch [718] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 677,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00018056057198543164,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 9, 6, 5, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 718,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 718, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227052.88721)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227088.124619)
('Worker processing elapsed time: ', 35.23740911483765, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[718]', 'EPOCH LOSS:', 0.046189926437908542, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 718, ']')


'Epoch [719] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 127,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00041232656993123304,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 7, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 719,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 719, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227088.129787)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227122.80652)
('Worker processing elapsed time: ', 34.67673301696777, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[719]', 'EPOCH LOSS:', 0.089638122268538373, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 719, ']')


'Epoch [720] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 424,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009492216678972296,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 7, 7, 8, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 720,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 720, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227122.812757)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227158.169123)
('Worker processing elapsed time: ', 35.35636591911316, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[720]', 'EPOCH LOSS:', 0.14250205164143045, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 720, ']')


'Epoch [721] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 623,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005369578864759936,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 6, 9, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 721,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 721, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227158.174198)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227191.964796)
('Worker processing elapsed time: ', 33.79059815406799, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[721]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 721, ']')


'Epoch [722] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1782,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00047760186224094186,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 722,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 722, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227191.97017)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227225.515332)
('Worker processing elapsed time: ', 33.545161962509155, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[722]', 'EPOCH LOSS:', 0.023814581707955605, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 722, ']')


'Epoch [723] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 947,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007353074752330469,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 5, 7, 7, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 723,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 723, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227225.52025)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227260.016273)
('Worker processing elapsed time: ', 34.49602293968201, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[723]', 'EPOCH LOSS:', 28.032207323130784, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 723, ']')


'Epoch [724] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 501,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005940438709971641,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 5, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 724,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 724, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227260.022073)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227294.091017)
('Worker processing elapsed time: ', 34.06894397735596, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[724]', 'EPOCH LOSS:', 7.0281521283203983, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 724, ']')


'Epoch [725] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 869,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00045120325037291387,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 7, 8, 4, 8, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 725,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 725, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227294.096231)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227328.772994)
('Worker processing elapsed time: ', 34.67676305770874, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[725]', 'EPOCH LOSS:', 1.2611911288786779, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 725, ']')


'Epoch [726] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1915,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009162395873477415,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 7, 5, 7, 5, 4, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 726,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 726, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227328.778363)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227363.80969)
('Worker processing elapsed time: ', 35.03132700920105, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[726]', 'EPOCH LOSS:', 36.682970778509876, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 726, ']')


'Epoch [727] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 645,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000447822530131262,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 9, 9, 5, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 727,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 727, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227363.814892)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227398.268976)
('Worker processing elapsed time: ', 34.45408391952515, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[727]', 'EPOCH LOSS:', 0.31415794010107612, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 727, ']')


'Epoch [728] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 794,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005567160355578016,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 9, 5, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 728,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 728, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227398.273931)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227433.16358)
('Worker processing elapsed time: ', 34.88964891433716, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[728]', 'EPOCH LOSS:', 0.06450772007108746, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 728, ']')


'Epoch [729] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 979,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005898718250369574,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 5, 6, 8, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 729,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 729, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227433.169526)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227467.616819)
('Worker processing elapsed time: ', 34.44729280471802, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[729]', 'EPOCH LOSS:', 0.050828423059663889, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 729, ']')


'Epoch [730] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 533,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006119060497257997,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 9, 8, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 730,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 730, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227467.622262)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227501.90762)
('Worker processing elapsed time: ', 34.28535795211792, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[730]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 730, ']')


'Epoch [731] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1322,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00034460809441992884,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 8, 8, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 731,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 731, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227501.913011)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227535.743024)
('Worker processing elapsed time: ', 33.830013036727905, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[731]', 'EPOCH LOSS:', 0.025099105710427637, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 731, ']')


'Epoch [732] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1866,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00021055163089483443,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 732,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 732, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227535.748015)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227570.079508)
('Worker processing elapsed time: ', 34.33149313926697, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[732]', 'EPOCH LOSS:', 0.076637004226296448, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 732, ']')


'Epoch [733] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1021,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003804364237680271,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 6, 5, 6, 7, 8, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 733,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 733, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227570.085624)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227606.051688)
('Worker processing elapsed time: ', 35.96606397628784, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[733]', 'EPOCH LOSS:', 0.029305512280636825, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 733, ']')


'Epoch [734] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 375,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00043307264583113006,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 6, 4, 7, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 734,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 734, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227606.057024)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227640.053285)
('Worker processing elapsed time: ', 33.99626088142395, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[734]', 'EPOCH LOSS:', 0.013893133817662141, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 734, ']')


'Epoch [735] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1770,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009969895182548687,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 4, 4, 9, 4, 4, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 735,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 735, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227640.059502)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227674.280813)
('Worker processing elapsed time: ', 34.22131109237671, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[735]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 735, ']')


'Epoch [736] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 785,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005865122646730382,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 9, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 736,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 736, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227674.286576)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227708.903201)
('Worker processing elapsed time: ', 34.6166250705719, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[736]', 'EPOCH LOSS:', 0.47534587086143948, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 736, ']')


'Epoch [737] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 717,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00013663811056574982,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 5, 4, 4, 6, 4, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 737,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 737, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227708.908164)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227744.783756)
('Worker processing elapsed time: ', 35.87559199333191, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[737]', 'EPOCH LOSS:', 0.033657990338956674, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 737, ']')


'Epoch [738] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1262,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008168189215409229,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 4, 5, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 738,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 738, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227744.788522)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227779.769291)
('Worker processing elapsed time: ', 34.98076891899109, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[738]', 'EPOCH LOSS:', 0.99554694466586613, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 738, ']')


'Epoch [739] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 442,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006541534184889225,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 6, 6, 6, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 739,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 739, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227779.774288)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227815.13001)
('Worker processing elapsed time: ', 35.355721950531006, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[739]', 'EPOCH LOSS:', 0.027813311515972817, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 739, ']')


'Epoch [740] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1922,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009822302107622396,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 6, 7, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 740,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 740, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227815.135786)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227849.413878)
('Worker processing elapsed time: ', 34.27809190750122, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[740]', 'EPOCH LOSS:', 263.39728440044939, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 740, ']')


'Epoch [741] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1241,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009914559608644596,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 7, 8, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 741,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 741, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227849.41962)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227883.658384)
('Worker processing elapsed time: ', 34.23876404762268, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[741]', 'EPOCH LOSS:', 1042.0082472775553, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 741, ']')


'Epoch [742] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 509,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006762461346347245,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 4, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 742,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 742, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227883.664251)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227918.229634)
('Worker processing elapsed time: ', 34.565382957458496, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[742]', 'EPOCH LOSS:', 0.44284651120639557, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 742, ']')


'Epoch [743] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 430,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000292962682722346,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 743,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 743, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227918.23607)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227952.803012)
('Worker processing elapsed time: ', 34.56694197654724, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[743]', 'EPOCH LOSS:', 37.986928791603987, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 743, ']')


'Epoch [744] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1023,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000541500298612625,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 7, 9, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 744,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 744, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227952.809089)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494227986.738895)
('Worker processing elapsed time: ', 33.92980599403381, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[744]', 'EPOCH LOSS:', 0.025053661680052648, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 744, ']')


'Epoch [745] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 799,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008366860742174832,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 8, 7, 8, 8, 7, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 745,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 745, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494227986.744326)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228021.744188)
('Worker processing elapsed time: ', 34.9998619556427, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[745]', 'EPOCH LOSS:', 0.064026252796787836, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 745, ']')


'Epoch [746] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 492,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00047742842971526924,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 6, 7, 8, 9, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 746,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 746, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228021.750365)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228056.464648)
('Worker processing elapsed time: ', 34.71428298950195, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[746]', 'EPOCH LOSS:', 0.25786623944342313, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 746, ']')


'Epoch [747] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 511,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007877710495952413,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 7, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 747,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 747, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228056.47058)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228090.558625)
('Worker processing elapsed time: ', 34.08804488182068, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[747]', 'EPOCH LOSS:', 363.2135786243079, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 747, ']')


'Epoch [748] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 281,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007766292281383545,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 748,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 748, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228090.564542)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228094.100322)
('Worker processing elapsed time: ', 3.5357799530029297, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[748]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 748, ']')


'Epoch [749] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1780,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005291112361840572,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 9, 5, 6, 8, 4, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 749,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 749, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228094.105442)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228128.392775)
('Worker processing elapsed time: ', 34.2873330116272, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[749]', 'EPOCH LOSS:', 0.023836148442071676, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 749, ']')


'Epoch [750] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 520,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005135178163191583,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 7, 4, 4, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 750,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 750, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228128.398296)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228162.85996)
('Worker processing elapsed time: ', 34.46166396141052, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[750]', 'EPOCH LOSS:', 0.080035891795309899, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 750, ']')


'Epoch [751] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 304,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008319760114580812,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 6, 6, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 751,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 751, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228162.865432)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228197.12012)
('Worker processing elapsed time: ', 34.254688024520874, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[751]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 751, ']')


'Epoch [752] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1478,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000854614344545941,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 8, 7, 9, 5, 5, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 752,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 752, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228197.126347)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228232.965754)
('Worker processing elapsed time: ', 35.839406967163086, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[752]', 'EPOCH LOSS:', 3.6017933771291615, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 752, ']')


'Epoch [753] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1983,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000730997665402578,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 753,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 753, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228232.971374)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228267.325338)
('Worker processing elapsed time: ', 34.35396385192871, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[753]', 'EPOCH LOSS:', 0.030140456692133642, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 753, ']')


'Epoch [754] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 593,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009859324912573069,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 9, 6, 8, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 754,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 754, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228267.330372)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228301.230632)
('Worker processing elapsed time: ', 33.90025997161865, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[754]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 754, ']')


'Epoch [755] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1568,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00041150934815810424,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 6, 4, 6, 7, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 755,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 755, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228301.235713)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228336.764946)
('Worker processing elapsed time: ', 35.5292329788208, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[755]', 'EPOCH LOSS:', 0.029868353425860888, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 755, ']')


'Epoch [756] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1656,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00045400949691247624,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 8, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 756,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 756, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228336.770359)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228370.503062)
('Worker processing elapsed time: ', 33.73270297050476, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[756]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 756, ']')


'Epoch [757] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 869,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016834198841004144,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 5, 4, 8, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 757,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 757, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228370.507842)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228404.482467)
('Worker processing elapsed time: ', 33.97462487220764, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[757]', 'EPOCH LOSS:', 0.025232412536315557, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 757, ']')


'Epoch [758] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1437,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00024594762501232287,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 7, 7, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 758,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 758, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228404.487715)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228438.635551)
('Worker processing elapsed time: ', 34.14783596992493, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[758]', 'EPOCH LOSS:', 23.962366955354145, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 758, ']')


'Epoch [759] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1206,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008433657356812074,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 8, 8, 4, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 759,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 759, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228438.641858)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228472.663043)
('Worker processing elapsed time: ', 34.02118492126465, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[759]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 759, ']')


'Epoch [760] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1631,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004627958934331763,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 760,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 760, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228472.668912)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228506.23377)
('Worker processing elapsed time: ', 33.564857959747314, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[760]', 'EPOCH LOSS:', 0.023899144764454275, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 760, ']')


'Epoch [761] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1625,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002740383064684361,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 7, 7, 6, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 761,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 761, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228506.239179)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228541.544122)
('Worker processing elapsed time: ', 35.3049430847168, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[761]', 'EPOCH LOSS:', 0.66237287306288817, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 761, ']')


'Epoch [762] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 557,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004390132276889515,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 5, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 762,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 762, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228541.549938)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228575.23129)
('Worker processing elapsed time: ', 33.68135213851929, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[762]', 'EPOCH LOSS:', 0.026903589498127545, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 762, ']')


'Epoch [763] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 163,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006283526420796167,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 6, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 763,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 763, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228575.237339)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228608.929235)
('Worker processing elapsed time: ', 33.691895961761475, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[763]', 'EPOCH LOSS:', 0.0263573366463816, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 763, ']')


'Epoch [764] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 757,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00033430751087518097,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 5, 9, 9, 5, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 764,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 764, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228608.93439)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228643.630802)
('Worker processing elapsed time: ', 34.69641184806824, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[764]', 'EPOCH LOSS:', 0.59191618343504715, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 764, ']')


'Epoch [765] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1247,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00017892458158643863,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 5, 5, 8, 6, 6, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 765,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 765, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228643.635944)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228679.541589)
('Worker processing elapsed time: ', 35.90564513206482, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[765]', 'EPOCH LOSS:', 0.028624801508155814, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 765, ']')


'Epoch [766] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 399,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00041303959599608064,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 6, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 766,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 766, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228679.547203)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228714.20834)
('Worker processing elapsed time: ', 34.661136865615845, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[766]', 'EPOCH LOSS:', 8.5260716855775787, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 766, ']')


'Epoch [767] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1330,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009484947178817256,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 8, 7, 5, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 767,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 767, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228714.213959)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228748.181822)
('Worker processing elapsed time: ', 33.96786308288574, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[767]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 767, ']')


'Epoch [768] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 204,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008251185834729832,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 8, 8, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 768,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 768, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228748.186675)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228783.326136)
('Worker processing elapsed time: ', 35.139461040496826, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[768]', 'EPOCH LOSS:', 0.064873146074296825, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 768, ']')


'Epoch [769] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 428,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008433845945371132,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 4, 9, 7, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 769,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 769, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228783.331143)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228817.756419)
('Worker processing elapsed time: ', 34.425276041030884, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[769]', 'EPOCH LOSS:', 0.36105634115414925, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 769, ']')


'Epoch [770] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 605,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00033871275681881727,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 6, 5, 6, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 770,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 770, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228817.76199)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228852.153999)
('Worker processing elapsed time: ', 34.392009019851685, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[770]', 'EPOCH LOSS:', 94.153912630383161, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 770, ']')


'Epoch [771] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 833,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00020757383458406122,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 5, 8, 5, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 771,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 771, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228852.159111)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228886.62359)
('Worker processing elapsed time: ', 34.464478969573975, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[771]', 'EPOCH LOSS:', 509.71157541496552, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 771, ']')


'Epoch [772] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1701,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006400319183936597,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 5, 4, 5, 6, 4, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 772,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 772, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228886.629179)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228921.504983)
('Worker processing elapsed time: ', 34.87580394744873, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[772]', 'EPOCH LOSS:', 2.4371629244851327, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 772, ']')


'Epoch [773] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 192,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007718611132715713,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 773,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 773, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228921.510115)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228955.864173)
('Worker processing elapsed time: ', 34.354058027267456, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[773]', 'EPOCH LOSS:', 0.04000928879878788, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 773, ']')


'Epoch [774] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1725,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008583260735786023,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 6, 5, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 774,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 774, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228955.869981)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494228989.724819)
('Worker processing elapsed time: ', 33.8548378944397, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[774]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 774, ']')


'Epoch [775] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1840,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009649401854267134,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 9, 7, 7, 6, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 775,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 775, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494228989.730008)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229024.361837)
('Worker processing elapsed time: ', 34.631829023361206, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[775]', 'EPOCH LOSS:', 0.062783563497189734, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 775, ']')


'Epoch [776] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1533,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008880120036891635,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 6, 8, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 776,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 776, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229024.36719)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229059.289784)
('Worker processing elapsed time: ', 34.92259407043457, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[776]', 'EPOCH LOSS:', 0.024488289553524785, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 776, ']')


'Epoch [777] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 666,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00036914479351265463,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 6, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 777,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 777, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229059.295281)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229093.869215)
('Worker processing elapsed time: ', 34.57393407821655, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[777]', 'EPOCH LOSS:', 23.146102484695561, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 777, ']')


'Epoch [778] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1013,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004891403966722596,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 5, 5, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 778,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 778, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229093.874294)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229128.910333)
('Worker processing elapsed time: ', 35.036038875579834, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[778]', 'EPOCH LOSS:', 2.0437132022799407, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 778, ']')


'Epoch [779] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1097,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00015833518835970122,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 9, 6, 8, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 779,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 779, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229128.915898)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229162.86769)
('Worker processing elapsed time: ', 33.95179200172424, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[779]', 'EPOCH LOSS:', 0.025221676346498852, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 779, ']')


'Epoch [780] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1526,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00031072520599872045,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 4, 8, 8, 4, 7, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 780,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 780, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229162.873412)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229197.198718)
('Worker processing elapsed time: ', 34.32530617713928, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[780]', 'EPOCH LOSS:', 0.024345026672098632, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 780, ']')


'Epoch [781] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1375,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009800626491377165,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 9, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 781,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 781, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229197.203703)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229231.891549)
('Worker processing elapsed time: ', 34.687846183776855, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[781]', 'EPOCH LOSS:', 0.47981465366736575, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 781, ']')


'Epoch [782] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1169,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00018814267887008892,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 6, 5, 7, 9, 9, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 782,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 782, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229231.897485)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229266.069479)
('Worker processing elapsed time: ', 34.17199397087097, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[782]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 782, ']')


'Epoch [783] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1146,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005583456688190719,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 7, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 783,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 783, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229266.074717)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229300.662948)
('Worker processing elapsed time: ', 34.58823084831238, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[783]', 'EPOCH LOSS:', 0.025931256784702171, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 783, ']')


'Epoch [784] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 189,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00010629414664504306,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 9, 6, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 784,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 784, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229300.670373)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229334.920382)
('Worker processing elapsed time: ', 34.250009059906006, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[784]', 'EPOCH LOSS:', 3.8684651841514306, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 784, ']')


'Epoch [785] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 378,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00042988570988624773,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 9, 6, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 785,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 785, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229334.926327)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229369.187452)
('Worker processing elapsed time: ', 34.26112508773804, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[785]', 'EPOCH LOSS:', 2.6952253076117532, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 785, ']')


'Epoch [786] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1174,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003114373342939829,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 4, 8, 6, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 786,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 786, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229369.192587)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229403.735294)
('Worker processing elapsed time: ', 34.542707204818726, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[786]', 'EPOCH LOSS:', 0.23211263809201124, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 786, ']')


'Epoch [787] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 908,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000844958057975404,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 7, 7, 6, 7, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 787,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 787, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229403.74084)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229437.800893)
('Worker processing elapsed time: ', 34.06005311012268, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[787]', 'EPOCH LOSS:', 0.024520526181673576, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 787, ']')


'Epoch [788] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 983,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005274998131311109,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 8, 5, 7, 7, 8, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 788,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 788, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229437.806484)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229473.780734)
('Worker processing elapsed time: ', 35.974250078201294, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[788]', 'EPOCH LOSS:', 6.1849573093734316, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 788, ']')


'Epoch [789] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 402,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000442980996771004,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 9, 4, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 789,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 789, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229473.786766)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229508.873051)
('Worker processing elapsed time: ', 35.08628487586975, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[789]', 'EPOCH LOSS:', 0.082972600504222921, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 789, ']')


'Epoch [790] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1746,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000855404615357836,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 7, 4, 6, 6, 6, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 790,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 790, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229508.879098)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229544.864535)
('Worker processing elapsed time: ', 35.9854371547699, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[790]', 'EPOCH LOSS:', 0.027518905882858411, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 790, ']')


'Epoch [791] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1250,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005211055472376337,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 6, 5, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 791,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 791, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229544.869822)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229578.659499)
('Worker processing elapsed time: ', 33.789676904678345, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[791]', 'EPOCH LOSS:', 0.02573136096280753, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 791, ']')


'Epoch [792] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 930,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005755143207892078,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 5, 8, 4, 5, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 792,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 792, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229578.665545)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229613.337432)
('Worker processing elapsed time: ', 34.671886920928955, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[792]', 'EPOCH LOSS:', 0.34868464686619233, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 792, ']')


'Epoch [793] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 305,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009443091643952892,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 4, 8, 4, 6, 4, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 793,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 793, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229613.342737)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229648.351759)
('Worker processing elapsed time: ', 35.00902199745178, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[793]', 'EPOCH LOSS:', 0.027926549718062806, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 793, ']')


'Epoch [794] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1674,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002783340102406578,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 5, 6, 6, 6, 8, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 794,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 794, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229648.357643)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229684.181965)
('Worker processing elapsed time: ', 35.82432222366333, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[794]', 'EPOCH LOSS:', 5.9052475821663775, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 794, ']')


'Epoch [795] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 673,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000624149985482813,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 4, 8, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 795,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 795, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229684.187241)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229717.98882)
('Worker processing elapsed time: ', 33.801578998565674, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[795]', 'EPOCH LOSS:', 0.024682110058498821, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 795, ']')


'Epoch [796] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1352,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004503118658401462,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 6, 7, 7, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 796,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 796, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229717.993837)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229752.494893)
('Worker processing elapsed time: ', 34.50105595588684, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[796]', 'EPOCH LOSS:', 0.53278019932181975, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 796, ']')


'Epoch [797] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1976,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004751145386493286,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 9, 6, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 797,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 797, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229752.501109)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229786.798436)
('Worker processing elapsed time: ', 34.29732704162598, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[797]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 797, ']')


'Epoch [798] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 752,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006503866635451739,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 6, 8, 4, 5, 9, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 798,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 798, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229786.803251)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229821.714543)
('Worker processing elapsed time: ', 34.91129207611084, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[798]', 'EPOCH LOSS:', 0.24432183214146641, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 798, ']')


'Epoch [799] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 567,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008205916735510824,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 6, 5, 7, 5, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 799,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 799, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229821.719503)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229826.946794)
('Worker processing elapsed time: ', 5.227291107177734, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[799]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 799, ']')


'Epoch [800] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1687,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007053941460667327,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 4, 7, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 800,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 800, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229826.951806)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229860.800955)
('Worker processing elapsed time: ', 33.849148988723755, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[800]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 800, ']')


'Epoch [801] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1709,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00028445176580108695,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 4, 7, 8, 4, 6, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 801,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 801, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229860.806608)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229895.739505)
('Worker processing elapsed time: ', 34.932897090911865, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[801]', 'EPOCH LOSS:', 3.8691170874315182, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 801, ']')


'Epoch [802] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 207,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009830182559037682,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 4, 7, 8, 8, 9, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 802,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 802, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229895.745541)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229930.724163)
('Worker processing elapsed time: ', 34.97862195968628, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[802]', 'EPOCH LOSS:', 65.402455967740551, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 802, ']')


'Epoch [803] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1156,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009108384926681362,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 4, 4, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 803,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 803, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229930.730036)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494229965.816751)
('Worker processing elapsed time: ', 35.08671498298645, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[803]', 'EPOCH LOSS:', 0.029729957008484884, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 803, ']')


'Epoch [804] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1985,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006437125562361249,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 9, 6, 8, 8, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 804,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 804, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494229965.822241)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230000.48398)
('Worker processing elapsed time: ', 34.661738872528076, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[804]', 'EPOCH LOSS:', 0.10051244267004017, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 804, ']')


'Epoch [805] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 209,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008442864095322519,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 6, 4, 4, 4, 6, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 805,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 805, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230000.489483)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230035.299492)
('Worker processing elapsed time: ', 34.81000876426697, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[805]', 'EPOCH LOSS:', 2.3033852103088424, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 805, ']')


'Epoch [806] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 428,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007048646340204331,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 9, 4, 6, 4, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 806,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 806, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230035.304864)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230069.955946)
('Worker processing elapsed time: ', 34.651082038879395, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[806]', 'EPOCH LOSS:', 0.93036591109944744, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 806, ']')


'Epoch [807] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 462,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00018602005623833046,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 4, 6, 6, 6, 8, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 807,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 807, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230069.961576)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230105.967232)
('Worker processing elapsed time: ', 36.005656003952026, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[807]', 'EPOCH LOSS:', 18.298870709674262, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 807, ']')


'Epoch [808] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 494,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00040125765383858963,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 9, 4, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 808,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 808, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230105.972229)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230139.753993)
('Worker processing elapsed time: ', 33.78176403045654, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[808]', 'EPOCH LOSS:', 0.023985223599643925, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 808, ']')


'Epoch [809] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1468,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008411864049086168,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 5, 7, 4, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 809,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 809, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230139.758737)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230174.250111)
('Worker processing elapsed time: ', 34.491374015808105, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[809]', 'EPOCH LOSS:', 0.51651110560275071, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 809, ']')


'Epoch [810] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 569,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005407198945849752,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 5, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 810,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 810, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230174.255182)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230208.465839)
('Worker processing elapsed time: ', 34.2106568813324, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[810]', 'EPOCH LOSS:', 3.6473743333977175, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 810, ']')


'Epoch [811] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 547,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009484872100207209,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 7, 4, 9, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 811,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 811, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230208.471845)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230243.782934)
('Worker processing elapsed time: ', 35.31108903884888, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[811]', 'EPOCH LOSS:', 0.12182730896817831, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 811, ']')


'Epoch [812] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1533,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008737854714680567,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 4, 9, 8, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 812,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 812, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230243.788183)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230279.090696)
('Worker processing elapsed time: ', 35.302513122558594, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[812]', 'EPOCH LOSS:', 2.4680025027818742, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 812, ']')


'Epoch [813] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 846,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00043864812440494406,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 8, 5, 7, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 813,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 813, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230279.095878)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230313.057072)
('Worker processing elapsed time: ', 33.96119403839111, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[813]', 'EPOCH LOSS:', 0.024817596363816374, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 813, ']')


'Epoch [814] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 841,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007485633374762558,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 8, 7, 5, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 814,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 814, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230313.06297)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230347.09656)
('Worker processing elapsed time: ', 34.03359007835388, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[814]', 'EPOCH LOSS:', 0.024830023974620492, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 814, ']')


'Epoch [815] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1256,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008616724257724166,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 7, 7, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 815,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 815, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230347.102556)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230382.104849)
('Worker processing elapsed time: ', 35.0022931098938, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[815]', 'EPOCH LOSS:', 3.1722701858042139, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 815, ']')


'Epoch [816] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 805,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006294545777257951,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 9, 9, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 816,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 816, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230382.109891)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230417.103185)
('Worker processing elapsed time: ', 34.99329400062561, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[816]', 'EPOCH LOSS:', 0.056382188921095144, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 816, ']')


'Epoch [817] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1687,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000825330624769007,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 8, 6, 7, 7, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 817,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 817, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230417.108356)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230452.723595)
('Worker processing elapsed time: ', 35.615238904953, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[817]', 'EPOCH LOSS:', 0.022704081861331684, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 817, ']')


'Epoch [818] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 631,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006863262508344339,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 8, 9, 9, 8, 4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 818,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 818, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230452.729194)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230488.59748)
('Worker processing elapsed time: ', 35.8682861328125, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[818]', 'EPOCH LOSS:', 2.3687000086780365, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 818, ']')


'Epoch [819] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1058,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008054962032778094,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 4, 8, 5, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 819,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 819, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230488.602701)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230522.600242)
('Worker processing elapsed time: ', 33.99754095077515, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[819]', 'EPOCH LOSS:', 0.025514927310680415, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 819, ']')


'Epoch [820] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 640,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00030859273728105565,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 5, 4, 6, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 820,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 820, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230522.604987)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230556.619659)
('Worker processing elapsed time: ', 34.01467204093933, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[820]', 'EPOCH LOSS:', 0.024818351913513191, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 820, ']')


'Epoch [821] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 597,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003351323625930692,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 5, 9, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 821,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 821, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230556.624971)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230590.904429)
('Worker processing elapsed time: ', 34.27945804595947, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[821]', 'EPOCH LOSS:', 40.50135609750852, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 821, ']')


'Epoch [822] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1223,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023483590789005542,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 7, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 822,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 822, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230590.909893)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230624.918367)
('Worker processing elapsed time: ', 34.00847387313843, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[822]', 'EPOCH LOSS:', 46.793298448044098, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 822, ']')


'Epoch [823] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1757,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006353703052965385,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 9, 9, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 823,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 823, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230624.923334)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230659.22705)
('Worker processing elapsed time: ', 34.30371618270874, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[823]', 'EPOCH LOSS:', 77.661794279375158, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 823, ']')


'Epoch [824] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1499,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006123214114652644,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 9, 6, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 824,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 824, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230659.233098)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230693.490815)
('Worker processing elapsed time: ', 34.25771689414978, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[824]', 'EPOCH LOSS:', 4082.6143541718034, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 824, ']')


'Epoch [825] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1222,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005800235561165021,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 9, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 825,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 825, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230693.495801)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230727.120709)
('Worker processing elapsed time: ', 33.62490797042847, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[825]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 825, ']')


'Epoch [826] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 237,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006591637713756674,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 4, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 826,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 826, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230727.126858)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230761.213764)
('Worker processing elapsed time: ', 34.08690595626831, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[826]', 'EPOCH LOSS:', 111.01620381527162, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 826, ']')


'Epoch [827] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 730,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00033344170272039443,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 8, 9, 4, 6, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 827,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 827, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230761.218833)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230795.288326)
('Worker processing elapsed time: ', 34.06949305534363, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[827]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 827, ']')


'Epoch [828] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1118,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009296363918154329,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 4, 6, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 828,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 828, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230795.293977)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230830.306396)
('Worker processing elapsed time: ', 35.01241898536682, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[828]', 'EPOCH LOSS:', 0.041141502311040183, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 828, ']')


'Epoch [829] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 997,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002734403619642549,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 7, 7, 5, 4, 5, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 829,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 829, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230830.312546)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230865.294759)
('Worker processing elapsed time: ', 34.98221302032471, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[829]', 'EPOCH LOSS:', 1.4831508951989902, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 829, ']')


'Epoch [830] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 166,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006046017990072834,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 8, 8, 7, 5, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 830,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 830, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230865.300051)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230899.915501)
('Worker processing elapsed time: ', 34.61545014381409, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[830]', 'EPOCH LOSS:', 0.045962833541249623, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 830, ']')


'Epoch [831] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1733,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007055011361493507,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 9, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 831,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 831, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230899.921278)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230934.660009)
('Worker processing elapsed time: ', 34.738730907440186, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[831]', 'EPOCH LOSS:', 0.11153969506011205, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 831, ']')


'Epoch [832] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 852,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00012394639220416866,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 832,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 832, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230934.66593)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494230969.006991)
('Worker processing elapsed time: ', 34.34106087684631, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[832]', 'EPOCH LOSS:', 0.063991108565983168, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 832, ']')


'Epoch [833] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1069,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00034049015890595994,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 8, 4, 9, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 833,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 833, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494230969.012245)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231003.457544)
('Worker processing elapsed time: ', 34.44529914855957, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[833]', 'EPOCH LOSS:', 4.2993500293133424, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 833, ']')


'Epoch [834] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1900,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003156999129330859,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 9, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 834,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 834, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231003.462398)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231038.153423)
('Worker processing elapsed time: ', 34.69102501869202, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[834]', 'EPOCH LOSS:', 0.20884837841721748, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 834, ']')


'Epoch [835] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 455,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008092872292467318,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 835,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 835, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231038.158612)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231072.534753)
('Worker processing elapsed time: ', 34.37614107131958, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[835]', 'EPOCH LOSS:', 0.19580809545468655, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 835, ']')


'Epoch [836] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 561,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004008259734963409,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 9, 6, 6, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 836,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 836, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231072.539518)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231107.787481)
('Worker processing elapsed time: ', 35.247962951660156, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[836]', 'EPOCH LOSS:', 0.026049628594227173, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 836, ']')


'Epoch [837] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 578,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004804665402395557,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 7, 4, 9, 4, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 837,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 837, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231107.79269)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231143.39606)
('Worker processing elapsed time: ', 35.60336995124817, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[837]', 'EPOCH LOSS:', 0.023889957295882981, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 837, ']')


'Epoch [838] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1227,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00044374279112464833,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 838,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 838, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231143.40214)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231177.306052)
('Worker processing elapsed time: ', 33.90391206741333, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[838]', 'EPOCH LOSS:', 0.76476375826598819, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 838, ']')


'Epoch [839] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 583,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00037030826261307546,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 7, 6, 5, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 839,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 839, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231177.310856)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231211.299928)
('Worker processing elapsed time: ', 33.9890718460083, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[839]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 839, ']')


'Epoch [840] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 322,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007793345197834845,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 6, 8, 8, 8, 4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 840,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 840, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231211.30526)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231245.577904)
('Worker processing elapsed time: ', 34.27264404296875, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[840]', 'EPOCH LOSS:', 0.025838391704187558, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 840, ']')


'Epoch [841] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 237,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007866961405784022,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 8, 4, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 841,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 841, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231245.582755)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231280.579891)
('Worker processing elapsed time: ', 34.99713587760925, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[841]', 'EPOCH LOSS:', 3.0137610820938563, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 841, ']')


'Epoch [842] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 688,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006715304275421868,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 8, 9, 7, 6, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 842,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 842, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231280.585705)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231316.193775)
('Worker processing elapsed time: ', 35.608069896698, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[842]', 'EPOCH LOSS:', 0.88504696800674731, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 842, ']')


'Epoch [843] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 650,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004342789923317479,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 5, 4, 8, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 843,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 843, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231316.198897)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231351.423612)
('Worker processing elapsed time: ', 35.22471523284912, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[843]', 'EPOCH LOSS:', 0.76359537017728563, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 843, ']')


'Epoch [844] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1755,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00035369065058397453,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 7, 8, 7, 4, 6, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 844,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 844, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231351.428506)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231387.245645)
('Worker processing elapsed time: ', 35.81713914871216, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[844]', 'EPOCH LOSS:', 127.74153457941772, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 844, ']')


'Epoch [845] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 909,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00040521689366662553,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 7, 4, 5, 4, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 845,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 845, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231387.251212)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231421.865058)
('Worker processing elapsed time: ', 34.61384606361389, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[845]', 'EPOCH LOSS:', 1.184098419737605, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 845, ']')


'Epoch [846] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1445,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004868063668421836,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 6, 8, 4, 5, 8, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 846,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 846, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231421.870619)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231457.680361)
('Worker processing elapsed time: ', 35.80974197387695, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[846]', 'EPOCH LOSS:', 0.38715005538672059, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 846, ']')


'Epoch [847] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1606,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00048776782477949053,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 847,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 847, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231457.686278)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231491.346365)
('Worker processing elapsed time: ', 33.66008687019348, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[847]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 847, ']')


'Epoch [848] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1943,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004896847517021137,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 5, 8, 8, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 848,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 848, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231491.35134)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231526.59607)
('Worker processing elapsed time: ', 35.24472999572754, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[848]', 'EPOCH LOSS:', 0.047483027745805116, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 848, ']')


'Epoch [849] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1144,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004659166908510384,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 8, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 849,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 849, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231526.601702)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231561.124948)
('Worker processing elapsed time: ', 34.52324604988098, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[849]', 'EPOCH LOSS:', 0.083351024407906482, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 849, ']')


'Epoch [850] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 216,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00014078192556095284,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 8, 8, 6, 7, 4, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 850,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 850, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231561.130364)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231595.379447)
('Worker processing elapsed time: ', 34.249083042144775, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[850]', 'EPOCH LOSS:', 0.024415374685296538, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 850, ']')


'Epoch [851] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 212,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00038001807248145365,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 9, 6, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 851,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 851, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231595.384411)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231629.155692)
('Worker processing elapsed time: ', 33.771281003952026, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[851]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 851, ']')


'Epoch [852] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1853,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005607909615451811,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 8, 7, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 852,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 852, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231629.161202)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231664.217765)
('Worker processing elapsed time: ', 35.05656313896179, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[852]', 'EPOCH LOSS:', 0.041743166204907957, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 852, ']')


'Epoch [853] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 454,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000343459799007347,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 8, 4, 4, 4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 853,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 853, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231664.223817)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231698.835943)
('Worker processing elapsed time: ', 34.612125873565674, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[853]', 'EPOCH LOSS:', 0.076105260810937048, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 853, ']')


'Epoch [854] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1705,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005286492942170222,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 854,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 854, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231698.840774)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231732.674659)
('Worker processing elapsed time: ', 33.833884954452515, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[854]', 'EPOCH LOSS:', 13.320415612444144, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 854, ']')


'Epoch [855] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 342,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004356180662033538,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 9, 7, 6, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 855,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 855, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231732.680906)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231766.729637)
('Worker processing elapsed time: ', 34.04873085021973, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[855]', 'EPOCH LOSS:', 0.024148739183188536, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 855, ']')


'Epoch [856] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1022,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000519807027555897,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 856,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 856, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231766.734346)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231800.260703)
('Worker processing elapsed time: ', 33.52635717391968, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[856]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 856, ']')


'Epoch [857] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 485,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004899892675318478,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 5, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 857,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 857, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231800.266452)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231834.897458)
('Worker processing elapsed time: ', 34.63100600242615, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[857]', 'EPOCH LOSS:', 0.052795076955536313, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 857, ']')


'Epoch [858] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1909,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009485755103067129,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 9, 4, 7, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 858,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 858, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231834.902528)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231870.453803)
('Worker processing elapsed time: ', 35.55127501487732, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[858]', 'EPOCH LOSS:', 0.024314207691477466, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 858, ']')


'Epoch [859] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1673,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00021657040396584756,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 5, 8, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 859,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 859, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231870.458768)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231904.659718)
('Worker processing elapsed time: ', 34.200950145721436, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[859]', 'EPOCH LOSS:', 6.2758451231095629, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 859, ']')


'Epoch [860] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1691,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00019793351606131138,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 860,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 860, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231904.665574)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231938.292703)
('Worker processing elapsed time: ', 33.6271288394928, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[860]', 'EPOCH LOSS:', 0.023676775801580591, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 860, ']')


'Epoch [861] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 389,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008517154435986659,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 4, 9, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 861,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 861, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231938.29839)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494231972.622606)
('Worker processing elapsed time: ', 34.32421612739563, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[861]', 'EPOCH LOSS:', 2753.0720987637123, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 861, ']')


'Epoch [862] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1889,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00042030942387910294,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 5, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 862,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 862, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494231972.635546)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232006.359101)
('Worker processing elapsed time: ', 33.72355508804321, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[862]', 'EPOCH LOSS:', 0.023181989712389803, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 862, ']')


'Epoch [863] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 432,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009773283635080095,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 6, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 863,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 863, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232006.363911)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232040.06356)
('Worker processing elapsed time: ', 33.69964909553528, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[863]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 863, ']')


'Epoch [864] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 467,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003629601254219436,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 9, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 864,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 864, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232040.068446)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232073.69411)
('Worker processing elapsed time: ', 33.6256639957428, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[864]', 'EPOCH LOSS:', 0.022885850075122588, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 864, ']')


'Epoch [865] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1196,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006759625857287836,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 6, 7, 5, 4, 8, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 865,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 865, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232073.699281)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232107.938135)
('Worker processing elapsed time: ', 34.238853931427, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[865]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 865, ']')


'Epoch [866] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1686,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00032211146783354884,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 7, 6, 8, 9, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 866,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 866, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232107.943156)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232143.587165)
('Worker processing elapsed time: ', 35.64400911331177, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[866]', 'EPOCH LOSS:', 0.025965787833440938, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 866, ']')


'Epoch [867] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 474,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006151799322831576,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 5, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 867,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 867, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232143.593064)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232177.67821)
('Worker processing elapsed time: ', 34.08514595031738, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[867]', 'EPOCH LOSS:', 2.7239129315931265, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 867, ']')


'Epoch [868] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 751,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008921921765671349,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 4, 4, 8, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 868,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 868, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232177.683348)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232212.160475)
('Worker processing elapsed time: ', 34.47712707519531, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[868]', 'EPOCH LOSS:', 0.38679421018811178, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 868, ']')


'Epoch [869] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 506,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006527587274289125,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 8, 6, 5, 8, 4, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 869,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 869, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232212.166518)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232246.9763)
('Worker processing elapsed time: ', 34.80978202819824, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[869]', 'EPOCH LOSS:', 212.80274306256328, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 869, ']')


'Epoch [870] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1091,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003870265751909325,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 7, 9, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 870,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 870, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232246.982383)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232280.785349)
('Worker processing elapsed time: ', 33.80296587944031, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[870]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 870, ']')


'Epoch [871] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 567,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007352828793265056,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 9, 4, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 871,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 871, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232280.790683)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232315.056803)
('Worker processing elapsed time: ', 34.266119956970215, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[871]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 871, ']')


'Epoch [872] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 826,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004087291183014813,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 6, 9, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 872,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 872, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232315.06227)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232348.838209)
('Worker processing elapsed time: ', 33.775938987731934, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[872]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 872, ']')


'Epoch [873] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 383,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002913212739137172,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 9, 4, 4, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 873,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 873, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232348.843143)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232382.768211)
('Worker processing elapsed time: ', 33.92506790161133, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[873]', 'EPOCH LOSS:', 0.022889555834277889, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 873, ']')


'Epoch [874] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1221,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000977682193169271,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 5, 4, 4, 4, 4, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 874,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 874, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232382.774127)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232418.702048)
('Worker processing elapsed time: ', 35.92792105674744, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[874]', 'EPOCH LOSS:', 0.025308668940980546, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 874, ']')


'Epoch [875] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1150,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008809520834654177,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 9, 7, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 875,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 875, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232418.707295)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232452.924959)
('Worker processing elapsed time: ', 34.21766400337219, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[875]', 'EPOCH LOSS:', 6.060346246181358, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 875, ']')


'Epoch [876] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 201,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001555670979080209,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 9, 6, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 876,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 876, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232452.930066)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232487.93714)
('Worker processing elapsed time: ', 35.00707387924194, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[876]', 'EPOCH LOSS:', 0.066834642172632852, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 876, ']')


'Epoch [877] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1316,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004615326206267408,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 9, 4, 7, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 877,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 877, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232487.942122)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232522.414993)
('Worker processing elapsed time: ', 34.47287106513977, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[877]', 'EPOCH LOSS:', 4887.6888291738678, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 877, ']')


'Epoch [878] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1320,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000824072797354507,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 9, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 878,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 878, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232522.420983)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232556.517341)
('Worker processing elapsed time: ', 34.09635782241821, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[878]', 'EPOCH LOSS:', 582.24620593710972, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 878, ']')


'Epoch [879] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1918,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00011214836505102008,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 7, 4, 4, 7, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 879,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 879, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232556.522576)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232592.067412)
('Worker processing elapsed time: ', 35.544835805892944, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[879]', 'EPOCH LOSS:', 0.17258093018813722, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 879, ']')


'Epoch [880] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 822,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002868048199589243,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 6, 7, 8, 6, 4, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 880,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 880, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232592.07323)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232627.032717)
('Worker processing elapsed time: ', 34.959486961364746, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[880]', 'EPOCH LOSS:', 0.35840195153202686, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 880, ']')


'Epoch [881] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1044,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000937560372734518,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 8, 9, 9, 8, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 881,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 881, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232627.038624)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232662.660574)
('Worker processing elapsed time: ', 35.621949911117554, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[881]', 'EPOCH LOSS:', 0.46957694495742386, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 881, ']')


'Epoch [882] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 570,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008355701727736455,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 882,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 882, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232662.665588)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232696.250519)
('Worker processing elapsed time: ', 33.58493113517761, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[882]', 'EPOCH LOSS:', 0.026412071451838659, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 882, ']')


'Epoch [883] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1805,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002488911766027884,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 8, 6, 9, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 883,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 883, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232696.255629)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232731.630942)
('Worker processing elapsed time: ', 35.37531304359436, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[883]', 'EPOCH LOSS:', 0.033611711260120199, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 883, ']')


'Epoch [884] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1126,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006854038920976106,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 8, 8, 6, 9, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 884,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 884, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232731.636702)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232766.267582)
('Worker processing elapsed time: ', 34.6308798789978, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[884]', 'EPOCH LOSS:', 3298.0168171821956, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 884, ']')


'Epoch [885] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 875,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00010072667209507305,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 885,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 885, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232766.272357)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232800.639098)
('Worker processing elapsed time: ', 34.36674094200134, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[885]', 'EPOCH LOSS:', 0.58532908769680614, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 885, ']')


'Epoch [886] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 447,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003586928555900464,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 5, 7, 5, 4, 4, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 886,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 886, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232800.645287)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232836.531547)
('Worker processing elapsed time: ', 35.88626003265381, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[886]', 'EPOCH LOSS:', 1.3164678206283709, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 886, ']')


'Epoch [887] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1373,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003941882290820582,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 6, 4, 5, 9, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 887,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 887, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232836.536541)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232870.799606)
('Worker processing elapsed time: ', 34.26306509971619, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[887]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 887, ']')


'Epoch [888] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 484,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005379817634749965,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 888,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 888, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232870.805007)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232905.188666)
('Worker processing elapsed time: ', 34.38365912437439, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[888]', 'EPOCH LOSS:', 0.067429377074848654, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 888, ']')


'Epoch [889] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1033,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00034147153234742296,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 889,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 889, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232905.194009)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232938.664945)
('Worker processing elapsed time: ', 33.4709358215332, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[889]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 889, ']')


'Epoch [890] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1523,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005626138886089108,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 8, 7, 8, 8, 8, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 890,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 890, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232938.67059)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494232973.621365)
('Worker processing elapsed time: ', 34.950775146484375, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[890]', 'EPOCH LOSS:', 551.21215652925343, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 890, ']')


'Epoch [891] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1246,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008727588145129119,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 6, 7, 9, 6, 8, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 891,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 891, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494232973.626726)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233007.873613)
('Worker processing elapsed time: ', 34.24688720703125, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[891]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 891, ']')


'Epoch [892] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1177,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000980422862738975,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 5, 4, 8, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 892,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 892, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233007.879302)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233041.843603)
('Worker processing elapsed time: ', 33.964300870895386, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[892]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 892, ']')


'Epoch [893] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 544,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007411320346870903,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 5, 6, 8, 6, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 893,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 893, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233041.849676)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233076.49091)
('Worker processing elapsed time: ', 34.641234159469604, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[893]', 'EPOCH LOSS:', 46.567952220747962, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 893, ']')


'Epoch [894] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 665,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009075024040900579,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 6, 7, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 894,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 894, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233076.497212)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233110.384656)
('Worker processing elapsed time: ', 33.88744401931763, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[894]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 894, ']')


'Epoch [895] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1533,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004674071712399202,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 4, 9, 9, 5, 7, 6, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 895,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 895, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233110.390813)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233144.548164)
('Worker processing elapsed time: ', 34.15735077857971, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[895]', 'EPOCH LOSS:', 0.024257899302256615, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 895, ']')


'Epoch [896] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1452,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004813107236654095,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 9, 6, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 896,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 896, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233144.553824)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233178.618299)
('Worker processing elapsed time: ', 34.06447505950928, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[896]', 'EPOCH LOSS:', 7.8548484690799478, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 896, ']')


'Epoch [897] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1169,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008820513119894789,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 6, 8, 4, 4, 7, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 897,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 897, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233178.623238)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233214.579329)
('Worker processing elapsed time: ', 35.95609092712402, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[897]', 'EPOCH LOSS:', 0.094097532275533705, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 897, ']')


'Epoch [898] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1240,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00029048681782434607,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 4, 8, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 898,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 898, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233214.58441)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233248.409642)
('Worker processing elapsed time: ', 33.82523202896118, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[898]', 'EPOCH LOSS:', 0.02587318692485811, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 898, ']')


'Epoch [899] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 251,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004443035106702852,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 5, 5, 7, 7, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 899,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 899, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233248.414842)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233283.994396)
('Worker processing elapsed time: ', 35.579554080963135, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[899]', 'EPOCH LOSS:', 621.56262412199931, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 899, ']')


'Epoch [900] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 824,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00048300937765806905,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 5, 8, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 900,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 900, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233283.99914)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233318.979031)
('Worker processing elapsed time: ', 34.97989106178284, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[900]', 'EPOCH LOSS:', 1.4108517607123487, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 900, ']')


'Epoch [901] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 119,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009284563954081889,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 5, 8, 5, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 901,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 901, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233318.984246)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233354.191006)
('Worker processing elapsed time: ', 35.20675992965698, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[901]', 'EPOCH LOSS:', 0.15381516397406511, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 901, ']')


'Epoch [902] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1400,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007457416734259762,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 7, 7, 6, 7, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 902,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 902, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233354.195998)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233388.897409)
('Worker processing elapsed time: ', 34.70141100883484, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[902]', 'EPOCH LOSS:', 0.12525647160998304, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 902, ']')


'Epoch [903] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 284,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004011482592649496,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 9, 9, 9, 7, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 903,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 903, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233388.902473)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233424.442351)
('Worker processing elapsed time: ', 35.539878129959106, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[903]', 'EPOCH LOSS:', 0.034298047762389004, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 903, ']')


'Epoch [904] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1541,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004958749231229645,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 5, 4, 7, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 904,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 904, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233424.44793)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233458.402345)
('Worker processing elapsed time: ', 33.95441484451294, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[904]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 904, ']')


'Epoch [905] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 508,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007937253909476135,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 905,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 905, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233458.40769)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233491.954216)
('Worker processing elapsed time: ', 33.546525955200195, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[905]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 905, ']')


'Epoch [906] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1770,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006196962804407665,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 6, 6, 4, 7, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 906,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 906, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233491.960091)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233526.397742)
('Worker processing elapsed time: ', 34.43765091896057, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[906]', 'EPOCH LOSS:', 4.4648165279405454, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 906, ']')


'Epoch [907] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 379,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00045379576685263424,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 5, 6, 4, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 907,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 907, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233526.403422)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233562.059954)
('Worker processing elapsed time: ', 35.6565318107605, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[907]', 'EPOCH LOSS:', 1.0812081833268352, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 907, ']')


'Epoch [908] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 139,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005367541099394407,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 5, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 908,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 908, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233562.065526)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233595.834603)
('Worker processing elapsed time: ', 33.76907706260681, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[908]', 'EPOCH LOSS:', 0.024680085065188099, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 908, ']')


'Epoch [909] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 157,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007984472008320577,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 909,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 909, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233595.839964)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233630.147576)
('Worker processing elapsed time: ', 34.30761218070984, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[909]', 'EPOCH LOSS:', 0.033345927611748358, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 909, ']')


'Epoch [910] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 546,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006662297116984655,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 9, 7, 5, 6, 9, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 910,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 910, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233630.152485)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233666.035891)
('Worker processing elapsed time: ', 35.88340616226196, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[910]', 'EPOCH LOSS:', 0.21149821278551043, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 910, ']')


'Epoch [911] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1819,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004608446786492509,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 5, 8, 5, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 911,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 911, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233666.040984)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233700.0077)
('Worker processing elapsed time: ', 33.966716051101685, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[911]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 911, ']')


'Epoch [912] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 966,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005123521476766029,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 7, 8, 5, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 912,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 912, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233700.012895)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233735.282756)
('Worker processing elapsed time: ', 35.2698609828949, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[912]', 'EPOCH LOSS:', 0.024679538554454244, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 912, ']')


'Epoch [913] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1341,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002805040452660666,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 6, 7, 4, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 913,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 913, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233735.287692)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233769.735166)
('Worker processing elapsed time: ', 34.447474002838135, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[913]', 'EPOCH LOSS:', 0.287789382262994, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 913, ']')


'Epoch [914] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1696,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005180561082206015,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 4, 4, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 914,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 914, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233769.741369)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233804.758119)
('Worker processing elapsed time: ', 35.01675009727478, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[914]', 'EPOCH LOSS:', 0.025108530451171739, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 914, ']')


'Epoch [915] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1731,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007756016566861974,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 5, 5, 4, 6, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 915,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 915, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233804.764193)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233840.434672)
('Worker processing elapsed time: ', 35.67047905921936, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[915]', 'EPOCH LOSS:', 9.7966024861945726, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 915, ']')


'Epoch [916] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1161,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004679599891788273,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 916,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 916, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233840.440343)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233874.825511)
('Worker processing elapsed time: ', 34.38516807556152, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[916]', 'EPOCH LOSS:', 0.23272656827890442, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 916, ']')


'Epoch [917] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1520,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023342559815293896,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 6, 7, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 917,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 917, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233874.830651)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233909.86532)
('Worker processing elapsed time: ', 35.034668922424316, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[917]', 'EPOCH LOSS:', 64.893712528789976, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 917, ']')


'Epoch [918] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1066,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007768225964769411,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 9, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 918,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 918, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233909.871196)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233943.55482)
('Worker processing elapsed time: ', 33.683624029159546, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[918]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 918, ']')


'Epoch [919] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1426,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006470348544432601,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 7, 8, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 919,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 919, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233943.560765)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494233977.922427)
('Worker processing elapsed time: ', 34.36166191101074, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[919]', 'EPOCH LOSS:', 1.7656447702741929, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 919, ']')


'Epoch [920] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 305,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00011026360738749619,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 6, 7, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 920,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 920, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494233977.928452)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234011.721191)
('Worker processing elapsed time: ', 33.792738914489746, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[920]', 'EPOCH LOSS:', 0.023660715732795694, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 920, ']')


'Epoch [921] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1250,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001215076032069066,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 5, 5, 4, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 921,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 921, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234011.726191)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234046.921688)
('Worker processing elapsed time: ', 35.195497035980225, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[921]', 'EPOCH LOSS:', 13.079819183449214, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 921, ']')


'Epoch [922] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 779,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009618612227749487,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 922,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 922, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234046.926939)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234080.796805)
('Worker processing elapsed time: ', 33.86986589431763, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[922]', 'EPOCH LOSS:', 0.40077891671352411, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 922, ']')


'Epoch [923] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1748,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006315034502956251,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 8, 9, 9, 5, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 923,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 923, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234080.802161)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234116.42537)
('Worker processing elapsed time: ', 35.62320899963379, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[923]', 'EPOCH LOSS:', 0.024527818602059496, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 923, ']')


'Epoch [924] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1178,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00027471794075480246,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 6, 9, 7, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 924,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 924, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234116.431447)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234150.490356)
('Worker processing elapsed time: ', 34.05890893936157, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[924]', 'EPOCH LOSS:', 0.025918938566960512, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 924, ']')


'Epoch [925] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 790,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002730270139936219,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 925,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 925, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234150.495223)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234184.017073)
('Worker processing elapsed time: ', 33.52184987068176, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[925]', 'EPOCH LOSS:', 0.021495946517624116, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 925, ']')


'Epoch [926] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1256,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005411837930977782,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 926,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 926, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234184.022037)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234217.566846)
('Worker processing elapsed time: ', 33.544808864593506, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[926]', 'EPOCH LOSS:', 0.025843384322896703, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 926, ']')


'Epoch [927] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 821,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000143897431872206,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 4, 9, 6, 8, 9, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 927,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 927, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234217.572177)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234251.738547)
('Worker processing elapsed time: ', 34.166370153427124, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[927]', 'EPOCH LOSS:', 0.024917851544077267, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 927, ']')


'Epoch [928] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1367,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00038112843144358897,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 8, 9, 4, 9, 8, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 928,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 928, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234251.745784)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234286.69663)
('Worker processing elapsed time: ', 34.95084595680237, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[928]', 'EPOCH LOSS:', 0.072922061456565407, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 928, ']')


'Epoch [929] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 254,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006173784497827834,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 4, 5, 4, 7, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 929,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 929, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234286.702073)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234321.346349)
('Worker processing elapsed time: ', 34.64427590370178, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[929]', 'EPOCH LOSS:', 6.1764585671413146, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 929, ']')


'Epoch [930] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 524,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006298516762763035,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 4, 8, 6, 6, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 930,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 930, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234321.351231)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234356.106157)
('Worker processing elapsed time: ', 34.75492596626282, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[930]', 'EPOCH LOSS:', 237.10924345700127, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 930, ']')


'Epoch [931] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1304,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007071846565618871,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 9, 5, 8, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 931,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 931, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234356.112115)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234390.534644)
('Worker processing elapsed time: ', 34.422528982162476, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[931]', 'EPOCH LOSS:', 11.991712702594109, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 931, ']')


'Epoch [932] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1531,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005205244962644657,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 4, 7, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 932,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 932, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234390.539771)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234425.60424)
('Worker processing elapsed time: ', 35.06446886062622, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[932]', 'EPOCH LOSS:', 3.6059277694472041, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 932, ']')


'Epoch [933] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1042,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023964699401283357,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 933,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 933, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234425.610044)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234460.020619)
('Worker processing elapsed time: ', 34.4105749130249, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[933]', 'EPOCH LOSS:', 0.14232100931335451, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 933, ']')


'Epoch [934] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 418,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003537101524454001,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 4, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 934,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 934, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234460.026101)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234494.086247)
('Worker processing elapsed time: ', 34.06014585494995, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[934]', 'EPOCH LOSS:', 51.982814697167079, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 934, ']')


'Epoch [935] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 881,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00026463438214945686,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 935,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 935, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234494.091015)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234528.532518)
('Worker processing elapsed time: ', 34.441502809524536, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[935]', 'EPOCH LOSS:', 0.036503527084230936, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 935, ']')


'Epoch [936] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1348,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006045843628689862,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 8, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 936,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 936, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234528.537966)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234562.193975)
('Worker processing elapsed time: ', 33.65600895881653, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[936]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 936, ']')


'Epoch [937] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 485,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009544653824493191,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 937,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 937, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234562.199198)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234596.494945)
('Worker processing elapsed time: ', 34.29574704170227, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[937]', 'EPOCH LOSS:', 0.043464226752483529, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 937, ']')


'Epoch [938] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 531,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003444832889299233,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 7, 7, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 938,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 938, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234596.499712)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234630.909044)
('Worker processing elapsed time: ', 34.409332036972046, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[938]', 'EPOCH LOSS:', 275.83740310692468, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 938, ']')


'Epoch [939] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 542,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008716825111898547,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 5, 8, 7, 8, 6, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 939,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 939, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234630.91431)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234665.745457)
('Worker processing elapsed time: ', 34.83114695549011, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[939]', 'EPOCH LOSS:', 0.12649418992681011, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 939, ']')


'Epoch [940] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 570,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004983662656459713,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 6, 7, 4, 4, 7, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 940,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 940, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234665.750545)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234699.91456)
('Worker processing elapsed time: ', 34.16401505470276, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[940]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 940, ']')


'Epoch [941] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1094,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006438457739957994,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 941,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 941, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234699.919795)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234733.71783)
('Worker processing elapsed time: ', 33.79803490638733, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[941]', 'EPOCH LOSS:', 0.65748756279454312, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 941, ']')


'Epoch [942] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 834,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003154594929815074,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 5, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 942,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 942, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234733.7233)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234767.831148)
('Worker processing elapsed time: ', 34.107847929000854, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[942]', 'EPOCH LOSS:', 42.911790487882257, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 942, ']')


'Epoch [943] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1969,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006294201432275119,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 6, 9, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 943,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 943, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234767.835988)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234802.242536)
('Worker processing elapsed time: ', 34.40654802322388, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[943]', 'EPOCH LOSS:', 4311.4489641070486, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 943, ']')


'Epoch [944] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 192,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005583837186327324,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 9, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 944,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 944, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234802.248877)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234836.286455)
('Worker processing elapsed time: ', 34.037577867507935, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[944]', 'EPOCH LOSS:', 157.27062724569103, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 944, ']')


'Epoch [945] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1013,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001748405132000649,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 945,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 945, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234836.292292)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234870.608028)
('Worker processing elapsed time: ', 34.315735816955566, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[945]', 'EPOCH LOSS:', 0.32457527324055857, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 945, ']')


'Epoch [946] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 513,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00045919948422121096,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 5, 5, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 946,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 946, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234870.61374)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234905.630933)
('Worker processing elapsed time: ', 35.01719307899475, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[946]', 'EPOCH LOSS:', 0.028953756929698617, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 946, ']')


'Epoch [947] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 378,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005492896570808691,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 6, 8, 9, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 947,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 947, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234905.636149)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234940.831182)
('Worker processing elapsed time: ', 35.19503307342529, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[947]', 'EPOCH LOSS:', 0.024155364128606249, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 947, ']')


'Epoch [948] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 616,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00023821003246721087,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 9, 4, 8, 8, 5, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 948,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 948, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234940.837172)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494234975.802246)
('Worker processing elapsed time: ', 34.96507406234741, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[948]', 'EPOCH LOSS:', 1111.8318744002524, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 948, ']')


'Epoch [949] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 238,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008869651657168227,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 8, 6, 6, 6, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 949,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 949, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494234975.807163)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235009.957726)
('Worker processing elapsed time: ', 34.15056300163269, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[949]', 'EPOCH LOSS:', 0.024840205388385439, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 949, ']')


'Epoch [950] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1826,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003227691111342978,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 5, 9, 6, 8, 8, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 950,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 950, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235009.962607)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235044.890424)
('Worker processing elapsed time: ', 34.92781710624695, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[950]', 'EPOCH LOSS:', 0.88311766690904081, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 950, ']')


'Epoch [951] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1629,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007839452347690548,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 9, 4, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 951,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 951, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235044.896588)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235078.953838)
('Worker processing elapsed time: ', 34.057250022888184, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[951]', 'EPOCH LOSS:', 4.7563035985578059, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 951, ']')


'Epoch [952] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 491,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003638032321816139,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 8, 9, 7, 5, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 952,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 952, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235078.958723)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235114.613852)
('Worker processing elapsed time: ', 35.655128955841064, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[952]', 'EPOCH LOSS:', 0.029175654391555471, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 952, ']')


'Epoch [953] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 868,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006568075468243951,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 6, 6, 8, 6, 7, 8, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 953,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 953, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235114.61896)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235149.596496)
('Worker processing elapsed time: ', 34.97753620147705, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[953]', 'EPOCH LOSS:', 0.53818139730579018, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 953, ']')


'Epoch [954] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1412,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00019522106220153709,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 5, 7, 6, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 954,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 954, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235149.601614)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235183.567278)
('Worker processing elapsed time: ', 33.96566390991211, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[954]', 'EPOCH LOSS:', 0.024656508262316211, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 954, ']')


'Epoch [955] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1631,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0004927103541363204,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 5, 6, 7, 9, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 955,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 955, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235183.573311)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235219.142257)
('Worker processing elapsed time: ', 35.56894588470459, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[955]', 'EPOCH LOSS:', 1.2528374227577483, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 955, ']')


'Epoch [956] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1854,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008543978963240753,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 4, 9, 8, 8, 9, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 956,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 956, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235219.148184)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235254.911789)
('Worker processing elapsed time: ', 35.76360487937927, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[956]', 'EPOCH LOSS:', 0.46927788782805613, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 956, ']')


'Epoch [957] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1966,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006073080453655263,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 7, 9, 6, 6, 6, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 957,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 957, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235254.91705)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235290.917117)
('Worker processing elapsed time: ', 36.00006723403931, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[957]', 'EPOCH LOSS:', 0.082727022765446426, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 957, ']')


'Epoch [958] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 794,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006841839104236734,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 5, 7, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 958,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 958, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235290.922285)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235324.857333)
('Worker processing elapsed time: ', 33.93504786491394, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[958]', 'EPOCH LOSS:', 0.024332166867731556, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 958, ']')


'Epoch [959] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1630,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009650468164351133,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 4, 8, 9, 6, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 959,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 959, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235324.862487)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235360.487784)
('Worker processing elapsed time: ', 35.62529683113098, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[959]', 'EPOCH LOSS:', 4.6135040945578076, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 959, ']')


'Epoch [960] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1365,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009347597245758905,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 8, 4, 5, 4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 960,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 960, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235360.493014)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235396.084546)
('Worker processing elapsed time: ', 35.59153199195862, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[960]', 'EPOCH LOSS:', 0.024181075155067688, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 960, ']')


'Epoch [961] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1861,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009065395712362803,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 9, 4, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 961,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 961, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235396.089779)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235431.097658)
('Worker processing elapsed time: ', 35.00787901878357, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[961]', 'EPOCH LOSS:', 0.07981976030201772, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 961, ']')


'Epoch [962] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 981,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000107535215651813,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 962,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 962, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235431.103497)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235465.000717)
('Worker processing elapsed time: ', 33.89721989631653, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[962]', 'EPOCH LOSS:', 169.96753752303763, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 962, ']')


'Epoch [963] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 901,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006485928000996607,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 7, 6, 9, 6, 8, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 963,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 963, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235465.006008)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235499.317579)
('Worker processing elapsed time: ', 34.31157112121582, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[963]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 963, ']')


'Epoch [964] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1910,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007885989906082437,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 9, 5, 9, 7, 6, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 964,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 964, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235499.323747)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235533.391473)
('Worker processing elapsed time: ', 34.067726135253906, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[964]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 964, ']')


'Epoch [965] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1755,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009792584597838672,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 4, 7, 7, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 965,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 965, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235533.39655)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235567.349001)
('Worker processing elapsed time: ', 33.95245099067688, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[965]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 965, ']')


'Epoch [966] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 931,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000816431942791965,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 966,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 966, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235567.354323)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235601.676732)
('Worker processing elapsed time: ', 34.32240915298462, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[966]', 'EPOCH LOSS:', 0.025504229181264423, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 966, ']')


'Epoch [967] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1413,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00013809852822901719,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 8, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 967,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 967, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235601.682166)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235635.686907)
('Worker processing elapsed time: ', 34.004740953445435, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[967]', 'EPOCH LOSS:', 4662.503617512556, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 967, ']')


'Epoch [968] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 805,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006128006475963109,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 968,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 968, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235635.69288)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235670.105712)
('Worker processing elapsed time: ', 34.41283202171326, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[968]', 'EPOCH LOSS:', 2.0434166850511875, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 968, ']')


'Epoch [969] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1903,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005812990760342236,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 9, 9, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 969,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 969, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235670.110559)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235704.123679)
('Worker processing elapsed time: ', 34.01311993598938, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[969]', 'EPOCH LOSS:', 2.8868133889337861, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 969, ']')


'Epoch [970] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 576,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005754402656764311,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 4, 5, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 970,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 970, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235704.129613)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235738.335065)
('Worker processing elapsed time: ', 34.20545196533203, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[970]', 'EPOCH LOSS:', 21.664977388747698, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 970, ']')


'Epoch [971] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1533,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00075262674018638,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 7, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 971,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 971, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235738.340451)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235773.082274)
('Worker processing elapsed time: ', 34.741822957992554, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[971]', 'EPOCH LOSS:', 0.095830387645156806, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 971, ']')


'Epoch [972] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1779,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009343572068313585,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 4, 5, 7, 9, 5, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 972,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 972, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235773.087356)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235808.82331)
('Worker processing elapsed time: ', 35.73595380783081, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[972]', 'EPOCH LOSS:', 0.019506724491247331, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 972, ']')


'Epoch [973] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1471,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002906893724407938,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 5, 7, 9, 4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 973,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 973, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235808.828995)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235843.048054)
('Worker processing elapsed time: ', 34.219058990478516, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[973]', 'EPOCH LOSS:', 0.024233740042095608, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 973, ']')


'Epoch [974] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 966,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00041509530411490175,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 974,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 974, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235843.053712)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235877.402374)
('Worker processing elapsed time: ', 34.34866213798523, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[974]', 'EPOCH LOSS:', 0.041408597090542516, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 974, ']')


'Epoch [975] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 706,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00024127608592000333,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 8, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 975,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 975, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235877.407669)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235912.126929)
('Worker processing elapsed time: ', 34.7192599773407, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[975]', 'EPOCH LOSS:', 0.056834760688436431, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 975, ']')


'Epoch [976] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1932,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.000709076239436254,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 976,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 976, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235912.132912)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235945.656709)
('Worker processing elapsed time: ', 33.523797035217285, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[976]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 976, ']')


'Epoch [977] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 738,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005133976460112911,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 9, 7, 5, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 977,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 977, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235945.662144)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494235979.497786)
('Worker processing elapsed time: ', 33.83564209938049, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[977]', 'EPOCH LOSS:', 0.02425468322330308, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 977, ']')


'Epoch [978] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 785,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00033389874253719537,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 8, 9, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 978,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 978, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494235979.503033)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236013.178561)
('Worker processing elapsed time: ', 33.675528049468994, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[978]', 'EPOCH LOSS:', 0.024185937207606582, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 978, ']')


'Epoch [979] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1245,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008085429426306064,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 8, 8, 8, 6, 7, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 979,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 979, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236013.184661)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236047.314549)
('Worker processing elapsed time: ', 34.12988805770874, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[979]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 979, ']')


'Epoch [980] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1472,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00012388497392869756,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 9, 9, 4, 9, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 980,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 980, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236047.320908)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236081.914971)
('Worker processing elapsed time: ', 34.59406304359436, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[980]', 'EPOCH LOSS:', 1.4614559820286537, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 980, ']')


'Epoch [981] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 138,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009244425375708046,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 7, 7, 7, 9, 5, 5, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 981,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 981, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236081.921251)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236117.936449)
('Worker processing elapsed time: ', 36.01519799232483, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[981]', 'EPOCH LOSS:', 0.1257746674477592, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 981, ']')


'Epoch [982] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1206,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009706352835956718,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 6, 5, 8, 4, 9, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 982,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 982, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236117.941998)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236152.097099)
('Worker processing elapsed time: ', 34.15510106086731, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[982]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 982, ']')


'Epoch [983] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 154,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0007843319544181927,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 5, 5, 5, 9, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 983,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 983, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236152.10277)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236186.068909)
('Worker processing elapsed time: ', 33.96613883972168, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[983]', 'EPOCH LOSS:', 0.0242790881568304, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 983, ']')


'Epoch [984] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1715,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00021346717587029886,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 984,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 984, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236186.074509)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236219.938165)
('Worker processing elapsed time: ', 33.86365604400635, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[984]', 'EPOCH LOSS:', 348.13219996045626, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 984, ']')


'Epoch [985] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 534,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003067736568567797,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 7, 8, 5, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 985,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 985, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236219.943344)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236253.778974)
('Worker processing elapsed time: ', 33.83562994003296, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[985]', 'EPOCH LOSS:', 0.026489766544289355, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 985, ']')


'Epoch [986] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 595,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0003228527854484724,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 6, 9, 4, 5, 7, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 986,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 986, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236253.785121)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236287.914532)
('Worker processing elapsed time: ', 34.12941098213196, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[986]', 'EPOCH LOSS:', 0.025861761187166956, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 986, ']')


'Epoch [987] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 573,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0005704685174195441,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 987,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 987, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236287.919999)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236321.795981)
('Worker processing elapsed time: ', 33.87598204612732, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[987]', 'EPOCH LOSS:', 2.7552336012722782, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 987, ']')


'Epoch [988] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1802,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00020982197915894274,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 6, 8, 4, 8, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 988,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 988, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236321.801616)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236357.425242)
('Worker processing elapsed time: ', 35.62362599372864, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[988]', 'EPOCH LOSS:', 0.024775262203448695, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 988, ']')


'Epoch [989] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1362,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009455801756563302,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 8, 6, 5, 9, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 989,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 989, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236357.431285)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236392.724705)
('Worker processing elapsed time: ', 35.29342007637024, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[989]', 'EPOCH LOSS:', 0.60095864996278869, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 989, ']')


'Epoch [990] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1213,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0006603145818630812,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 9, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 990,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 990, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236392.730269)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236426.528027)
('Worker processing elapsed time: ', 33.79775810241699, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[990]', 'EPOCH LOSS:', 11.343875723238622, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 990, ']')


'Epoch [991] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1772,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009555043093436163,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 7, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 991,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 991, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236426.533617)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236461.210403)
('Worker processing elapsed time: ', 34.676785945892334, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[991]', 'EPOCH LOSS:', 0.04326880436353106, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 991, ']')


'Epoch [992] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 601,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0008461319973614755,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 5, 8, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 992,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 992, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236461.215928)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236495.041296)
('Worker processing elapsed time: ', 33.82536792755127, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[992]', 'EPOCH LOSS:', 0.02602157587753403, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 992, ']')


'Epoch [993] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1932,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00011184179160649878,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 6, 8, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 993,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 993, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236495.046129)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236529.256895)
('Worker processing elapsed time: ', 34.210766077041626, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[993]', 'EPOCH LOSS:', 2711.1158678432921, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 993, ']')


'Epoch [994] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1240,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0009600642396152615,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 9, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 994,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 994, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236529.261799)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236563.904088)
('Worker processing elapsed time: ', 34.64228892326355, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[994]', 'EPOCH LOSS:', 1.027477186707912, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 994, ']')


'Epoch [995] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1702,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0002880225416274421,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 5, 5, 5, 4, 7, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 995,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 995, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236563.909285)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236598.176021)
('Worker processing elapsed time: ', 34.26673603057861, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[995]', 'EPOCH LOSS:', 0.023552433848566798, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 995, ']')


'Epoch [996] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1558,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00028378481982467357,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 9, 5, 4, 4, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 996,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 996, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236598.181874)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236632.378163)
('Worker processing elapsed time: ', 34.1962890625, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[996]', 'EPOCH LOSS:', 52.463036764304931, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 996, ']')


'Epoch [997] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1503,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00047159899959402945,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 8, 5, 6, 5, 5, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 997,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 997, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236632.383424)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236668.30775)
('Worker processing elapsed time: ', 35.924325942993164, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[997]', 'EPOCH LOSS:', 22.073278504773942, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 997, ']')


'Epoch [998] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1998,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00019578229839166633,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 6, 5, 8, 4, 4, 6, 9, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 998,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 998, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236668.31337)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236702.609252)
('Worker processing elapsed time: ', 34.29588198661804, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[998]', 'EPOCH LOSS:', 0.023545256936559643, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 998, ']')


'Epoch [999] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 1046,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00016961088286921442,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 6, 9, 8, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 999,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 999, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236702.614532)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236737.038491)
('Worker processing elapsed time: ', 34.42395901679993, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[999]', 'EPOCH LOSS:', 3.5754193805837002, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 999, ']')


'Epoch [1000] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 141,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00025494165570527866,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 9, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 1000,
 'opt_epoch_iter': 1000,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 1,
 'sync_replicas': False,
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 1000, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236737.044449)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236771.477244)
('Worker processing elapsed time: ', 34.43279480934143, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[1000]', 'EPOCH LOSS:', 0.35242224204286732, 'BEST LOSS:', 0.010035713448830577)
('END OF Optimizer EPOCH =====================>>[', 1000, ']')


END OF STAGED EPOCH #######################>>[ 1 ]
START OF STAGED EPOCH #######################>> [ 2 ]
'Epoch [1] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 537,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009359108623431255,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 1,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 1, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236771.48378)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236804.595207)
('Worker processing elapsed time: ', 33.11142706871033, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[1]', 'EPOCH LOSS:', 0.0039528391767320575, 'BEST LOSS:', 0.0039528391767320575)
('END OF Optimizer EPOCH =====================>>[', 1, ']')


'Epoch [2] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 529,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005612026085699927,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 2,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 2, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236804.600984)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236838.211064)
('Worker processing elapsed time: ', 33.6100800037384, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[2]', 'EPOCH LOSS:', 0.52417113753288991, 'BEST LOSS:', 0.0039528391767320575)
('END OF Optimizer EPOCH =====================>>[', 2, ']')


'Epoch [3] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 266,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008711660375621594,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 5, 5, 8, 4, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 3,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 3, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236838.216791)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236872.360632)
('Worker processing elapsed time: ', 34.1438410282135, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[3]', 'EPOCH LOSS:', 0.023999930942020426, 'BEST LOSS:', 0.0039528391767320575)
('END OF Optimizer EPOCH =====================>>[', 3, ']')


'Epoch [4] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 613,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004421723369352782,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 4,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 4, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236872.366191)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236906.426336)
('Worker processing elapsed time: ', 34.060145139694214, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[4]', 'EPOCH LOSS:', 0.021264546898825776, 'BEST LOSS:', 0.0039528391767320575)
('END OF Optimizer EPOCH =====================>>[', 4, ']')


'Epoch [5] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 495,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003596712962464524,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 7, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 5,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 5, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236906.431318)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236940.718429)
('Worker processing elapsed time: ', 34.287111043930054, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[5]', 'EPOCH LOSS:', 0.022997071448728677, 'BEST LOSS:', 0.0039528391767320575)
('END OF Optimizer EPOCH =====================>>[', 5, ']')


'Epoch [6] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 556,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000229453932164393,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 6,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 6, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236940.724034)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494236974.140918)
('Worker processing elapsed time: ', 33.41688394546509, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[6]', 'EPOCH LOSS:', 0.026314853926479708, 'BEST LOSS:', 0.0039528391767320575)
('END OF Optimizer EPOCH =====================>>[', 6, ']')


'Epoch [7] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 437,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003641279193010704,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 7, 7, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 7,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 7, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494236974.145732)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237008.019635)
('Worker processing elapsed time: ', 33.873903036117554, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[7]', 'EPOCH LOSS:', 0.02491170049120155, 'BEST LOSS:', 0.0039528391767320575)
('END OF Optimizer EPOCH =====================>>[', 7, ']')


'Epoch [8] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 255,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004417364383881712,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 6, 3, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 8,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 8, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237008.025896)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237041.831605)
('Worker processing elapsed time: ', 33.80570888519287, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[8]', 'EPOCH LOSS:', 0.024751303372206355, 'BEST LOSS:', 0.0039528391767320575)
('END OF Optimizer EPOCH =====================>>[', 8, ']')


'Epoch [9] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 337,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006054218629946407,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 1, 8, 1, 8, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 9,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 9, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237041.837455)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237079.781961)
('Worker processing elapsed time: ', 37.9445059299469, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[9]', 'EPOCH LOSS:', 0.0007575894805059726, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 9, ']')


'Epoch [10] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 391,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009275969879818202,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 10,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 10, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237079.787408)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237113.36577)
('Worker processing elapsed time: ', 33.57836198806763, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[10]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 10, ']')


'Epoch [11] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 542,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008182757716357903,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 1, 5, 8, 8, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 11,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 11, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237113.371114)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237148.873383)
('Worker processing elapsed time: ', 35.50226902961731, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[11]', 'EPOCH LOSS:', 0.019512325970993626, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 11, ']')


'Epoch [12] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 414,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007231616541629965,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 2, 5, 1, 5, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 12,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 12, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237148.878438)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237186.805599)
('Worker processing elapsed time: ', 37.92716097831726, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[12]', 'EPOCH LOSS:', 0.01912999319752719, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 12, ']')


'Epoch [13] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 411,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008553482248468422,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 13,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 13, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237186.81115)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237220.430435)
('Worker processing elapsed time: ', 33.619284868240356, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[13]', 'EPOCH LOSS:', 0.024016286682951596, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 13, ']')


'Epoch [14] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 609,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004966368925628496,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 7, 5, 4, 7, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 14,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 14, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237220.435934)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237256.193354)
('Worker processing elapsed time: ', 35.75741982460022, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[14]', 'EPOCH LOSS:', 0.018994793240406326, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 14, ']')


'Epoch [15] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 235,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00045864539268322517,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 8, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 15,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 15, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237256.198478)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237289.928249)
('Worker processing elapsed time: ', 33.72977089881897, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[15]', 'EPOCH LOSS:', 0.0015042526064792297, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 15, ']')


'Epoch [16] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 304,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00034808776963972507,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 2, 3, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 16,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 16, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237289.934141)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237324.590581)
('Worker processing elapsed time: ', 34.656440019607544, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[16]', 'EPOCH LOSS:', 0.023415474658337466, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 16, ']')


'Epoch [17] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 356,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001454686585968851,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 17,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 17, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237324.595856)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237358.116068)
('Worker processing elapsed time: ', 33.520211935043335, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[17]', 'EPOCH LOSS:', 0.023532098663124088, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 17, ']')


'Epoch [18] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 505,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009490990231994594,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 4, 5, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 18,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 18, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237358.121676)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237391.951259)
('Worker processing elapsed time: ', 33.829582929611206, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[18]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 18, ']')


'Epoch [19] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 216,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008034162228558359,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 4, 3, 6, 1, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 19,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 19, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237391.95673)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237426.606445)
('Worker processing elapsed time: ', 34.649715185165405, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[19]', 'EPOCH LOSS:', 0.42398175090109941, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 19, ']')


'Epoch [20] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 293,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000536443991874114,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 6, 8, 1, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 20,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 20, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237426.61121)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237463.009982)
('Worker processing elapsed time: ', 36.39877200126648, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[20]', 'EPOCH LOSS:', 0.022527881190469359, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 20, ']')


'Epoch [21] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 465,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000623899839099803,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 21,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 21, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237463.015182)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237497.356977)
('Worker processing elapsed time: ', 34.34179496765137, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[21]', 'EPOCH LOSS:', 0.023883850343785654, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 21, ']')


'Epoch [22] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 524,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006344398950773004,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 4, 1, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 22,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 22, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237497.362621)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237533.338328)
('Worker processing elapsed time: ', 35.975706815719604, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[22]', 'EPOCH LOSS:', 0.020378360150066086, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 22, ']')


'Epoch [23] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 431,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005553179999937079,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 8, 4, 7, 5, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 23,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 23, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237533.343221)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237571.618323)
('Worker processing elapsed time: ', 38.27510213851929, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[23]', 'EPOCH LOSS:', 0.021531514442037554, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 23, ']')


'Epoch [24] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 437,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008887985586964586,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 5, 8, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 24,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 24, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237571.62386)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237605.934153)
('Worker processing elapsed time: ', 34.310293197631836, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[24]', 'EPOCH LOSS:', 1959.8635694581506, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 24, ']')


'Epoch [25] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 371,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006385181905311501,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 8, 6, 6, 7, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 25,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 25, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237605.939665)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237644.168238)
('Worker processing elapsed time: ', 38.228572845458984, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[25]', 'EPOCH LOSS:', 0.019924941758738671, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 25, ']')


'Epoch [26] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 238,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009614381772467067,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 26,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 26, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237644.173776)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237677.664884)
('Worker processing elapsed time: ', 33.49110817909241, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[26]', 'EPOCH LOSS:', 0.14289750390177564, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 26, ']')


'Epoch [27] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 498,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000933170268901774,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 27,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 27, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237677.670219)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237710.932529)
('Worker processing elapsed time: ', 33.26231002807617, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[27]', 'EPOCH LOSS:', 0.024636735171367927, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 27, ']')


'Epoch [28] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 261,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007635972871124187,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 28,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 28, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237710.937831)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237744.111521)
('Worker processing elapsed time: ', 33.1736900806427, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[28]', 'EPOCH LOSS:', 1.3566261533425712, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 28, ']')


'Epoch [29] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 236,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00014768215445126027,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 5, 7, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 29,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 29, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237744.116534)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237777.948839)
('Worker processing elapsed time: ', 33.83230495452881, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[29]', 'EPOCH LOSS:', 0.02505239328479646, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 29, ']')


'Epoch [30] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 429,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00048212199946412384,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 5, 6, 4, 4, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 30,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 30, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237777.954407)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237816.360241)
('Worker processing elapsed time: ', 38.40583395957947, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[30]', 'EPOCH LOSS:', 0.024469150349960642, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 30, ']')


'Epoch [31] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 437,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009511295839424726,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 3, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 31,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 31, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237816.366145)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237850.149766)
('Worker processing elapsed time: ', 33.783621072769165, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[31]', 'EPOCH LOSS:', 0.024886127405991283, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 31, ']')


'Epoch [32] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 387,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000855542023165155,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 32,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 32, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237850.156236)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237883.406861)
('Worker processing elapsed time: ', 33.250625133514404, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[32]', 'EPOCH LOSS:', 0.023533423222616822, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 32, ']')


'Epoch [33] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 424,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00040747865078548406,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 33,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 33, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237883.412888)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237916.921652)
('Worker processing elapsed time: ', 33.508764028549194, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[33]', 'EPOCH LOSS:', 0.023570473787205559, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 33, ']')


'Epoch [34] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 541,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003510198299401661,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 6, 6, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 34,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 34, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237916.926476)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237953.06878)
('Worker processing elapsed time: ', 36.14230394363403, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[34]', 'EPOCH LOSS:', 0.018842000702911751, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 34, ']')


'Epoch [35] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 588,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008650111225014952,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 35,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 35, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237953.073631)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494237986.601319)
('Worker processing elapsed time: ', 33.52768802642822, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[35]', 'EPOCH LOSS:', 0.01858935939945262, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 35, ']')


'Epoch [36] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 235,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006582565340936329,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 36,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 36, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494237986.6065)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238020.73272)
('Worker processing elapsed time: ', 34.12621998786926, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[36]', 'EPOCH LOSS:', 0.018764029911564336, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 36, ']')


'Epoch [37] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 226,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004911605292691763,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 3, 7, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 37,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 37, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238020.738685)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238054.681718)
('Worker processing elapsed time: ', 33.94303321838379, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[37]', 'EPOCH LOSS:', 0.024056653632231771, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 37, ']')


'Epoch [38] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 439,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004922313381321652,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 38,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 38, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238054.689861)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238087.773318)
('Worker processing elapsed time: ', 33.08345699310303, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[38]', 'EPOCH LOSS:', 0.011493200814366741, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 38, ']')


'Epoch [39] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 375,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007078232216393334,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 39,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 39, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238087.778663)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238121.730601)
('Worker processing elapsed time: ', 33.95193815231323, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[39]', 'EPOCH LOSS:', 0.016024225340999784, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 39, ']')


'Epoch [40] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 460,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006611695031373606,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 6, 5, 6, 8, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 40,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 40, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238121.736927)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238157.484184)
('Worker processing elapsed time: ', 35.74725699424744, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[40]', 'EPOCH LOSS:', 0.020264209506867997, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 40, ']')


'Epoch [41] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 580,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001834780492826854,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 4, 3, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 41,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 41, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238157.490187)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238191.285447)
('Worker processing elapsed time: ', 33.795259952545166, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[41]', 'EPOCH LOSS:', 0.026268439066503595, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 41, ']')


'Epoch [42] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 351,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002562668704371411,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 42,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 42, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238191.290513)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238224.452132)
('Worker processing elapsed time: ', 33.16161894798279, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[42]', 'EPOCH LOSS:', 0.7361024810932264, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 42, ']')


'Epoch [43] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 573,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00034628028368928834,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 7, 7, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 43,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 43, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238224.458156)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238258.304026)
('Worker processing elapsed time: ', 33.84586977958679, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[43]', 'EPOCH LOSS:', 0.026476399286270994, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 43, ']')


'Epoch [44] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 458,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00038888961832211064,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 8, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 44,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 44, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238258.308859)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238292.017814)
('Worker processing elapsed time: ', 33.70895481109619, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[44]', 'EPOCH LOSS:', 0.0024555277443560797, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 44, ']')


'Epoch [45] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 319,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004614434739512971,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 1, 1, 7, 4, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 45,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 45, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238292.022853)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238326.957942)
('Worker processing elapsed time: ', 34.935089111328125, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[45]', 'EPOCH LOSS:', 0.2636988175538913, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 45, ']')


'Epoch [46] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 506,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003884675223600404,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 3, 3, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 46,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 46, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238326.962877)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238360.769032)
('Worker processing elapsed time: ', 33.80615496635437, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[46]', 'EPOCH LOSS:', 0.025729381872232525, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 46, ']')


'Epoch [47] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 525,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002909387668277657,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 5, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 47,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 47, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238360.774064)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238394.93165)
('Worker processing elapsed time: ', 34.157585859298706, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[47]', 'EPOCH LOSS:', 0.66883468869266716, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 47, ']')


'Epoch [48] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 418,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006440260520924414,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 48,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 48, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238394.937476)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238428.212492)
('Worker processing elapsed time: ', 33.27501606941223, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[48]', 'EPOCH LOSS:', 1.5553083528525111, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 48, ']')


'Epoch [49] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 438,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009164405047435435,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 49,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 49, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238428.218206)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238461.525872)
('Worker processing elapsed time: ', 33.307666063308716, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[49]', 'EPOCH LOSS:', 0.024829838966703991, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 49, ']')


'Epoch [50] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 344,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005875081730017933,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 50,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 50, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238461.531057)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238495.167422)
('Worker processing elapsed time: ', 33.63636517524719, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[50]', 'EPOCH LOSS:', 34.428912528193337, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 50, ']')


'Epoch [51] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 323,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005894685964059586,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 2, 8, 6, 4, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 51,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 51, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238495.172511)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238530.189439)
('Worker processing elapsed time: ', 35.01692795753479, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[51]', 'EPOCH LOSS:', 0.029663267692344255, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 51, ']')


'Epoch [52] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 341,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007218940588620919,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 52,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 52, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238530.194504)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238563.805965)
('Worker processing elapsed time: ', 33.61146092414856, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[52]', 'EPOCH LOSS:', 2.5554075603858419, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 52, ']')


'Epoch [53] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 450,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007102645079330942,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 53,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 53, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238563.811719)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238597.238282)
('Worker processing elapsed time: ', 33.426563024520874, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[53]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 53, ']')


'Epoch [54] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 212,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00013536632776702407,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 2, 3, 7, 6, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 54,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 54, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238597.244469)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238631.862628)
('Worker processing elapsed time: ', 34.61815905570984, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[54]', 'EPOCH LOSS:', 508.71112446897178, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 54, ']')


'Epoch [55] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 396,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007335388203349234,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 3, 1, 5, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 55,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 55, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238631.869088)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238666.279751)
('Worker processing elapsed time: ', 34.41066312789917, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[55]', 'EPOCH LOSS:', 0.024371868982169913, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 55, ']')


'Epoch [56] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 402,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001641641720980531,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 56,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 56, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238666.285409)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238699.853105)
('Worker processing elapsed time: ', 33.56769609451294, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[56]', 'EPOCH LOSS:', 0.038269022903362031, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 56, ']')


'Epoch [57] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 504,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007872249924730076,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 4, 8, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 57,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 57, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238699.858626)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238734.050502)
('Worker processing elapsed time: ', 34.19187617301941, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[57]', 'EPOCH LOSS:', 0.29180911482747873, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 57, ']')


'Epoch [58] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 449,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006856691244375931,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 58,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 58, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238734.056515)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238767.846866)
('Worker processing elapsed time: ', 33.790350914001465, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[58]', 'EPOCH LOSS:', 0.023232690458396797, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 58, ']')


'Epoch [59] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 519,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003471774681373706,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 7, 6, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 59,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 59, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238767.852051)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238801.777231)
('Worker processing elapsed time: ', 33.925179958343506, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[59]', 'EPOCH LOSS:', 0.0021439056251177009, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 59, ']')


'Epoch [60] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 354,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000761270045708492,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 60,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 60, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238801.782388)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238835.546902)
('Worker processing elapsed time: ', 33.76451396942139, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[60]', 'EPOCH LOSS:', 4.0514380498189473, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 60, ']')


'Epoch [61] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 338,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00014731854085660637,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 61,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 61, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238835.552993)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238869.646622)
('Worker processing elapsed time: ', 34.093628883361816, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[61]', 'EPOCH LOSS:', 0.029498783018345798, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 61, ']')


'Epoch [62] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 359,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005416462743293941,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 4, 7, 5, 7, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 62,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 62, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238869.652026)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238904.971058)
('Worker processing elapsed time: ', 35.319031953811646, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[62]', 'EPOCH LOSS:', 0.19145460253319005, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 62, ']')


'Epoch [63] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 323,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00017000096846275477,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 8, 4, 8, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 63,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 63, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238904.976122)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238938.994517)
('Worker processing elapsed time: ', 34.01839518547058, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[63]', 'EPOCH LOSS:', 0.024640031521256576, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 63, ']')


'Epoch [64] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 250,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002321529428153886,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 64,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 64, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238939.000492)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494238972.249328)
('Worker processing elapsed time: ', 33.24883580207825, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[64]', 'EPOCH LOSS:', 1.0382851671438047, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 64, ']')


'Epoch [65] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 273,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007877325709904853,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 65,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 65, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494238972.254438)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239005.495577)
('Worker processing elapsed time: ', 33.24113917350769, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[65]', 'EPOCH LOSS:', 0.0016899613420274585, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 65, ']')


'Epoch [66] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 599,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004891951022639863,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 4, 7, 2, 3, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 66,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 66, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239005.501861)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239041.946062)
('Worker processing elapsed time: ', 36.44420099258423, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[66]', 'EPOCH LOSS:', 1.0976291527132391, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 66, ']')


'Epoch [67] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 246,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008605346694615058,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 5, 3, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 67,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 67, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239041.951551)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239076.527951)
('Worker processing elapsed time: ', 34.5764000415802, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[67]', 'EPOCH LOSS:', 0.023699396951348626, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 67, ']')


'Epoch [68] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 502,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006264817222402296,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 8, 6, 1, 2, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 68,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 68, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239076.53391)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239110.537563)
('Worker processing elapsed time: ', 34.003653049468994, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[68]', 'EPOCH LOSS:', 0.024492031699490699, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 68, ']')


'Epoch [69] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 220,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008999245905581311,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 8, 4, 6, 8, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 69,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 69, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239110.542739)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239145.207765)
('Worker processing elapsed time: ', 34.66502618789673, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[69]', 'EPOCH LOSS:', 0.025609375593246178, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 69, ']')


'Epoch [70] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 262,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00048441408046062825,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 3, 3, 7, 8, 7, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 70,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 70, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239145.214137)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239182.790916)
('Worker processing elapsed time: ', 37.57677888870239, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[70]', 'EPOCH LOSS:', 0.015530560767400819, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 70, ']')


'Epoch [71] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 619,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002404972549212021,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 6, 5, 4, 4, 7, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 71,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 71, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239182.796127)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239222.285777)
('Worker processing elapsed time: ', 39.48965001106262, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[71]', 'EPOCH LOSS:', 0.021732379650865885, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 71, ']')


'Epoch [72] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 448,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008161574648610683,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 7, 6, 7, 5, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 72,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 72, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239222.290759)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239257.317443)
('Worker processing elapsed time: ', 35.02668380737305, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[72]', 'EPOCH LOSS:', 0.67227884248679648, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 72, ']')


'Epoch [73] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 590,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00030084985836732465,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 73,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 73, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239257.323267)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239290.519233)
('Worker processing elapsed time: ', 33.19596600532532, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[73]', 'EPOCH LOSS:', 1.7097289640848454, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 73, ']')


'Epoch [74] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 211,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007105985021168132,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 74,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 74, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239290.524256)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239323.547906)
('Worker processing elapsed time: ', 33.02364993095398, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[74]', 'EPOCH LOSS:', 0.014680782477520931, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 74, ']')


'Epoch [75] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 319,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00036404522307427716,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 7, 5, 4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 75,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 75, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239323.553822)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239358.737303)
('Worker processing elapsed time: ', 35.183480978012085, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[75]', 'EPOCH LOSS:', 0.024979225008588107, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 75, ']')


'Epoch [76] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 456,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007445079747740097,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 3, 3, 2, 6, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 76,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 76, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239358.743418)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239394.340844)
('Worker processing elapsed time: ', 35.59742593765259, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[76]', 'EPOCH LOSS:', 29.586965329331608, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 76, ']')


'Epoch [77] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 262,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00019586095752925148,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 8, 4, 1, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 77,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 77, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239394.34691)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239428.223882)
('Worker processing elapsed time: ', 33.87697196006775, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[77]', 'EPOCH LOSS:', 0.024273732211673071, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 77, ']')


'Epoch [78] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 384,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00038986846932798254,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 3, 5, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 78,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 78, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239428.229457)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239462.263575)
('Worker processing elapsed time: ', 34.03411817550659, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[78]', 'EPOCH LOSS:', 0.96345221080176968, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 78, ']')


'Epoch [79] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 371,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00012397039674497178,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 1, 1, 3, 2, 1, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 79,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 79, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239462.269394)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239497.412635)
('Worker processing elapsed time: ', 35.14324116706848, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[79]', 'EPOCH LOSS:', 0.19013032358104612, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 79, ']')


'Epoch [80] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 363,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002883083910658814,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 3, 1, 2, 2, 7, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 80,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 80, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239497.418262)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239535.306971)
('Worker processing elapsed time: ', 37.88870906829834, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[80]', 'EPOCH LOSS:', 0.018268587812381897, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 80, ']')


'Epoch [81] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 537,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008565502530755782,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 81,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 81, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239535.312307)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239568.782619)
('Worker processing elapsed time: ', 33.47031211853027, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[81]', 'EPOCH LOSS:', 0.018796074104792993, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 81, ']')


'Epoch [82] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 611,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001942765007041193,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 82,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 82, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239568.788268)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239602.852557)
('Worker processing elapsed time: ', 34.064288854599, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[82]', 'EPOCH LOSS:', 0.036232252386023137, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 82, ']')


'Epoch [83] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 284,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00011216285727401996,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 7, 6, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 83,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 83, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239602.858368)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239637.879596)
('Worker processing elapsed time: ', 35.021228075027466, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[83]', 'EPOCH LOSS:', 1.3888813412515546, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 83, ']')


'Epoch [84] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 438,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000423619551421206,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 1, 7, 4, 1, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 84,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 84, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239637.885007)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239673.214266)
('Worker processing elapsed time: ', 35.329259157180786, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[84]', 'EPOCH LOSS:', 6.515837018864322, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 84, ']')


'Epoch [85] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 423,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000705001235192131,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 85,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 85, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239673.219057)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239706.807765)
('Worker processing elapsed time: ', 33.58870792388916, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[85]', 'EPOCH LOSS:', 2.0667819542866694, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 85, ']')


'Epoch [86] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 261,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006308178173184309,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 2, 7, 4, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 86,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 86, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239706.813633)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239743.164642)
('Worker processing elapsed time: ', 36.351009130477905, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[86]', 'EPOCH LOSS:', 0.023021752578592361, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 86, ']')


'Epoch [87] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 535,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00027012679797016356,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 87,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 87, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239743.170103)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239776.437981)
('Worker processing elapsed time: ', 33.26787781715393, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[87]', 'EPOCH LOSS:', 1.4918380534152818, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 87, ']')


'Epoch [88] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 509,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008737038648191292,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 88,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 88, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239776.443704)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239809.851969)
('Worker processing elapsed time: ', 33.408265113830566, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[88]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 88, ']')


'Epoch [89] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 413,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00026655814502243706,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 89,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 89, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239809.856957)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239843.376097)
('Worker processing elapsed time: ', 33.519140005111694, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[89]', 'EPOCH LOSS:', 1.9026909242908323, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 89, ']')


'Epoch [90] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 540,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00018103814753763002,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 7, 7, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 90,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 90, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239843.380999)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239877.280022)
('Worker processing elapsed time: ', 33.899022817611694, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[90]', 'EPOCH LOSS:', 0.026301792466581334, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 90, ']')


'Epoch [91] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 567,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003109052564789612,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 4, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 91,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 91, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239877.285669)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239911.84951)
('Worker processing elapsed time: ', 34.56384086608887, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[91]', 'EPOCH LOSS:', 0.027435777572994952, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 91, ']')


'Epoch [92] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 405,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000770281762913288,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 5, 3, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 92,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 92, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239911.854493)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239945.69157)
('Worker processing elapsed time: ', 33.837077140808105, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[92]', 'EPOCH LOSS:', 0.024283696582711234, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 92, ']')


'Epoch [93] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 541,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008446409080507195,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 93,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 93, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239945.69714)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494239979.480096)
('Worker processing elapsed time: ', 33.78295612335205, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[93]', 'EPOCH LOSS:', 0.027080101386637023, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 93, ']')


'Epoch [94] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 313,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005911983178130284,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 94,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 94, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494239979.484989)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240012.889622)
('Worker processing elapsed time: ', 33.40463304519653, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[94]', 'EPOCH LOSS:', 0.021252939819245847, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 94, ']')


'Epoch [95] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 581,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006487116810790574,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 95,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 95, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240012.894969)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240046.934634)
('Worker processing elapsed time: ', 34.03966498374939, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[95]', 'EPOCH LOSS:', 0.026214476626233725, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 95, ']')


'Epoch [96] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 232,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007795702348173261,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 3, 3, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 96,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 96, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240046.939476)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240081.095457)
('Worker processing elapsed time: ', 34.15598106384277, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[96]', 'EPOCH LOSS:', 1.6946083462451287, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 96, ']')


'Epoch [97] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 365,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008193927363035533,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 97,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 97, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240081.101305)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240114.578659)
('Worker processing elapsed time: ', 33.47735404968262, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[97]', 'EPOCH LOSS:', 2.105891610544171, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 97, ']')


'Epoch [98] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 325,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00043361143975001546,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 8, 4, 3, 8, 1, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 98,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 98, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240114.584868)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240148.825015)
('Worker processing elapsed time: ', 34.24014711380005, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[98]', 'EPOCH LOSS:', 0.024488404279295717, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 98, ']')


'Epoch [99] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 383,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005907002622839587,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 1, 1, 6, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 99,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 99, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240148.8301)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240185.308762)
('Worker processing elapsed time: ', 36.47866201400757, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[99]', 'EPOCH LOSS:', 0.022884366244744762, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 99, ']')


'Epoch [100] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 552,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00024850437791164423,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 100,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 100, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240185.314205)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240218.4389)
('Worker processing elapsed time: ', 33.12469506263733, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[100]', 'EPOCH LOSS:', 0.016210723300388526, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 100, ']')


'Epoch [101] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 361,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000960423527527495,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 1, 3, 4, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 101,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 101, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240218.444039)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240252.34639)
('Worker processing elapsed time: ', 33.90235090255737, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[101]', 'EPOCH LOSS:', 0.023204936746023131, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 101, ']')


'Epoch [102] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 344,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001132509957929395,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 102,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 102, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240252.351551)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240285.603671)
('Worker processing elapsed time: ', 33.25212001800537, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[102]', 'EPOCH LOSS:', 2.259585931139434, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 102, ']')


'Epoch [103] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 260,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009355416418241288,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 3, 2, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 103,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 103, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240285.608821)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240320.158011)
('Worker processing elapsed time: ', 34.549190044403076, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[103]', 'EPOCH LOSS:', 0.015444567413217293, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 103, ']')


'Epoch [104] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 427,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009645906527597554,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 1, 4, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 104,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 104, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240320.163487)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240354.40287)
('Worker processing elapsed time: ', 34.23938298225403, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[104]', 'EPOCH LOSS:', 0.99591962402806933, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 104, ']')


'Epoch [105] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 255,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007508397113390265,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 3, 3, 4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 105,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 105, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240354.409097)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240390.68642)
('Worker processing elapsed time: ', 36.27732300758362, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[105]', 'EPOCH LOSS:', 0.022328409826534591, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 105, ']')


'Epoch [106] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 223,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005635566286681349,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 8, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 106,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 106, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240390.69255)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240425.269511)
('Worker processing elapsed time: ', 34.576961040496826, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[106]', 'EPOCH LOSS:', 0.025925460247342276, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 106, ']')


'Epoch [107] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 348,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000967936183458226,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 8, 2, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 107,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 107, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240425.274442)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240459.929953)
('Worker processing elapsed time: ', 34.655511140823364, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[107]', 'EPOCH LOSS:', 0.021947918160994784, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 107, ']')


'Epoch [108] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 423,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00043116234119567394,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 7, 2, 5, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 108,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 108, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240459.934895)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240496.810094)
('Worker processing elapsed time: ', 36.87519907951355, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[108]', 'EPOCH LOSS:', 0.020243778631116035, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 108, ']')


'Epoch [109] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 612,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003025883245891882,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 3, 7, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 109,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 109, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240496.815277)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240533.078469)
('Worker processing elapsed time: ', 36.26319193840027, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[109]', 'EPOCH LOSS:', 0.020458616932360264, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 109, ']')


'Epoch [110] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 380,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00022445765454669404,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 1, 2, 4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 110,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 110, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240533.083365)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240567.320978)
('Worker processing elapsed time: ', 34.23761296272278, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[110]', 'EPOCH LOSS:', 5.7784140870200611, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 110, ']')


'Epoch [111] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 232,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008487270549171829,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 111,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 111, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240567.326013)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240601.655211)
('Worker processing elapsed time: ', 34.32919788360596, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[111]', 'EPOCH LOSS:', 0.024942851933562805, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 111, ']')


'Epoch [112] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 298,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004947589872355761,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1, 1, 5, 2, 6, 3, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 112,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 112, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240601.660407)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240635.759178)
('Worker processing elapsed time: ', 34.0987708568573, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[112]', 'EPOCH LOSS:', 0.022736020930901487, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 112, ']')


'Epoch [113] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 368,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006602264043650467,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 2, 1, 7, 8, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 113,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 113, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240635.764256)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240673.577912)
('Worker processing elapsed time: ', 37.81365609169006, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[113]', 'EPOCH LOSS:', 0.0012479816698286669, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 113, ']')


'Epoch [114] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 354,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006270455259537167,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 2, 3, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 114,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 114, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240673.584016)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240707.441343)
('Worker processing elapsed time: ', 33.85732698440552, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[114]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 114, ']')


'Epoch [115] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 336,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008675308416639382,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 8, 4, 6, 1, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 115,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 115, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240707.446589)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240741.843581)
('Worker processing elapsed time: ', 34.39699196815491, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[115]', 'EPOCH LOSS:', 1.0073282971333304, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 115, ']')


'Epoch [116] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 233,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00040029945787576596,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 116,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 116, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240741.848482)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240775.438165)
('Worker processing elapsed time: ', 33.589683055877686, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[116]', 'EPOCH LOSS:', 20.110881796355276, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 116, ']')


'Epoch [117] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 353,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005089888434023186,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 117,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 117, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240775.443124)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240808.679119)
('Worker processing elapsed time: ', 33.235995054244995, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[117]', 'EPOCH LOSS:', 0.020523951401433309, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 117, ']')


'Epoch [118] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 297,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000408571938555092,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 118,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 118, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240808.685094)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240842.534599)
('Worker processing elapsed time: ', 33.84950494766235, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[118]', 'EPOCH LOSS:', 4.1314724219768744, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 118, ']')


'Epoch [119] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 248,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008021458023216604,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 3, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 119,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 119, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240842.540633)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240876.363763)
('Worker processing elapsed time: ', 33.82313013076782, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[119]', 'EPOCH LOSS:', 0.059542231116856005, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 119, ']')


'Epoch [120] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 247,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002069437196980195,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 5, 1, 5, 6, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 120,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 120, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240876.369514)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240910.948196)
('Worker processing elapsed time: ', 34.57868194580078, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[120]', 'EPOCH LOSS:', 2.7267455906960167, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 120, ']')


'Epoch [121] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 294,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002817069917773158,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 7, 2, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 121,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 121, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240910.953631)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240945.923341)
('Worker processing elapsed time: ', 34.96971011161804, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[121]', 'EPOCH LOSS:', 0.11727679245365005, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 121, ']')


'Epoch [122] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 466,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003776532366077948,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 122,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 122, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240945.928986)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494240979.986471)
('Worker processing elapsed time: ', 34.0574848651886, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[122]', 'EPOCH LOSS:', 0.023178990206736555, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 122, ']')


'Epoch [123] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 489,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005993881859278775,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 5, 2, 8, 4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 123,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 123, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494240979.991931)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241015.129698)
('Worker processing elapsed time: ', 35.13776707649231, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[123]', 'EPOCH LOSS:', 0.020687134876538484, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 123, ']')


'Epoch [124] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 607,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007440004363746874,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 1, 8, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 124,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 124, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241015.134672)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241048.967514)
('Worker processing elapsed time: ', 33.832842111587524, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[124]', 'EPOCH LOSS:', 0.025832981427142559, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 124, ']')


'Epoch [125] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 372,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006388800740745487,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 7, 2, 4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 125,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 125, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241048.973562)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241083.400729)
('Worker processing elapsed time: ', 34.42716693878174, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[125]', 'EPOCH LOSS:', 34.05667803420873, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 125, ']')


'Epoch [126] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 300,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00019999432515859688,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 4, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 126,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 126, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241083.406387)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241117.143464)
('Worker processing elapsed time: ', 33.73707699775696, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[126]', 'EPOCH LOSS:', 0.022602320255818747, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 126, ']')


'Epoch [127] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 527,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005681865021043263,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 1, 1, 8, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 127,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 127, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241117.148691)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241151.350128)
('Worker processing elapsed time: ', 34.20143699645996, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[127]', 'EPOCH LOSS:', 0.5401806588408683, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 127, ']')


'Epoch [128] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 209,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005513748196357795,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 128,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 128, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241151.35579)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241184.84844)
('Worker processing elapsed time: ', 33.49265003204346, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[128]', 'EPOCH LOSS:', 0.023814926383002446, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 128, ']')


'Epoch [129] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 232,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007945332974811728,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 129,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 129, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241184.853658)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241218.04886)
('Worker processing elapsed time: ', 33.195202112197876, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[129]', 'EPOCH LOSS:', 0.86639089826286975, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 129, ']')


'Epoch [130] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 467,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005700216904990425,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 130,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 130, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241218.053715)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241251.436344)
('Worker processing elapsed time: ', 33.38262891769409, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[130]', 'EPOCH LOSS:', 0.023740739328168937, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 130, ']')


'Epoch [131] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 618,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009306658560186194,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 131,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 131, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241251.441686)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241284.893224)
('Worker processing elapsed time: ', 33.4515380859375, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[131]', 'EPOCH LOSS:', 0.21066856963544017, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 131, ']')


'Epoch [132] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 264,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009281186152029988,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 132,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 132, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241284.898242)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241318.958331)
('Worker processing elapsed time: ', 34.060089111328125, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[132]', 'EPOCH LOSS:', 0.023076975297756774, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 132, ']')


'Epoch [133] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 431,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00016350405141859808,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 133,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 133, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241318.964258)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241356.115116)
('Worker processing elapsed time: ', 37.15085792541504, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[133]', 'EPOCH LOSS:', 0.017674926762367985, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 133, ']')


'Epoch [134] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 389,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000614600955952011,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 8, 5, 6, 3, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 134,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 134, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241356.120925)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241391.424426)
('Worker processing elapsed time: ', 35.30350112915039, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[134]', 'EPOCH LOSS:', 1.1981416732775809, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 134, ']')


'Epoch [135] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 460,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007761900727358185,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 135,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 135, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241391.429874)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241424.824199)
('Worker processing elapsed time: ', 33.39432501792908, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[135]', 'EPOCH LOSS:', 0.087001832014289029, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 135, ']')


'Epoch [136] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 352,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007659841174866117,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 136,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 136, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241424.829665)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241457.935756)
('Worker processing elapsed time: ', 33.106091022491455, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[136]', 'EPOCH LOSS:', 0.0061217120568927801, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 136, ']')


'Epoch [137] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 551,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00026262813245711753,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 137,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 137, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241457.941785)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241491.52445)
('Worker processing elapsed time: ', 33.58266496658325, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[137]', 'EPOCH LOSS:', 0.023033488385868647, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 137, ']')


'Epoch [138] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 263,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007676744350115494,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 138,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 138, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241491.530315)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241525.235822)
('Worker processing elapsed time: ', 33.705507040023804, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[138]', 'EPOCH LOSS:', 0.017685227473458277, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 138, ']')


'Epoch [139] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 402,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000462362922950411,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 139,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 139, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241525.240938)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241558.638518)
('Worker processing elapsed time: ', 33.39758014678955, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[139]', 'EPOCH LOSS:', 0.024461167246891163, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 139, ']')


'Epoch [140] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 318,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003513726963847038,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 140,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 140, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241558.643448)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241592.754827)
('Worker processing elapsed time: ', 34.11137890815735, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[140]', 'EPOCH LOSS:', 0.024717785200330952, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 140, ']')


'Epoch [141] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 261,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006719328563526245,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 5, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 141,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 141, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241592.760051)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241627.329693)
('Worker processing elapsed time: ', 34.569642066955566, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[141]', 'EPOCH LOSS:', 0.026411111637049494, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 141, ']')


'Epoch [142] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 409,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000517465604348423,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 142,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 142, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241627.335173)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241661.008056)
('Worker processing elapsed time: ', 33.67288303375244, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[142]', 'EPOCH LOSS:', 0.021060463402225067, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 142, ']')


'Epoch [143] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 230,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003739798661259498,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 143,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 143, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241661.013021)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241694.554843)
('Worker processing elapsed time: ', 33.54182195663452, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[143]', 'EPOCH LOSS:', 0.025445740985208423, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 143, ']')


'Epoch [144] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 436,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000952056196790691,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 144,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 144, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241694.55991)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241728.096634)
('Worker processing elapsed time: ', 33.53672385215759, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[144]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 144, ']')


'Epoch [145] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 572,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00025032944097095266,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 145,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 145, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241728.10185)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241761.478188)
('Worker processing elapsed time: ', 33.37633800506592, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[145]', 'EPOCH LOSS:', 1.9093038585310886, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 145, ']')


'Epoch [146] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 560,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003724602897133702,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 4, 7, 2, 7, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 146,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 146, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241761.484251)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241797.495647)
('Worker processing elapsed time: ', 36.0113959312439, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[146]', 'EPOCH LOSS:', 0.359013760419163, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 146, ']')


'Epoch [147] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 247,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007377268729254443,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 147,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 147, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241797.501555)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241831.209374)
('Worker processing elapsed time: ', 33.70781898498535, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[147]', 'EPOCH LOSS:', 0.0045465945834796901, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 147, ']')


'Epoch [148] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 614,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009885304804243284,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 2, 7, 2, 4, 2, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 148,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 148, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241831.214623)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241867.41829)
('Worker processing elapsed time: ', 36.2036669254303, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[148]', 'EPOCH LOSS:', 0.17674976843608162, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 148, ']')


'Epoch [149] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 244,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005601123565936888,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 149,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 149, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241867.424289)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241901.883705)
('Worker processing elapsed time: ', 34.459415912628174, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[149]', 'EPOCH LOSS:', 0.021197229296417659, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 149, ']')


'Epoch [150] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 447,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005581326720755743,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 6, 5, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 150,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 150, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241901.889979)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241937.416484)
('Worker processing elapsed time: ', 35.5265052318573, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[150]', 'EPOCH LOSS:', 0.024670332821636037, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 150, ']')


'Epoch [151] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 345,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006429193411565525,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 2, 4, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 151,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 151, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241937.421879)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494241971.216621)
('Worker processing elapsed time: ', 33.79474186897278, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[151]', 'EPOCH LOSS:', 0.02395299849974097, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 151, ']')


'Epoch [152] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 546,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005882120992180154,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 1, 7, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 152,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 152, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494241971.222221)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242005.038493)
('Worker processing elapsed time: ', 33.816272020339966, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[152]', 'EPOCH LOSS:', 0.026040293679253208, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 152, ']')


'Epoch [153] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 390,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00036372824500120185,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 153,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 153, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242005.044658)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242038.70539)
('Worker processing elapsed time: ', 33.66073203086853, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[153]', 'EPOCH LOSS:', 1.3211818939481443, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 153, ']')


'Epoch [154] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 447,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00014483897975007675,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 154,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 154, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242038.710317)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242072.044349)
('Worker processing elapsed time: ', 33.33403205871582, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[154]', 'EPOCH LOSS:', 0.02419718425581267, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 154, ']')


'Epoch [155] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 221,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00039335217384206945,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 155,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 155, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242072.049816)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242105.555231)
('Worker processing elapsed time: ', 33.505415201187134, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[155]', 'EPOCH LOSS:', 0.02223052408870313, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 155, ']')


'Epoch [156] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 503,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006177942032172354,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 8, 4, 1, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 156,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 156, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242105.560286)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242139.503758)
('Worker processing elapsed time: ', 33.943471908569336, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[156]', 'EPOCH LOSS:', 0.025104431619567078, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 156, ']')


'Epoch [157] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 255,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009496979691444553,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 1, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 157,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 157, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242139.509696)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242173.497839)
('Worker processing elapsed time: ', 33.98814296722412, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[157]', 'EPOCH LOSS:', 4.0781087270222924, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 157, ']')


'Epoch [158] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 498,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002685744016103389,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 158,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 158, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242173.503506)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242206.566106)
('Worker processing elapsed time: ', 33.06260013580322, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[158]', 'EPOCH LOSS:', 0.014835447243180906, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 158, ']')


'Epoch [159] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 496,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004707303133484864,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 159,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 159, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242206.571431)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242240.450722)
('Worker processing elapsed time: ', 33.87929105758667, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[159]', 'EPOCH LOSS:', 1.1753920204813109, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 159, ']')


'Epoch [160] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 237,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00024810057819977887,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 160,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 160, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242240.456623)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242273.842573)
('Worker processing elapsed time: ', 33.3859498500824, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[160]', 'EPOCH LOSS:', 0.022833697843830086, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 160, ']')


'Epoch [161] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 250,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00036749582916908703,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 8, 3, 6, 8, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 161,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 161, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242273.847775)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242310.239175)
('Worker processing elapsed time: ', 36.39140009880066, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[161]', 'EPOCH LOSS:', 0.01388450355499354, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 161, ']')


'Epoch [162] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 283,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007572119463773741,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 1, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 162,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 162, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242310.245247)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242344.299756)
('Worker processing elapsed time: ', 34.05450916290283, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[162]', 'EPOCH LOSS:', 0.16748695276234374, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 162, ']')


'Epoch [163] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 369,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00012853197315437965,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 5, 3, 2, 8, 3, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 163,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 163, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242344.305621)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242378.985721)
('Worker processing elapsed time: ', 34.680100202560425, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[163]', 'EPOCH LOSS:', 0.02239602935993016, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 163, ']')


'Epoch [164] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 556,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005174711076558404,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 6, 3, 6, 2, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 164,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 164, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242378.991624)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242415.092537)
('Worker processing elapsed time: ', 36.10091280937195, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[164]', 'EPOCH LOSS:', 3.1742666153098629, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 164, ']')


'Epoch [165] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 295,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000715872674538853,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 165,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 165, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242415.097976)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242449.110879)
('Worker processing elapsed time: ', 34.0129029750824, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[165]', 'EPOCH LOSS:', 0.021351304930594884, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 165, ']')


'Epoch [166] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 366,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007512877815233302,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 166,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 166, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242449.115944)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242482.643018)
('Worker processing elapsed time: ', 33.527074098587036, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[166]', 'EPOCH LOSS:', 0.022964958570765897, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 166, ']')


'Epoch [167] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 482,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007674236009622197,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 2, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 167,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 167, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242482.647942)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242516.213911)
('Worker processing elapsed time: ', 33.56596899032593, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[167]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 167, ']')


'Epoch [168] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 370,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006996556040290063,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 3, 3, 3, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 168,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 168, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242516.219489)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242549.951802)
('Worker processing elapsed time: ', 33.73231291770935, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[168]', 'EPOCH LOSS:', 0.02276035428591984, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 168, ']')


'Epoch [169] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 616,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002331222483630993,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 4, 4, 2, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 169,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 169, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242549.956606)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242587.445477)
('Worker processing elapsed time: ', 37.4888710975647, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[169]', 'EPOCH LOSS:', 0.020064552393635981, 'BEST LOSS:', 0.0007575894805059726)
('END OF Optimizer EPOCH =====================>>[', 169, ']')


'Epoch [170] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 507,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009224346912679274,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 7, 7, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 170,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 170, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242587.450603)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242624.3116)
('Worker processing elapsed time: ', 36.86099696159363, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[170]', 'EPOCH LOSS:', 2.5544787520616208e-05, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 170, ']')


'Epoch [171] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 599,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005515047423482679,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 3, 2, 8, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 171,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 171, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242624.317608)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242659.343436)
('Worker processing elapsed time: ', 35.02582788467407, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[171]', 'EPOCH LOSS:', 0.63219738204193732, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 171, ']')


'Epoch [172] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 561,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00011473712731751933,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 2, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 172,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 172, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242659.348976)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242693.433656)
('Worker processing elapsed time: ', 34.08468008041382, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[172]', 'EPOCH LOSS:', 7.5316604717830673, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 172, ']')


'Epoch [173] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 424,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006608878152709326,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 173,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 173, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242693.439017)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242726.692532)
('Worker processing elapsed time: ', 33.253515005111694, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[173]', 'EPOCH LOSS:', 0.0051698614947308166, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 173, ']')


'Epoch [174] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 521,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008704819822396149,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 8, 7, 7, 6, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 174,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 174, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242726.698806)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242762.332596)
('Worker processing elapsed time: ', 35.633790016174316, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[174]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 174, ']')


'Epoch [175] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 340,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008409856185460958,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 175,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 175, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242762.338594)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242795.771956)
('Worker processing elapsed time: ', 33.43336200714111, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[175]', 'EPOCH LOSS:', 2.1084695777068041, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 175, ']')


'Epoch [176] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 503,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007476793690566319,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 6, 2, 1, 1, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 176,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 176, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242795.776991)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242829.845794)
('Worker processing elapsed time: ', 34.06880307197571, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[176]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 176, ']')


'Epoch [177] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 437,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007001717823708549,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 177,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 177, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242829.85157)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242864.201555)
('Worker processing elapsed time: ', 34.349985122680664, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[177]', 'EPOCH LOSS:', 0.024312815156213911, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 177, ']')


'Epoch [178] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 281,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002565990533990801,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 178,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 178, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242864.207234)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242897.424379)
('Worker processing elapsed time: ', 33.21714520454407, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[178]', 'EPOCH LOSS:', 0.023477971672386869, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 178, ']')


'Epoch [179] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 228,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005729533425753617,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 179,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 179, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242897.429786)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242931.387409)
('Worker processing elapsed time: ', 33.95762300491333, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[179]', 'EPOCH LOSS:', 0.068672037946791095, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 179, ']')


'Epoch [180] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 213,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007407356839670646,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 6, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 180,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 180, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242931.392941)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494242965.356191)
('Worker processing elapsed time: ', 33.963249921798706, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[180]', 'EPOCH LOSS:', 22.474927282379227, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 180, ']')


'Epoch [181] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 445,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00011659437328784576,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 5, 5, 6, 1, 1, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 181,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 181, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494242965.361745)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243000.221633)
('Worker processing elapsed time: ', 34.85988783836365, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[181]', 'EPOCH LOSS:', 0.016363096974238007, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 181, ']')


'Epoch [182] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 553,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003794908965661653,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 182,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 182, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243000.226555)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243034.112298)
('Worker processing elapsed time: ', 33.88574290275574, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[182]', 'EPOCH LOSS:', 1.1468691346275415, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 182, ']')


'Epoch [183] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 414,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004286301408361604,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 1, 3, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 183,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 183, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243034.117944)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243068.014743)
('Worker processing elapsed time: ', 33.896799087524414, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[183]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 183, ']')


'Epoch [184] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 456,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002015579772017143,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 5, 2, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 184,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 184, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243068.020997)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243101.784263)
('Worker processing elapsed time: ', 33.76326584815979, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[184]', 'EPOCH LOSS:', 0.024088625964566405, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 184, ']')


'Epoch [185] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 589,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007899353220203238,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 4, 2, 7, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 185,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 185, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243101.789471)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243135.957672)
('Worker processing elapsed time: ', 34.168201208114624, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[185]', 'EPOCH LOSS:', 10.228193288774207, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 185, ']')


'Epoch [186] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 388,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007809680917908007,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 5, 1, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 186,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 186, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243135.963733)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243171.181459)
('Worker processing elapsed time: ', 35.21772599220276, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[186]', 'EPOCH LOSS:', 0.023504619581774171, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 186, ']')


'Epoch [187] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 285,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008807159203516211,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 8, 1, 1, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 187,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 187, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243171.186378)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243205.746233)
('Worker processing elapsed time: ', 34.55985498428345, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[187]', 'EPOCH LOSS:', 0.020532121706770372, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 187, ']')


'Epoch [188] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 244,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00041479862214088077,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 188,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 188, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243205.751679)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243239.787049)
('Worker processing elapsed time: ', 34.035370111465454, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[188]', 'EPOCH LOSS:', 0.026654959689657663, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 188, ']')


'Epoch [189] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 293,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001583121413024608,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 189,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 189, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243239.792856)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243273.811554)
('Worker processing elapsed time: ', 34.01869797706604, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[189]', 'EPOCH LOSS:', 0.027434820622573384, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 189, ']')


'Epoch [190] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 261,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001495094472676624,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 3, 3, 8, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 190,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 190, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243273.817043)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243307.848383)
('Worker processing elapsed time: ', 34.03133988380432, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[190]', 'EPOCH LOSS:', 0.024231804034519092, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 190, ']')


'Epoch [191] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 426,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00013917654543945258,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 1, 2, 8, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 191,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 191, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243307.853204)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243343.352354)
('Worker processing elapsed time: ', 35.4991500377655, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[191]', 'EPOCH LOSS:', 0.018659039920147936, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 191, ']')


'Epoch [192] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 592,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004492950385517404,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 192,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 192, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243343.357929)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243376.386421)
('Worker processing elapsed time: ', 33.02849197387695, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[192]', 'EPOCH LOSS:', 0.0025267434028649737, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 192, ']')


'Epoch [193] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 425,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005080789873844837,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 193,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 193, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243376.391254)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243409.801037)
('Worker processing elapsed time: ', 33.409783124923706, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[193]', 'EPOCH LOSS:', 0.024261652046765048, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 193, ']')


'Epoch [194] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 552,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005554705963692508,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 1, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 194,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 194, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243409.805753)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243443.509375)
('Worker processing elapsed time: ', 33.70362210273743, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[194]', 'EPOCH LOSS:', 0.026473470794903628, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 194, ']')


'Epoch [195] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 238,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00023400004630041895,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 1, 8, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 195,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 195, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243443.514413)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243477.511175)
('Worker processing elapsed time: ', 33.99676179885864, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[195]', 'EPOCH LOSS:', 0.089393747136089455, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 195, ']')


'Epoch [196] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 409,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009245643659247311,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 7, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 196,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 196, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243477.516965)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243511.341857)
('Worker processing elapsed time: ', 33.82489204406738, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[196]', 'EPOCH LOSS:', 0.062851850091912992, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 196, ']')


'Epoch [197] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 253,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000309631745774897,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 8, 2, 5, 7, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 197,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 197, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243511.346858)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243548.830928)
('Worker processing elapsed time: ', 37.48407006263733, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[197]', 'EPOCH LOSS:', 0.022377276151404609, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 197, ']')


'Epoch [198] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 556,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004542731868543468,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1, 1, 3, 8, 5, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 198,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 198, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243548.836532)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243584.199597)
('Worker processing elapsed time: ', 35.363064765930176, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[198]', 'EPOCH LOSS:', 0.018772922436079999, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 198, ']')


'Epoch [199] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 344,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005935788305435191,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 1, 4, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 199,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 199, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243584.205699)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243618.001754)
('Worker processing elapsed time: ', 33.79605507850647, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[199]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 199, ']')


'Epoch [200] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 357,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008181936771485362,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 4, 4, 4, 6, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 200,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 200, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243618.007148)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243655.77689)
('Worker processing elapsed time: ', 37.769742012023926, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[200]', 'EPOCH LOSS:', 0.00024310814727833901, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 200, ']')


'Epoch [201] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 581,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000670146938127048,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 201,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 201, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243655.781943)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243689.20386)
('Worker processing elapsed time: ', 33.42191696166992, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[201]', 'EPOCH LOSS:', 1.1653578171559638, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 201, ']')


'Epoch [202] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 415,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00033962477734142014,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 202,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 202, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243689.208914)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243723.173928)
('Worker processing elapsed time: ', 33.96501398086548, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[202]', 'EPOCH LOSS:', 0.033927464832860355, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 202, ']')


'Epoch [203] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 254,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000181504046827834,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 203,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 203, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243723.179427)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243756.22543)
('Worker processing elapsed time: ', 33.046003103256226, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[203]', 'EPOCH LOSS:', 0.012468100309985708, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 203, ']')


'Epoch [204] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 264,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008410069136566059,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 204,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 204, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243756.230313)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243789.700844)
('Worker processing elapsed time: ', 33.47053098678589, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[204]', 'EPOCH LOSS:', 0.015329603482265333, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 204, ']')


'Epoch [205] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 446,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006794353693364874,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 8, 1, 2, 3, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 205,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 205, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243789.706008)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243824.597063)
('Worker processing elapsed time: ', 34.8910551071167, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[205]', 'EPOCH LOSS:', 0.020005201333989669, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 205, ']')


'Epoch [206] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 397,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001673899209605578,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 206,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 206, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243824.602291)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243857.76309)
('Worker processing elapsed time: ', 33.16079878807068, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[206]', 'EPOCH LOSS:', 0.017092912765793108, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 206, ']')


'Epoch [207] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 432,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00044708669884031534,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 6, 3, 4, 5, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 207,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 207, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243857.768955)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243895.730665)
('Worker processing elapsed time: ', 37.96170997619629, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[207]', 'EPOCH LOSS:', 0.015445926032795742, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 207, ']')


'Epoch [208] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 303,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00024957469587661373,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 8, 5, 4, 7, 6, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 208,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 208, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243895.736994)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243930.604684)
('Worker processing elapsed time: ', 34.867690086364746, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[208]', 'EPOCH LOSS:', 0.022424292804686923, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 208, ']')


'Epoch [209] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 605,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003937490255024091,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 8, 6, 2, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 209,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 209, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243930.610064)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243964.486887)
('Worker processing elapsed time: ', 33.87682294845581, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[209]', 'EPOCH LOSS:', 0.025895079134951079, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 209, ']')


'Epoch [210] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 212,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00023204435507984643,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 210,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 210, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243964.491667)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494243998.38481)
('Worker processing elapsed time: ', 33.89314293861389, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[210]', 'EPOCH LOSS:', 3.3352485047577409, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 210, ']')


'Epoch [211] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 301,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006436719282704505,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 1, 1, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 211,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 211, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494243998.390836)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244032.634331)
('Worker processing elapsed time: ', 34.24349498748779, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[211]', 'EPOCH LOSS:', 20.287926956086395, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 211, ']')


'Epoch [212] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 501,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003727624746825129,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 2, 2, 5, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 212,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 212, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244032.639169)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244066.6309)
('Worker processing elapsed time: ', 33.99173092842102, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[212]', 'EPOCH LOSS:', 0.024517252232278054, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 212, ']')


'Epoch [213] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 539,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004685102310294267,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 7, 8, 6, 6, 8, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 213,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 213, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244066.636671)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244103.085901)
('Worker processing elapsed time: ', 36.44922995567322, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[213]', 'EPOCH LOSS:', 0.025719708554121725, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 213, ']')


'Epoch [214] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 370,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00021482116942283538,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 214,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 214, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244103.091027)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244137.419201)
('Worker processing elapsed time: ', 34.328173875808716, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[214]', 'EPOCH LOSS:', 0.017478012347195874, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 214, ']')


'Epoch [215] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 544,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004286180231191602,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 4, 8, 6, 3, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 215,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 215, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244137.424135)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244172.973619)
('Worker processing elapsed time: ', 35.54948401451111, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[215]', 'EPOCH LOSS:', 5.0999152485351429, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 215, ']')


'Epoch [216] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 556,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00015785675561169068,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 3, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 216,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 216, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244172.979518)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244206.799695)
('Worker processing elapsed time: ', 33.82017707824707, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[216]', 'EPOCH LOSS:', 3.4204997354198117, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 216, ']')


'Epoch [217] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 621,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006336244430798766,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 8, 4, 3, 4, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 217,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 217, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244206.805496)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244246.18063)
('Worker processing elapsed time: ', 39.375133991241455, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[217]', 'EPOCH LOSS:', 0.0043808151697234182, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 217, ']')


'Epoch [218] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 268,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00011369233329025388,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 2, 3, 1, 3, 3, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 218,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 218, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244246.185809)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244280.896111)
('Worker processing elapsed time: ', 34.710302114486694, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[218]', 'EPOCH LOSS:', 39.377687331134403, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 218, ']')


'Epoch [219] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 447,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004492569666280556,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 219,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 219, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244280.901423)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244314.200173)
('Worker processing elapsed time: ', 33.298749923706055, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[219]', 'EPOCH LOSS:', 0.27705530695113922, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 219, ']')


'Epoch [220] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 602,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008427225315523378,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 220,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 220, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244314.205866)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244347.722221)
('Worker processing elapsed time: ', 33.51635479927063, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[220]', 'EPOCH LOSS:', 0.0014224358187433976, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 220, ']')


'Epoch [221] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 586,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009671447950532488,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 1, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 221,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 221, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244347.727863)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244381.766063)
('Worker processing elapsed time: ', 34.03819990158081, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[221]', 'EPOCH LOSS:', 0.027788604253539604, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 221, ']')


'Epoch [222] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 231,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00020027447901191625,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 8, 7, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 222,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 222, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244381.771473)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244415.969499)
('Worker processing elapsed time: ', 34.198026180267334, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[222]', 'EPOCH LOSS:', 2.316421638390846, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 222, ']')


'Epoch [223] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 526,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000273430896371773,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 223,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 223, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244415.974688)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244449.005455)
('Worker processing elapsed time: ', 33.03076696395874, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[223]', 'EPOCH LOSS:', 0.016074595034045725, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 223, ']')


'Epoch [224] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 310,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006066354229078906,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 224,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 224, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244449.011459)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244483.370787)
('Worker processing elapsed time: ', 34.35932779312134, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[224]', 'EPOCH LOSS:', 0.021376716379856709, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 224, ']')


'Epoch [225] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 238,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003566124242361375,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 225,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 225, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244483.375812)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244517.686517)
('Worker processing elapsed time: ', 34.310704946517944, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[225]', 'EPOCH LOSS:', 0.024986682487980773, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 225, ']')


'Epoch [226] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 393,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009700948913484471,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 3, 5, 1, 7, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 226,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 226, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244517.69281)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244553.026027)
('Worker processing elapsed time: ', 35.33321690559387, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[226]', 'EPOCH LOSS:', 0.074984568738203222, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 226, ']')


'Epoch [227] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 372,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009046466178863427,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 6, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 227,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 227, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244553.031881)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244587.111755)
('Worker processing elapsed time: ', 34.07987380027771, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[227]', 'EPOCH LOSS:', 0.58035614466028018, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 227, ']')


'Epoch [228] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 440,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003574072767049643,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 3, 5, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 228,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 228, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244587.117224)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244622.803813)
('Worker processing elapsed time: ', 35.68658900260925, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[228]', 'EPOCH LOSS:', 0.023447227627315218, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 228, ']')


'Epoch [229] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 416,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00040506802613980307,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 229,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 229, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244622.809546)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244656.197088)
('Worker processing elapsed time: ', 33.38754200935364, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[229]', 'EPOCH LOSS:', 0.02002080719410174, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 229, ']')


'Epoch [230] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 431,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002113021611350219,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 1, 2, 3, 8, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 230,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 230, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244656.202362)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244691.183598)
('Worker processing elapsed time: ', 34.98123598098755, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[230]', 'EPOCH LOSS:', 27.036532664523509, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 230, ']')


'Epoch [231] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 367,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002046531189968452,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 231,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 231, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244691.189584)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244724.407989)
('Worker processing elapsed time: ', 33.21840500831604, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[231]', 'EPOCH LOSS:', 0.015865899735911272, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 231, ']')


'Epoch [232] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 590,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00020293936249848573,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 1, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 232,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 232, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244724.413209)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244758.315597)
('Worker processing elapsed time: ', 33.90238809585571, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[232]', 'EPOCH LOSS:', 5.6553911194157003, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 232, ']')


'Epoch [233] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 208,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005723400927965063,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 1, 1, 2, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 233,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 233, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244758.320776)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244794.62821)
('Worker processing elapsed time: ', 36.30743408203125, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[233]', 'EPOCH LOSS:', 0.018241857272833718, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 233, ']')


'Epoch [234] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 331,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005588052776407776,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 234,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 234, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244794.633646)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244828.019873)
('Worker processing elapsed time: ', 33.38622689247131, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[234]', 'EPOCH LOSS:', 0.24607930596856373, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 234, ']')


'Epoch [235] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 499,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005766963662992123,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 235,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 235, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244828.024847)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244861.634947)
('Worker processing elapsed time: ', 33.61010003089905, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[235]', 'EPOCH LOSS:', 0.024599812668702228, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 235, ']')


'Epoch [236] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 404,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006263214236936867,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 236,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 236, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244861.639963)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244895.421911)
('Worker processing elapsed time: ', 33.78194808959961, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[236]', 'EPOCH LOSS:', 6.4712940747312242, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 236, ']')


'Epoch [237] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 433,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008862226558088745,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 237,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 237, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244895.426741)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244929.209942)
('Worker processing elapsed time: ', 33.78320121765137, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[237]', 'EPOCH LOSS:', 0.025069982444175125, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 237, ']')


'Epoch [238] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 237,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00010747560586899007,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 6, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 238,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 238, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244929.215512)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244963.208346)
('Worker processing elapsed time: ', 33.992833852767944, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[238]', 'EPOCH LOSS:', 0.94089147092983083, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 238, ']')


'Epoch [239] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 394,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000748610535975203,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 5, 8, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 239,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 239, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244963.214388)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494244998.88661)
('Worker processing elapsed time: ', 35.67222213745117, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[239]', 'EPOCH LOSS:', 0.067228737608287598, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 239, ']')


'Epoch [240] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 512,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00021293209572004548,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 6, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 240,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 240, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494244998.891771)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245032.495294)
('Worker processing elapsed time: ', 33.60352301597595, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[240]', 'EPOCH LOSS:', 0.026419933191534906, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 240, ']')


'Epoch [241] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 450,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003097084897781497,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 6, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 241,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 241, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245032.500213)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245066.453968)
('Worker processing elapsed time: ', 33.953755140304565, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[241]', 'EPOCH LOSS:', 21.008203173441323, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 241, ']')


'Epoch [242] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 436,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001396600714450375,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 242,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 242, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245066.459908)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245100.831325)
('Worker processing elapsed time: ', 34.37141704559326, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[242]', 'EPOCH LOSS:', 0.035699621847399193, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 242, ']')


'Epoch [243] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 421,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00015173125327270836,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 8, 1, 2, 8, 5, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 243,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 243, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245100.837516)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245135.845367)
('Worker processing elapsed time: ', 35.007850885391235, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[243]', 'EPOCH LOSS:', 0.024988263115513071, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 243, ']')


'Epoch [244] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 539,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006651844805546838,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 4, 2, 5, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 244,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 244, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245135.850863)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245170.235595)
('Worker processing elapsed time: ', 34.38473200798035, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[244]', 'EPOCH LOSS:', 0.019524997811637732, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 244, ']')


'Epoch [245] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 279,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009583251942784371,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 245,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 245, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245170.241558)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245203.394899)
('Worker processing elapsed time: ', 33.1533408164978, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[245]', 'EPOCH LOSS:', 0.014329029056168332, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 245, ']')


'Epoch [246] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 321,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006710908968535654,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 8, 1, 3, 8, 8, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 246,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 246, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245203.40115)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245241.394999)
('Worker processing elapsed time: ', 37.99384903907776, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[246]', 'EPOCH LOSS:', 0.017727698382670298, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 246, ']')


'Epoch [247] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 553,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00029496161405377836,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 247,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 247, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245241.400971)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245275.499774)
('Worker processing elapsed time: ', 34.09880304336548, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[247]', 'EPOCH LOSS:', 0.033839326656694722, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 247, ']')


'Epoch [248] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 428,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007159480812296277,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 248,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 248, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245275.505401)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245308.606143)
('Worker processing elapsed time: ', 33.10074210166931, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[248]', 'EPOCH LOSS:', 0.0048515833886199032, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 248, ']')


'Epoch [249] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 542,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00011077074448971776,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 6, 7, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 249,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 249, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245308.611706)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245344.659809)
('Worker processing elapsed time: ', 36.04810309410095, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[249]', 'EPOCH LOSS:', 0.023911731914837038, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 249, ']')


'Epoch [250] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 596,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007197491513175334,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1, 5, 7, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 250,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 250, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245344.664875)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245378.341872)
('Worker processing elapsed time: ', 33.67699694633484, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[250]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 250, ']')


'Epoch [251] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 568,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006150582362675239,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 251,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 251, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245378.347358)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245411.651878)
('Worker processing elapsed time: ', 33.30452013015747, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[251]', 'EPOCH LOSS:', 0.026448186665840121, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 251, ']')


'Epoch [252] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 449,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006298107168048231,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 7, 2, 6, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 252,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 252, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245411.658095)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245446.199076)
('Worker processing elapsed time: ', 34.54098105430603, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[252]', 'EPOCH LOSS:', 0.06995350983004156, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 252, ']')


'Epoch [253] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 297,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006128965315274934,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 253,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 253, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245446.203885)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245480.310757)
('Worker processing elapsed time: ', 34.10687184333801, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[253]', 'EPOCH LOSS:', 0.0235973541971921, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 253, ']')


'Epoch [254] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 334,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007465162593094658,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 2, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 254,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 254, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245480.315845)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245514.427905)
('Worker processing elapsed time: ', 34.11206007003784, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[254]', 'EPOCH LOSS:', 2.3874359758403552, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 254, ']')


'Epoch [255] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 473,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00045499021409960757,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 8, 7, 1, 8, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 255,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 255, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245514.432869)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245553.15049)
('Worker processing elapsed time: ', 38.717621088027954, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[255]', 'EPOCH LOSS:', 0.018124109792607833, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 255, ']')


'Epoch [256] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 420,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005001645454598665,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 256,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 256, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245553.155718)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245586.979439)
('Worker processing elapsed time: ', 33.823720932006836, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[256]', 'EPOCH LOSS:', 1.1404390124662978, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 256, ']')


'Epoch [257] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 601,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008535221976612483,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 7, 7, 2, 5, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 257,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 257, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245586.984791)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245621.048713)
('Worker processing elapsed time: ', 34.06392192840576, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[257]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 257, ']')


'Epoch [258] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 308,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000929156800229505,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 258,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 258, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245621.054381)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245654.425091)
('Worker processing elapsed time: ', 33.370710134506226, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[258]', 'EPOCH LOSS:', 0.023411532629079634, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 258, ']')


'Epoch [259] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 420,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008128754284494391,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 7, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 259,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 259, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245654.430675)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245688.144595)
('Worker processing elapsed time: ', 33.71391987800598, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[259]', 'EPOCH LOSS:', 0.024482505440228397, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 259, ']')


'Epoch [260] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 241,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00018202574547882574,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 260,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 260, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245688.149751)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245721.303088)
('Worker processing elapsed time: ', 33.15333700180054, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[260]', 'EPOCH LOSS:', 0.021641827785629793, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 260, ']')


'Epoch [261] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 587,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00014007442716232072,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 8, 3, 3, 6, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 261,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 261, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245721.308088)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245758.885509)
('Worker processing elapsed time: ', 37.57742094993591, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[261]', 'EPOCH LOSS:', 0.26281389649571418, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 261, ']')


'Epoch [262] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 438,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00017896329520264952,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 262,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 262, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245758.896508)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245792.371909)
('Worker processing elapsed time: ', 33.47540092468262, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[262]', 'EPOCH LOSS:', 0.032571555869254941, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 262, ']')


'Epoch [263] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 608,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007788772407567752,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 8, 3, 8, 7, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 263,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 263, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245792.376871)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245831.912311)
('Worker processing elapsed time: ', 39.53543996810913, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[263]', 'EPOCH LOSS:', 0.019996066887086703, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 263, ']')


'Epoch [264] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 321,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00016326739902037212,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 8, 7, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 264,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 264, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245831.917822)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245865.709865)
('Worker processing elapsed time: ', 33.79204320907593, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[264]', 'EPOCH LOSS:', 0.024749561607988843, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 264, ']')


'Epoch [265] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 223,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004746013508757122,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 5, 6, 2, 3, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 265,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 265, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245865.715695)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245900.454647)
('Worker processing elapsed time: ', 34.73895215988159, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[265]', 'EPOCH LOSS:', 34.096504531307353, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 265, ']')


'Epoch [266] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 422,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008938122177502457,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 5, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 266,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 266, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245900.460689)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245934.174552)
('Worker processing elapsed time: ', 33.713862895965576, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[266]', 'EPOCH LOSS:', 0.024399625638121657, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 266, ']')


'Epoch [267] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 497,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00017818178788650064,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 7, 1, 7, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 267,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 267, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245934.180137)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494245968.027492)
('Worker processing elapsed time: ', 33.847355127334595, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[267]', 'EPOCH LOSS:', 0.024731025006929652, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 267, ']')


'Epoch [268] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 210,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004068611472707078,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 268,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 268, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494245968.03305)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246001.107648)
('Worker processing elapsed time: ', 33.07459783554077, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[268]', 'EPOCH LOSS:', 0.017559883672166918, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 268, ']')


'Epoch [269] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 428,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00034979564627804244,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 7, 8, 6, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 269,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 269, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246001.1132)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246036.934173)
('Worker processing elapsed time: ', 35.82097315788269, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[269]', 'EPOCH LOSS:', 0.020692106934987992, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 269, ']')


'Epoch [270] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 397,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002316712300471026,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 270,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 270, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246036.939884)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246070.050951)
('Worker processing elapsed time: ', 33.111067056655884, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[270]', 'EPOCH LOSS:', 0.014083669602853672, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 270, ']')


'Epoch [271] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 379,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007116706952040204,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 8, 6, 8, 3, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 271,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 271, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246070.057306)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246105.40915)
('Worker processing elapsed time: ', 35.35184383392334, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[271]', 'EPOCH LOSS:', 0.65154034130991489, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 271, ']')


'Epoch [272] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 405,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00015216628864656826,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 3, 3, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 272,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 272, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246105.414192)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246139.162009)
('Worker processing elapsed time: ', 33.747817039489746, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[272]', 'EPOCH LOSS:', 0.01088578813254365, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 272, ']')


'Epoch [273] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 485,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007682470838191963,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 273,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 273, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246139.166843)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246172.456461)
('Worker processing elapsed time: ', 33.28961801528931, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[273]', 'EPOCH LOSS:', 0.024036227102678203, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 273, ']')


'Epoch [274] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 344,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002782284971137688,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 4, 3, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 274,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 274, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246172.462107)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246207.096338)
('Worker processing elapsed time: ', 34.634231090545654, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[274]', 'EPOCH LOSS:', 0.043363174783047458, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 274, ']')


'Epoch [275] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 209,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009624627004323934,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 3, 8, 2, 2, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 275,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 275, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246207.102059)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246241.853436)
('Worker processing elapsed time: ', 34.75137710571289, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[275]', 'EPOCH LOSS:', 1.4951773377082069, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 275, ']')


'Epoch [276] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 526,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00042478844291779635,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 1, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 276,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 276, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246241.858796)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246275.781896)
('Worker processing elapsed time: ', 33.923100233078, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[276]', 'EPOCH LOSS:', 2.8182150309917113, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 276, ']')


'Epoch [277] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 296,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00044725403738779296,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 3, 3, 2, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 277,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 277, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246275.786965)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246309.761099)
('Worker processing elapsed time: ', 33.97413420677185, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[277]', 'EPOCH LOSS:', 0.022749911447139795, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 277, ']')


'Epoch [278] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 471,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00016116689859389235,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 7, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 278,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 278, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246309.766377)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246343.3925)
('Worker processing elapsed time: ', 33.62612295150757, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[278]', 'EPOCH LOSS:', 0.023598111692868049, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 278, ']')


'Epoch [279] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 419,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008188011316837132,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 279,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 279, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246343.398018)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246376.757906)
('Worker processing elapsed time: ', 33.35988807678223, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[279]', 'EPOCH LOSS:', 1.1810111958583347, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 279, ']')


'Epoch [280] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 508,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00044603768678886735,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 8, 5, 2, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 280,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 280, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246376.763414)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246413.960993)
('Worker processing elapsed time: ', 37.19757914543152, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[280]', 'EPOCH LOSS:', 0.013038982428302247, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 280, ']')


'Epoch [281] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 488,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006929732973051338,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1, 4, 8, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 281,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 281, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246413.966082)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246449.900505)
('Worker processing elapsed time: ', 35.934422969818115, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[281]', 'EPOCH LOSS:', 0.022394823354694577, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 281, ']')


'Epoch [282] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 431,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007860188980001778,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 5, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 282,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 282, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246449.906684)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246483.987751)
('Worker processing elapsed time: ', 34.08106708526611, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[282]', 'EPOCH LOSS:', 0.40139686458296603, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 282, ']')


'Epoch [283] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 209,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008964094490622752,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 283,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 283, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246483.99328)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246518.118073)
('Worker processing elapsed time: ', 34.12479305267334, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[283]', 'EPOCH LOSS:', 0.0043423081966328268, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 283, ']')


'Epoch [284] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 458,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006605092621954643,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 8, 4, 2, 1, 7, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 284,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 284, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246518.123501)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246553.664948)
('Worker processing elapsed time: ', 35.541446924209595, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[284]', 'EPOCH LOSS:', 0.036632277293051171, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 284, ']')


'Epoch [285] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 350,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009768731177379796,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 4, 7, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 285,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 285, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246553.670996)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246587.914503)
('Worker processing elapsed time: ', 34.24350714683533, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[285]', 'EPOCH LOSS:', 1.7021904248536046, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 285, ']')


'Epoch [286] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 577,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00017896814689458617,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 286,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 286, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246587.919387)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246621.12037)
('Worker processing elapsed time: ', 33.20098280906677, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[286]', 'EPOCH LOSS:', 0.026329276113506453, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 286, ']')


'Epoch [287] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 560,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004561553472590442,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 287,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 287, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246621.125845)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246654.368105)
('Worker processing elapsed time: ', 33.24225997924805, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[287]', 'EPOCH LOSS:', 0.0015525391181293322, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 287, ']')


'Epoch [288] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 409,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009325141912223928,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 7, 8, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 288,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 288, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246654.37388)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246688.696603)
('Worker processing elapsed time: ', 34.322723150253296, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[288]', 'EPOCH LOSS:', 13.350825736618081, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 288, ']')


'Epoch [289] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 454,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000294634139794549,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 8, 3, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 289,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 289, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246688.702156)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246723.023229)
('Worker processing elapsed time: ', 34.321072816848755, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[289]', 'EPOCH LOSS:', 658.50469427992675, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 289, ']')


'Epoch [290] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 220,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007284797815801821,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 290,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 290, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246723.029069)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246756.408932)
('Worker processing elapsed time: ', 33.379863023757935, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[290]', 'EPOCH LOSS:', 0.93430631863618474, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 290, ']')


'Epoch [291] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 621,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002701415000341121,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 7, 5, 3, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 291,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 291, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246756.414097)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246790.66617)
('Worker processing elapsed time: ', 34.25207281112671, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[291]', 'EPOCH LOSS:', 455.59623235274029, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 291, ']')


'Epoch [292] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 371,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00018121658207429208,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 292,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 292, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246790.672074)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246824.847058)
('Worker processing elapsed time: ', 34.174983978271484, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[292]', 'EPOCH LOSS:', 0.061910089272209048, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 292, ']')


'Epoch [293] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 403,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008429964062245016,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 293,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 293, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246824.852329)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246858.306419)
('Worker processing elapsed time: ', 33.454089879989624, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[293]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 293, ']')


'Epoch [294] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 535,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00012015165341391877,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 294,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 294, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246858.312498)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246892.184076)
('Worker processing elapsed time: ', 33.871577978134155, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[294]', 'EPOCH LOSS:', 0.45063687152318527, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 294, ']')


'Epoch [295] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 398,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007583365204285802,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 295,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 295, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246892.189644)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246926.149805)
('Worker processing elapsed time: ', 33.960160970687866, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[295]', 'EPOCH LOSS:', 0.012239898814477913, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 295, ']')


'Epoch [296] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 569,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00041453580170343904,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 296,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 296, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246926.155057)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246959.71634)
('Worker processing elapsed time: ', 33.561283111572266, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[296]', 'EPOCH LOSS:', 0.026474118675000539, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 296, ']')


'Epoch [297] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 324,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006046746845194695,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 3, 4, 6, 2, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 297,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 297, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246959.721946)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494246994.256912)
('Worker processing elapsed time: ', 34.53496599197388, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[297]', 'EPOCH LOSS:', 0.10013514309222155, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 297, ']')


'Epoch [298] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 561,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009894582494510552,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 298,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 298, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494246994.262787)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247027.664647)
('Worker processing elapsed time: ', 33.401859998703, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[298]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 298, ']')


'Epoch [299] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 521,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005269371424546877,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 299,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 299, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247027.670509)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247061.263764)
('Worker processing elapsed time: ', 33.593254804611206, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[299]', 'EPOCH LOSS:', 0.024387729674250243, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 299, ']')


'Epoch [300] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 325,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008275680610990095,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 4, 6, 3, 8, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 300,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 300, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247061.269848)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247096.403219)
('Worker processing elapsed time: ', 35.133370876312256, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[300]', 'EPOCH LOSS:', 0.46446280268163875, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 300, ']')


'Epoch [301] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 406,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002725078876450168,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 5, 3, 5, 1, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 301,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 301, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247096.408105)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247131.807979)
('Worker processing elapsed time: ', 35.399874210357666, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[301]', 'EPOCH LOSS:', 16.099740002429222, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 301, ']')


'Epoch [302] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 608,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009126069387652048,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 7, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 302,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 302, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247131.813503)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247165.354053)
('Worker processing elapsed time: ', 33.540549993515015, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[302]', 'EPOCH LOSS:', 0.025802397084422314, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 302, ']')


'Epoch [303] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 219,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009263057368729657,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 6, 3, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 303,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 303, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247165.360128)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247199.170588)
('Worker processing elapsed time: ', 33.81046009063721, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[303]', 'EPOCH LOSS:', 0.024404828896290583, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 303, ']')


'Epoch [304] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 386,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003006773079296328,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 8, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 304,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 304, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247199.175807)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247232.836483)
('Worker processing elapsed time: ', 33.66067600250244, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[304]', 'EPOCH LOSS:', 0.022843371251591565, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 304, ']')


'Epoch [305] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 612,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00040010987137535387,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 6, 2, 5, 7, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 305,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 305, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247232.841986)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247272.279268)
('Worker processing elapsed time: ', 39.4372820854187, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[305]', 'EPOCH LOSS:', 0.017454242715194787, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 305, ']')


'Epoch [306] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 509,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008554531271208961,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 306,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 306, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247272.28529)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247305.907067)
('Worker processing elapsed time: ', 33.621777057647705, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[306]', 'EPOCH LOSS:', 0.26459677773339441, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 306, ']')


'Epoch [307] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 285,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006089459945262548,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 307,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 307, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247305.912528)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247339.520463)
('Worker processing elapsed time: ', 33.60793495178223, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[307]', 'EPOCH LOSS:', 1.0191859414143347, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 307, ']')


'Epoch [308] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 575,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000644677480887161,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 8, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 308,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 308, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247339.525732)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247373.25142)
('Worker processing elapsed time: ', 33.725687980651855, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[308]', 'EPOCH LOSS:', 0.026391909473066796, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 308, ']')


'Epoch [309] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 607,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009289545578221991,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 1, 6, 6, 6, 2, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 309,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 309, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247373.256602)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247407.370055)
('Worker processing elapsed time: ', 34.11345291137695, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[309]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 309, ']')


'Epoch [310] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 265,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008977112642697598,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 310,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 310, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247407.376511)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247441.26702)
('Worker processing elapsed time: ', 33.89050889015198, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[310]', 'EPOCH LOSS:', 0.36752437893353074, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 310, ']')


'Epoch [311] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 416,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002658869433639204,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 5, 1, 5, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 311,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 311, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247441.273229)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247475.306347)
('Worker processing elapsed time: ', 34.03311800956726, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[311]', 'EPOCH LOSS:', 0.02465479074423791, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 311, ']')


'Epoch [312] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 322,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009020170064386675,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 3, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 312,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 312, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247475.311244)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247509.403689)
('Worker processing elapsed time: ', 34.092444896698, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[312]', 'EPOCH LOSS:', 3.7043350415801535, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 312, ']')


'Epoch [313] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 524,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00025394890362438467,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 313,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 313, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247509.40968)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247543.213215)
('Worker processing elapsed time: ', 33.8035352230072, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[313]', 'EPOCH LOSS:', 0.98987332709424114, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 313, ']')


'Epoch [314] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 373,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00042362946588187563,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 314,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 314, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247543.218588)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247577.338127)
('Worker processing elapsed time: ', 34.1195387840271, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[314]', 'EPOCH LOSS:', 0.022993549751565624, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 314, ']')


'Epoch [315] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 373,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006191522073084983,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 5, 1, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 315,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 315, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247577.343605)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247612.757808)
('Worker processing elapsed time: ', 35.41420292854309, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[315]', 'EPOCH LOSS:', 0.040915488364837084, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 315, ']')


'Epoch [316] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 425,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00011842315137694981,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 316,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 316, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247612.762867)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247646.593008)
('Worker processing elapsed time: ', 33.83014106750488, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[316]', 'EPOCH LOSS:', 2.7317475802286877, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 316, ']')


'Epoch [317] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 355,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005717809234397271,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 317,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 317, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247646.598001)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247680.044767)
('Worker processing elapsed time: ', 33.4467658996582, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[317]', 'EPOCH LOSS:', 0.23338657027075316, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 317, ']')


'Epoch [318] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 441,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002915078448519849,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 3, 4, 8, 1, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 318,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 318, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247680.050742)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247715.757768)
('Worker processing elapsed time: ', 35.70702600479126, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[318]', 'EPOCH LOSS:', 2.3049121772889145, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 318, ']')


'Epoch [319] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 535,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008119709008557793,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 5, 2, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 319,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 319, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247715.763927)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247749.575277)
('Worker processing elapsed time: ', 33.81135010719299, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[319]', 'EPOCH LOSS:', 0.026389269584684794, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 319, ']')


'Epoch [320] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 565,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006652300884294956,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 5, 8, 1, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 320,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 320, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247749.58017)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247784.593229)
('Worker processing elapsed time: ', 35.01305913925171, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[320]', 'EPOCH LOSS:', 2.485932101758662, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 320, ']')


'Epoch [321] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 565,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009140731022671514,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 321,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 321, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247784.598313)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247817.733256)
('Worker processing elapsed time: ', 33.13494300842285, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[321]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 321, ']')


'Epoch [322] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 578,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008657543452288555,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 8, 1, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 322,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 322, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247817.738202)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247854.077067)
('Worker processing elapsed time: ', 36.33886480331421, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[322]', 'EPOCH LOSS:', 0.0016162141041513251, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 322, ']')


'Epoch [323] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 310,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000958329080033764,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 2, 3, 1, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 323,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 323, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247854.082246)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247888.473838)
('Worker processing elapsed time: ', 34.391592025756836, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[323]', 'EPOCH LOSS:', 0.043088973639257136, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 323, ']')


'Epoch [324] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 559,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000965363503951262,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 2, 3, 1, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 324,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 324, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247888.479045)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247922.472684)
('Worker processing elapsed time: ', 33.99363899230957, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[324]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 324, ']')


'Epoch [325] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 471,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005513645766432435,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 5, 8, 8, 6, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 325,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 325, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247922.47819)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247961.0361)
('Worker processing elapsed time: ', 38.55790996551514, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[325]', 'EPOCH LOSS:', 0.092264101620665173, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 325, ']')


'Epoch [326] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 594,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009140828864129647,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 326,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 326, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247961.041625)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494247994.855359)
('Worker processing elapsed time: ', 33.81373405456543, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[326]', 'EPOCH LOSS:', 0.15350013486880854, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 326, ']')


'Epoch [327] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 383,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00011672271146969553,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 327,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 327, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494247994.861228)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248028.017443)
('Worker processing elapsed time: ', 33.15621495246887, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[327]', 'EPOCH LOSS:', 0.019492525483348924, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 327, ']')


'Epoch [328] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 581,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00032717069025876746,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 6, 2, 5, 7, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 328,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 328, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248028.022843)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248063.654328)
('Worker processing elapsed time: ', 35.63148522377014, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[328]', 'EPOCH LOSS:', 0.021443981890494405, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 328, ']')


'Epoch [329] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 536,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00066810838314946,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 329,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 329, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248063.659638)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248097.064417)
('Worker processing elapsed time: ', 33.40477895736694, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[329]', 'EPOCH LOSS:', 0.026348509379901384, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 329, ']')


'Epoch [330] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 408,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006374022344589937,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 7, 3, 6, 4, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 330,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 330, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248097.069471)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248132.532006)
('Worker processing elapsed time: ', 35.46253514289856, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[330]', 'EPOCH LOSS:', 0.024706532311981909, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 330, ']')


'Epoch [331] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 317,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005423010211564772,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 331,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 331, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248132.538302)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248165.772719)
('Worker processing elapsed time: ', 33.23441696166992, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[331]', 'EPOCH LOSS:', 0.024899410864478243, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 331, ']')


'Epoch [332] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 380,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009220156054327256,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 332,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 332, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248165.778132)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248199.376001)
('Worker processing elapsed time: ', 33.59786891937256, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[332]', 'EPOCH LOSS:', 0.022993561944016774, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 332, ']')


'Epoch [333] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 277,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00034867544684222273,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 333,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 333, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248199.381512)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248233.230718)
('Worker processing elapsed time: ', 33.84920597076416, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[333]', 'EPOCH LOSS:', 0.037748546178121685, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 333, ']')


'Epoch [334] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 534,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003074541427528473,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 334,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 334, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248233.236596)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248267.017794)
('Worker processing elapsed time: ', 33.78119778633118, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[334]', 'EPOCH LOSS:', 0.024162107787369307, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 334, ']')


'Epoch [335] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 522,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005123284600313609,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 6, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 335,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 335, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248267.022888)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248301.081813)
('Worker processing elapsed time: ', 34.05892515182495, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[335]', 'EPOCH LOSS:', 0.59654823169832094, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 335, ']')


'Epoch [336] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 334,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006062915916851342,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 336,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 336, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248301.087093)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248334.535239)
('Worker processing elapsed time: ', 33.44814586639404, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[336]', 'EPOCH LOSS:', 0.0218315080414016, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 336, ']')


'Epoch [337] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 300,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005093312359260097,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 337,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 337, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248334.540028)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248368.597232)
('Worker processing elapsed time: ', 34.05720400810242, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[337]', 'EPOCH LOSS:', 0.010869528805574617, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 337, ']')


'Epoch [338] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 452,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003942044325066276,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 2, 5, 5, 8, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 338,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 338, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248368.602442)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248403.105783)
('Worker processing elapsed time: ', 34.50334095954895, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[338]', 'EPOCH LOSS:', 0.61311603399026848, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 338, ']')


'Epoch [339] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 304,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001301993983269707,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 1, 3, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 339,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 339, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248403.111073)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248437.396503)
('Worker processing elapsed time: ', 34.28542995452881, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[339]', 'EPOCH LOSS:', 106.95915426914466, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 339, ']')


'Epoch [340] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 597,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007652747996709498,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 5, 5, 1, 6, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 340,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 340, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248437.401615)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248473.029462)
('Worker processing elapsed time: ', 35.62784719467163, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[340]', 'EPOCH LOSS:', 0.019758300639052109, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 340, ']')


'Epoch [341] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 234,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007612108221397492,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 6, 1, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 341,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 341, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248473.035473)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248507.216919)
('Worker processing elapsed time: ', 34.181445837020874, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[341]', 'EPOCH LOSS:', 0.02532977649120334, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 341, ']')


'Epoch [342] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 553,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006633316313959868,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 3, 5, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 342,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 342, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248507.222304)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248543.38453)
('Worker processing elapsed time: ', 36.16222596168518, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[342]', 'EPOCH LOSS:', 0.02003480243479756, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 342, ']')


'Epoch [343] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 404,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000910569769617636,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 1, 4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 343,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 343, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248543.38981)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248577.268026)
('Worker processing elapsed time: ', 33.8782160282135, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[343]', 'EPOCH LOSS:', 0.024316512968384538, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 343, ']')


'Epoch [344] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 475,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00019641844111443357,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 4, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 344,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 344, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248577.273531)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248610.949173)
('Worker processing elapsed time: ', 33.675642013549805, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[344]', 'EPOCH LOSS:', 0.024186293327847434, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 344, ']')


'Epoch [345] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 218,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008859510529019384,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 6, 2, 4, 6, 8, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 345,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 345, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248610.954752)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248645.650398)
('Worker processing elapsed time: ', 34.69564604759216, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[345]', 'EPOCH LOSS:', 2.7541931324398443, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 345, ']')


'Epoch [346] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 319,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00032870329010313527,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1, 1, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 346,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 346, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248645.656134)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248679.256256)
('Worker processing elapsed time: ', 33.60012221336365, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[346]', 'EPOCH LOSS:', 0.02487810054858618, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 346, ']')


'Epoch [347] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 403,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003510745724972559,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 1, 5, 1, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 347,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 347, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248679.261779)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248715.949134)
('Worker processing elapsed time: ', 36.687355041503906, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[347]', 'EPOCH LOSS:', 0.021106646244278115, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 347, ']')


'Epoch [348] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 517,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005427038044411611,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 3, 8, 1, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 348,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 348, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248715.954813)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248750.815602)
('Worker processing elapsed time: ', 34.86078906059265, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[348]', 'EPOCH LOSS:', 2.4054062613877165, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 348, ']')


'Epoch [349] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 222,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006781492527165431,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 349,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 349, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248750.820453)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248784.225929)
('Worker processing elapsed time: ', 33.405476093292236, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[349]', 'EPOCH LOSS:', 0.10223650143742949, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 349, ']')


'Epoch [350] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 208,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001693793201249863,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 350,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 350, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248784.231578)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248817.357497)
('Worker processing elapsed time: ', 33.12591886520386, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[350]', 'EPOCH LOSS:', 0.020315527975473431, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 350, ']')


'Epoch [351] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 595,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006708989361400954,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 4, 8, 7, 7, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 351,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 351, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248817.36278)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248853.363395)
('Worker processing elapsed time: ', 36.0006148815155, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[351]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 351, ']')


'Epoch [352] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 505,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000651820987859436,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 352,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 352, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248853.369423)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248886.512531)
('Worker processing elapsed time: ', 33.14310812950134, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[352]', 'EPOCH LOSS:', 0.005065592702901963, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 352, ']')


'Epoch [353] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 340,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003230407712654068,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 4, 8, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 353,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 353, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248886.518084)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248921.867796)
('Worker processing elapsed time: ', 35.349711894989014, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[353]', 'EPOCH LOSS:', 0.022761695278181455, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 353, ']')


'Epoch [354] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 436,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006977050982032262,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 354,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 354, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248921.873544)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248955.533395)
('Worker processing elapsed time: ', 33.65985107421875, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[354]', 'EPOCH LOSS:', 0.44763064614803849, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 354, ']')


'Epoch [355] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 355,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006915386248112003,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 355,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 355, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248955.538416)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494248988.771751)
('Worker processing elapsed time: ', 33.23333501815796, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[355]', 'EPOCH LOSS:', 0.27814265203995642, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 355, ']')


'Epoch [356] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 553,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009734419629552389,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 356,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 356, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494248988.777342)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249021.976752)
('Worker processing elapsed time: ', 33.19940996170044, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[356]', 'EPOCH LOSS:', 0.027052295724850316, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 356, ']')


'Epoch [357] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 373,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005256830513625588,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 7, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 357,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 357, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249021.982136)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249055.616099)
('Worker processing elapsed time: ', 33.633963108062744, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[357]', 'EPOCH LOSS:', 0.022899920051340669, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 357, ']')


'Epoch [358] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 278,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007163845621919887,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 358,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 358, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249055.621025)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249089.055898)
('Worker processing elapsed time: ', 33.43487286567688, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[358]', 'EPOCH LOSS:', 0.023749395001907394, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 358, ']')


'Epoch [359] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 368,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009356467416906969,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 3, 1, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 359,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 359, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249089.061953)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249122.899343)
('Worker processing elapsed time: ', 33.83738994598389, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[359]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 359, ']')


'Epoch [360] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 525,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006167378326206594,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 360,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 360, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249122.904387)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249156.553094)
('Worker processing elapsed time: ', 33.648706912994385, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[360]', 'EPOCH LOSS:', 0.18398312808543107, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 360, ']')


'Epoch [361] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 582,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00083742023472772,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 6, 7, 2, 4, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 361,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 361, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249156.558249)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249195.609724)
('Worker processing elapsed time: ', 39.051475048065186, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[361]', 'EPOCH LOSS:', 0.012858728805278529, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 361, ']')


'Epoch [362] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 343,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00018328731491166204,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 3, 2, 2, 8, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 362,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 362, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249195.615215)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249232.311172)
('Worker processing elapsed time: ', 36.69595694541931, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[362]', 'EPOCH LOSS:', 0.019636987627840819, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 362, ']')


'Epoch [363] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 532,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003498518047049,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 1, 4, 4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 363,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 363, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249232.317439)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249268.20721)
('Worker processing elapsed time: ', 35.88977098464966, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[363]', 'EPOCH LOSS:', 0.020444281827655834, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 363, ']')


'Epoch [364] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 521,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003329448212594844,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 364,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 364, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249268.212724)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249301.31421)
('Worker processing elapsed time: ', 33.10148596763611, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[364]', 'EPOCH LOSS:', 0.012006314051169654, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 364, ']')


'Epoch [365] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 395,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00019631146690943636,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1, 2, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 365,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 365, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249301.31908)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249335.424608)
('Worker processing elapsed time: ', 34.10552787780762, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[365]', 'EPOCH LOSS:', 1.3874187166057217, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 365, ']')


'Epoch [366] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 363,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003081553604667895,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 366,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 366, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249335.43016)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249368.518668)
('Worker processing elapsed time: ', 33.088507890701294, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[366]', 'EPOCH LOSS:', 0.021251491962435781, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 366, ']')


'Epoch [367] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 389,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007645334507010143,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 3, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 367,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 367, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249368.524781)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249402.580753)
('Worker processing elapsed time: ', 34.0559720993042, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[367]', 'EPOCH LOSS:', 0.14353568332118782, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 367, ']')


'Epoch [368] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 544,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007407181222873996,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 368,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 368, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249402.586533)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249436.039984)
('Worker processing elapsed time: ', 33.45345091819763, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[368]', 'EPOCH LOSS:', 2.4623987360727431, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 368, ']')


'Epoch [369] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 355,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003404942263807503,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 369,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 369, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249436.046064)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249469.541785)
('Worker processing elapsed time: ', 33.495721101760864, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[369]', 'EPOCH LOSS:', 0.023577914866146979, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 369, ']')


'Epoch [370] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 213,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005657922096236804,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 3, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 370,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 370, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249469.546942)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249503.636025)
('Worker processing elapsed time: ', 34.08908295631409, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[370]', 'EPOCH LOSS:', 0.28567763363745818, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 370, ']')


'Epoch [371] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 416,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002563697141717208,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 1, 2, 2, 8, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 371,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 371, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249503.64171)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249541.821868)
('Worker processing elapsed time: ', 38.18015789985657, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[371]', 'EPOCH LOSS:', 0.016614409877294463, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 371, ']')


'Epoch [372] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 313,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006282143699428371,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 372,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 372, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249541.82703)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249575.252541)
('Worker processing elapsed time: ', 33.42551112174988, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[372]', 'EPOCH LOSS:', 0.024008163963075847, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 372, ']')


'Epoch [373] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 469,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008689421079288918,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 373,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 373, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249575.257678)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249608.922797)
('Worker processing elapsed time: ', 33.665118932724, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[373]', 'EPOCH LOSS:', 1.273311742923688, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 373, ']')


'Epoch [374] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 450,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007504289405304524,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 374,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 374, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249608.928248)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249642.568651)
('Worker processing elapsed time: ', 33.640403032302856, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[374]', 'EPOCH LOSS:', 1.1537445267269435, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 374, ']')


'Epoch [375] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 513,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00021393281701252264,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 5, 3, 7, 3, 7, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 375,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 375, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249642.573633)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249681.363052)
('Worker processing elapsed time: ', 38.78941893577576, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[375]', 'EPOCH LOSS:', 0.021611689903288194, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 375, ']')


'Epoch [376] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 612,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006353673569804708,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 2, 6, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 376,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 376, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249681.368698)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249717.751543)
('Worker processing elapsed time: ', 36.38284516334534, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[376]', 'EPOCH LOSS:', 0.019673699684956886, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 376, ']')


'Epoch [377] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 257,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00018496116889996311,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 377,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 377, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249717.756448)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249751.387146)
('Worker processing elapsed time: ', 33.63069796562195, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[377]', 'EPOCH LOSS:', 0.19681682231342199, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 377, ']')


'Epoch [378] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 564,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00016580730775273326,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 378,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 378, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249751.392162)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249784.842529)
('Worker processing elapsed time: ', 33.45036697387695, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[378]', 'EPOCH LOSS:', 0.026732744993094634, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 378, ']')


'Epoch [379] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 354,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008508911393421345,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 7, 3, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 379,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 379, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249784.847808)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249818.683461)
('Worker processing elapsed time: ', 33.83565306663513, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[379]', 'EPOCH LOSS:', 0.023574039819751563, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 379, ']')


'Epoch [380] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 280,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008921563046025108,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 4, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 380,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 380, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249818.688671)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249853.263266)
('Worker processing elapsed time: ', 34.57459497451782, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[380]', 'EPOCH LOSS:', 0.023720825661937811, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 380, ']')


'Epoch [381] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 377,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003862734978868189,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 4, 4, 1, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 381,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 381, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249853.269202)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249887.228967)
('Worker processing elapsed time: ', 33.95976495742798, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[381]', 'EPOCH LOSS:', 0.023033047135978348, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 381, ']')


'Epoch [382] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 487,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008766012688086281,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 382,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 382, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249887.234081)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249920.678526)
('Worker processing elapsed time: ', 33.44444489479065, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[382]', 'EPOCH LOSS:', 0.9995387945253027, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 382, ']')


'Epoch [383] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 490,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007642167317859481,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 6, 5, 4, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 383,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 383, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249920.684788)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249958.188234)
('Worker processing elapsed time: ', 37.503446102142334, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[383]', 'EPOCH LOSS:', 0.0063844819343177039, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 383, ']')


'Epoch [384] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 287,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006438453581408925,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 8, 5, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 384,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 384, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249958.193997)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494249992.019382)
('Worker processing elapsed time: ', 33.825385093688965, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[384]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 384, ']')


'Epoch [385] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 564,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006967480053931114,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 385,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 385, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494249992.0244)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250025.818533)
('Worker processing elapsed time: ', 33.79413294792175, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[385]', 'EPOCH LOSS:', 0.018658319470138466, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 385, ']')


'Epoch [386] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 229,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005426735745672852,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 386,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 386, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250025.823386)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250059.509997)
('Worker processing elapsed time: ', 33.68661093711853, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[386]', 'EPOCH LOSS:', 23.653239040539898, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 386, ']')


'Epoch [387] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 555,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003753158407205175,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 387,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 387, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250059.515038)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250092.999235)
('Worker processing elapsed time: ', 33.48419690132141, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[387]', 'EPOCH LOSS:', 0.025188899512771697, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 387, ']')


'Epoch [388] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 335,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00019278085014591984,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 388,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 388, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250093.00425)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250126.315894)
('Worker processing elapsed time: ', 33.311643838882446, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[388]', 'EPOCH LOSS:', 0.024329424098216368, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 388, ']')


'Epoch [389] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 305,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00044851342538027614,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 389,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 389, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250126.321334)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250160.089603)
('Worker processing elapsed time: ', 33.768269062042236, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[389]', 'EPOCH LOSS:', 0.20595528597617377, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 389, ']')


'Epoch [390] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 216,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000624610740326473,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 390,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 390, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250160.095144)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250193.376425)
('Worker processing elapsed time: ', 33.28128099441528, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[390]', 'EPOCH LOSS:', 0.024211199764102218, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 390, ']')


'Epoch [391] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 547,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00012635922085853528,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1, 8, 1, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 391,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 391, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250193.382527)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250228.30132)
('Worker processing elapsed time: ', 34.918792963027954, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[391]', 'EPOCH LOSS:', 0.03543861353580216, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 391, ']')


'Epoch [392] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 341,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003468033920154544,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 392,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 392, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250228.321513)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250261.829889)
('Worker processing elapsed time: ', 33.508376121520996, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[392]', 'EPOCH LOSS:', 0.024253813463939219, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 392, ']')


'Epoch [393] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 431,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004935468921601394,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 6, 3, 8, 6, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 393,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 393, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250261.835354)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250300.549969)
('Worker processing elapsed time: ', 38.71461486816406, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[393]', 'EPOCH LOSS:', 0.012960297654468045, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 393, ']')


'Epoch [394] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 459,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006266942278907819,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 394,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 394, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250300.555157)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250334.02774)
('Worker processing elapsed time: ', 33.472583055496216, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[394]', 'EPOCH LOSS:', 0.017091291484230931, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 394, ']')


'Epoch [395] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 560,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003373132000825896,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 395,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 395, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250334.032757)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250367.472164)
('Worker processing elapsed time: ', 33.439406871795654, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[395]', 'EPOCH LOSS:', 0.521964868771428, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 395, ']')


'Epoch [396] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 459,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00015116860671205888,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 7, 7, 4, 7, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 396,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 396, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250367.478354)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250406.371111)
('Worker processing elapsed time: ', 38.892756938934326, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[396]', 'EPOCH LOSS:', 0.015485337747602565, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 396, ']')


'Epoch [397] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 388,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007995452958362648,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 397,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 397, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250406.376529)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250439.481744)
('Worker processing elapsed time: ', 33.105215072631836, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[397]', 'EPOCH LOSS:', 0.004633638628143145, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 397, ']')


'Epoch [398] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 230,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006873233785361044,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 5, 6, 8, 3, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 398,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 398, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250439.487352)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250477.264165)
('Worker processing elapsed time: ', 37.77681303024292, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[398]', 'EPOCH LOSS:', 0.024635362230247839, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 398, ']')


'Epoch [399] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 349,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003088177709757381,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 3, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 399,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 399, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250477.269598)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250511.143012)
('Worker processing elapsed time: ', 33.873414039611816, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[399]', 'EPOCH LOSS:', 0.059304719930006244, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 399, ']')


'Epoch [400] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 593,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00038296349580411407,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 5, 5, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 400,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 400, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250511.148055)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250547.458889)
('Worker processing elapsed time: ', 36.31083393096924, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[400]', 'EPOCH LOSS:', 0.0063678643458841815, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 400, ']')


'Epoch [401] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 533,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008710107825070129,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 8, 3, 1, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 401,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 401, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250547.465224)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250581.396484)
('Worker processing elapsed time: ', 33.931259870529175, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[401]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 401, ']')


'Epoch [402] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 300,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007115354758201265,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 402,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 402, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250581.402385)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250614.697681)
('Worker processing elapsed time: ', 33.29529595375061, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[402]', 'EPOCH LOSS:', 0.22360123579661953, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 402, ']')


'Epoch [403] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 322,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004152279867865956,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 8, 7, 4, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 403,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 403, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250614.702876)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250648.522648)
('Worker processing elapsed time: ', 33.81977200508118, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[403]', 'EPOCH LOSS:', 0.02463530955402609, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 403, ']')


'Epoch [404] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 424,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007811616458046895,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 6, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 404,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 404, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250648.527551)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250682.548269)
('Worker processing elapsed time: ', 34.02071809768677, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[404]', 'EPOCH LOSS:', 0.90734430379922215, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 404, ']')


'Epoch [405] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 618,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006230667965371333,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 2, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 405,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 405, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250682.554071)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250716.563287)
('Worker processing elapsed time: ', 34.00921607017517, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[405]', 'EPOCH LOSS:', 0.18289024033361789, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 405, ']')


'Epoch [406] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 581,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00011066116095263606,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 406,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 406, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250716.569195)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250750.983247)
('Worker processing elapsed time: ', 34.41405200958252, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[406]', 'EPOCH LOSS:', 0.53937164669740301, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 406, ']')


'Epoch [407] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 340,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000894123638917616,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 407,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 407, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250750.98913)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250784.302712)
('Worker processing elapsed time: ', 33.31358194351196, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[407]', 'EPOCH LOSS:', 0.91900584510698613, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 407, ']')


'Epoch [408] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 612,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00013892330235156945,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 5, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 408,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 408, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250784.30801)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250818.353186)
('Worker processing elapsed time: ', 34.04517579078674, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[408]', 'EPOCH LOSS:', 802.53027548218347, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 408, ']')


'Epoch [409] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 329,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007937042299850847,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 6, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 409,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 409, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250818.358375)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250853.061844)
('Worker processing elapsed time: ', 34.703469038009644, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[409]', 'EPOCH LOSS:', 0.024173687114300031, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 409, ']')


'Epoch [410] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 563,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008234899829450556,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 410,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 410, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250853.067899)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250886.619333)
('Worker processing elapsed time: ', 33.55143404006958, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[410]', 'EPOCH LOSS:', 0.026659634618533715, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 410, ']')


'Epoch [411] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 283,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00082089957209854,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 6, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 411,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 411, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250886.624753)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250920.32445)
('Worker processing elapsed time: ', 33.69969701766968, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[411]', 'EPOCH LOSS:', 0.023412163194676282, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 411, ']')


'Epoch [412] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 371,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007217502262444743,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 1, 3, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 412,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 412, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250920.330374)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250954.183639)
('Worker processing elapsed time: ', 33.853265047073364, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[412]', 'EPOCH LOSS:', 0.022732030633685833, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 412, ']')


'Epoch [413] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 444,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005408964076722195,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 8, 3, 8, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 413,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 413, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250954.188607)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494250988.720281)
('Worker processing elapsed time: ', 34.53167390823364, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[413]', 'EPOCH LOSS:', 665.28572763690909, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 413, ']')


'Epoch [414] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 259,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006138795547848159,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 414,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 414, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494250988.725934)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251022.127903)
('Worker processing elapsed time: ', 33.40196895599365, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[414]', 'EPOCH LOSS:', 0.0019271923081139435, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 414, ']')


'Epoch [415] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 565,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009357374615800995,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 415,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 415, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251022.133853)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251055.269708)
('Worker processing elapsed time: ', 33.135854959487915, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[415]', 'EPOCH LOSS:', 0.02323875358059501, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 415, ']')


'Epoch [416] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 248,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003220517684590884,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 1, 2, 1, 7, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 416,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 416, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251055.275053)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251089.425585)
('Worker processing elapsed time: ', 34.15053200721741, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[416]', 'EPOCH LOSS:', 0.025231576768869914, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 416, ']')


'Epoch [417] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 425,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008485050552458698,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 417,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 417, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251089.43147)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251123.211889)
('Worker processing elapsed time: ', 33.78041911125183, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[417]', 'EPOCH LOSS:', 0.01175911325247846, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 417, ']')


'Epoch [418] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 547,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00017227847950829732,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 1, 2, 6, 2, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 418,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 418, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251123.217154)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251159.505551)
('Worker processing elapsed time: ', 36.28839707374573, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[418]', 'EPOCH LOSS:', 0.018424727250104685, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 418, ']')


'Epoch [419] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 214,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003747173609303131,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 8, 1, 7, 7, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 419,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 419, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251159.510911)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251195.969054)
('Worker processing elapsed time: ', 36.45814299583435, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[419]', 'EPOCH LOSS:', 0.01961445588452675, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 419, ']')


'Epoch [420] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 221,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005914173447391685,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 420,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 420, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251195.974287)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251229.536223)
('Worker processing elapsed time: ', 33.561935901641846, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[420]', 'EPOCH LOSS:', 0.025909045031764125, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 420, ']')


'Epoch [421] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 506,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007458195804778882,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 421,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 421, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251229.542012)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251262.709707)
('Worker processing elapsed time: ', 33.16769504547119, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[421]', 'EPOCH LOSS:', 0.013210812995509063, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 421, ']')


'Epoch [422] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 282,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009921525427203806,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 422,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 422, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251262.714838)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251296.343612)
('Worker processing elapsed time: ', 33.6287739276886, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[422]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 422, ']')


'Epoch [423] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 321,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00071579987274465,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 8, 4, 8, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 423,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 423, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251296.349597)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251330.860989)
('Worker processing elapsed time: ', 34.51139211654663, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[423]', 'EPOCH LOSS:', 0.053652874266623775, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 423, ']')


'Epoch [424] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 354,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00025682877384611345,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 6, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 424,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 424, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251330.874179)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251365.617452)
('Worker processing elapsed time: ', 34.74327301979065, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[424]', 'EPOCH LOSS:', 0.020423394850371132, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 424, ']')


'Epoch [425] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 509,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001229763840733247,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 5, 3, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 425,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 425, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251365.622402)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251399.848688)
('Worker processing elapsed time: ', 34.22628593444824, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[425]', 'EPOCH LOSS:', 4.9669440250387016, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 425, ']')


'Epoch [426] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 548,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003663369969721682,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 426,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 426, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251399.854227)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251433.429507)
('Worker processing elapsed time: ', 33.57527995109558, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[426]', 'EPOCH LOSS:', 0.026011969618717077, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 426, ']')


'Epoch [427] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 491,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00013762069452307303,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 427,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 427, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251433.434562)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251467.057922)
('Worker processing elapsed time: ', 33.62335991859436, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[427]', 'EPOCH LOSS:', 4.4887509398256418, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 427, ']')


'Epoch [428] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 596,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002576942023722282,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 428,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 428, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251467.06301)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251501.472493)
('Worker processing elapsed time: ', 34.40948295593262, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[428]', 'EPOCH LOSS:', 0.023957251812158156, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 428, ']')


'Epoch [429] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 406,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008744334317502967,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 429,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 429, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251501.478382)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251534.755615)
('Worker processing elapsed time: ', 33.27723288536072, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[429]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 429, ']')


'Epoch [430] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 331,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007959435962446734,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 7, 8, 7, 1, 8, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 430,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 430, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251534.761404)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251569.375556)
('Worker processing elapsed time: ', 34.61415195465088, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[430]', 'EPOCH LOSS:', 1.5157753571571535, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 430, ']')


'Epoch [431] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 347,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006313306879334531,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 431,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 431, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251569.382203)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251603.169791)
('Worker processing elapsed time: ', 33.78758788108826, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[431]', 'EPOCH LOSS:', 0.014964793136510337, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 431, ']')


'Epoch [432] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 621,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007498039301136131,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 432,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 432, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251603.174758)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251636.586448)
('Worker processing elapsed time: ', 33.41168999671936, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[432]', 'EPOCH LOSS:', 0.025381131829398312, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 432, ']')


'Epoch [433] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 348,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003993415421910197,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 433,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 433, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251636.591886)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251670.433173)
('Worker processing elapsed time: ', 33.8412868976593, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[433]', 'EPOCH LOSS:', 0.021001372786478446, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 433, ']')


'Epoch [434] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 358,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007035513348405138,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 6, 2, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 434,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 434, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251670.438373)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251705.841005)
('Worker processing elapsed time: ', 35.402631998062134, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[434]', 'EPOCH LOSS:', 0.016186731494997133, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 434, ']')


'Epoch [435] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 512,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007329506729646903,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 6, 2, 7, 7, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 435,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 435, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251705.846126)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251741.754384)
('Worker processing elapsed time: ', 35.90825796127319, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[435]', 'EPOCH LOSS:', 0.85074963693630556, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 435, ']')


'Epoch [436] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 410,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006406253614707313,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 8, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 436,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 436, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251741.759785)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251776.469285)
('Worker processing elapsed time: ', 34.7095000743866, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[436]', 'EPOCH LOSS:', 0.022020703437025137, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 436, ']')


'Epoch [437] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 382,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009000458424743485,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 1, 6, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 437,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 437, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251776.474545)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251811.904506)
('Worker processing elapsed time: ', 35.42996096611023, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[437]', 'EPOCH LOSS:', 0.022941955305739045, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 437, ']')


'Epoch [438] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 254,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008274370484143892,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 438,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 438, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251811.909953)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251845.820363)
('Worker processing elapsed time: ', 33.910409927368164, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[438]', 'EPOCH LOSS:', 0.10648702644376215, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 438, ']')


'Epoch [439] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 475,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007774466936620212,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 3, 8, 1, 7, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 439,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 439, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251845.826146)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251880.866545)
('Worker processing elapsed time: ', 35.04039907455444, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[439]', 'EPOCH LOSS:', 0.019672457621924922, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 439, ']')


'Epoch [440] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 503,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007465209269883603,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 7, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 440,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 440, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251880.871636)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251915.575401)
('Worker processing elapsed time: ', 34.70376515388489, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[440]', 'EPOCH LOSS:', 0.0012320274261124764, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 440, ']')


'Epoch [441] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 401,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006972896709524681,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 441,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 441, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251915.581302)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251949.343118)
('Worker processing elapsed time: ', 33.76181602478027, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[441]', 'EPOCH LOSS:', 0.016387841928912048, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 441, ']')


'Epoch [442] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 579,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006689538474687323,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 3, 7, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 442,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 442, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251949.349503)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494251983.591331)
('Worker processing elapsed time: ', 34.241827964782715, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[442]', 'EPOCH LOSS:', 0.41851859365441763, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 442, ']')


'Epoch [443] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 364,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005352761121457949,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 443,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 443, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494251983.597248)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252016.793362)
('Worker processing elapsed time: ', 33.19611382484436, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[443]', 'EPOCH LOSS:', 1.6841060822359837, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 443, ']')


'Epoch [444] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 533,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001799881228756432,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 3, 2, 4, 1, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 444,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 444, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252016.798939)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252055.384239)
('Worker processing elapsed time: ', 38.58529996871948, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[444]', 'EPOCH LOSS:', 0.017650156043120105, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 444, ']')


'Epoch [445] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 578,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007122926467230284,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 445,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 445, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252055.390025)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252088.508182)
('Worker processing elapsed time: ', 33.118157148361206, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[445]', 'EPOCH LOSS:', 0.023026563051000891, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 445, ']')


'Epoch [446] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 348,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000685042459790744,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 446,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 446, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252088.513908)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252122.642031)
('Worker processing elapsed time: ', 34.12812304496765, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[446]', 'EPOCH LOSS:', 0.017075062029006507, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 446, ']')


'Epoch [447] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 407,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000693727340173496,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 447,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 447, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252122.647217)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252156.283086)
('Worker processing elapsed time: ', 33.63586902618408, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[447]', 'EPOCH LOSS:', 0.024220371836392338, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 447, ']')


'Epoch [448] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 541,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006618299102908089,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 3, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 448,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 448, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252156.28897)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252189.987417)
('Worker processing elapsed time: ', 33.69844698905945, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[448]', 'EPOCH LOSS:', 0.026210035317586082, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 448, ']')


'Epoch [449] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 486,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007712961335910386,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 449,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 449, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252189.992979)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252223.508713)
('Worker processing elapsed time: ', 33.51573395729065, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[449]', 'EPOCH LOSS:', 0.0083406119605071976, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 449, ']')


'Epoch [450] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 266,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00017964674527001193,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 450,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 450, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252223.51399)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252256.569594)
('Worker processing elapsed time: ', 33.055603981018066, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[450]', 'EPOCH LOSS:', 0.019522456356478788, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 450, ']')


'Epoch [451] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 274,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007672494491930801,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 1, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 451,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 451, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252256.574687)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252290.333134)
('Worker processing elapsed time: ', 33.75844693183899, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[451]', 'EPOCH LOSS:', 0.024070059504429246, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 451, ']')


'Epoch [452] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 437,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005400698120097461,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 452,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 452, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252290.338181)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252323.634602)
('Worker processing elapsed time: ', 33.29642105102539, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[452]', 'EPOCH LOSS:', 0.016864909429956044, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 452, ']')


'Epoch [453] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 269,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00016957084661484417,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 4, 4, 6, 1, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 453,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 453, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252323.639979)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252358.475719)
('Worker processing elapsed time: ', 34.835740089416504, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[453]', 'EPOCH LOSS:', 3.3444375200844081, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 453, ']')


'Epoch [454] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 339,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00020941445456005789,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 454,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 454, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252358.481115)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252392.855387)
('Worker processing elapsed time: ', 34.374271869659424, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[454]', 'EPOCH LOSS:', 0.024962336258307675, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 454, ']')


'Epoch [455] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 259,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00038083307790672905,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 455,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 455, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252392.86073)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252426.409683)
('Worker processing elapsed time: ', 33.54895305633545, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[455]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 455, ']')


'Epoch [456] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 518,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008489625889209256,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 456,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 456, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252426.415216)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252459.81432)
('Worker processing elapsed time: ', 33.39910411834717, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[456]', 'EPOCH LOSS:', 0.026268624481531392, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 456, ']')


'Epoch [457] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 534,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000558257447974604,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 7, 8, 1, 8, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 457,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 457, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252459.819415)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252498.547917)
('Worker processing elapsed time: ', 38.72850179672241, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[457]', 'EPOCH LOSS:', 0.022619880963334157, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 457, ']')


'Epoch [458] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 307,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001072257154246636,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 3, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 458,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 458, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252498.553136)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252532.199141)
('Worker processing elapsed time: ', 33.64600491523743, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[458]', 'EPOCH LOSS:', 0.021714120854136008, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 458, ']')


'Epoch [459] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 402,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003604553650535837,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 5, 5, 2, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 459,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 459, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252532.20433)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252567.632711)
('Worker processing elapsed time: ', 35.42838096618652, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[459]', 'EPOCH LOSS:', 0.024752350257225185, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 459, ']')


'Epoch [460] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 525,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008980823396778341,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 460,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 460, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252567.638528)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252601.429413)
('Worker processing elapsed time: ', 33.79088497161865, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[460]', 'EPOCH LOSS:', 0.011047595972559202, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 460, ']')


'Epoch [461] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 342,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009930102305020629,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 3, 3, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 461,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 461, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252601.434649)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252636.706601)
('Worker processing elapsed time: ', 35.27195191383362, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[461]', 'EPOCH LOSS:', 0.022950512451213177, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 461, ']')


'Epoch [462] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 360,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007309978623283027,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 3, 5, 1, 5, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 462,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 462, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252636.711928)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252674.671598)
('Worker processing elapsed time: ', 37.959670066833496, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[462]', 'EPOCH LOSS:', 0.0019222557262289755, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 462, ']')


'Epoch [463] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 375,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008561798291808954,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 463,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 463, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252674.677161)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252708.107336)
('Worker processing elapsed time: ', 33.43017506599426, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[463]', 'EPOCH LOSS:', 0.017517444326276631, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 463, ']')


'Epoch [464] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 221,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006003286051231999,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1, 6, 3, 6, 2, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 464,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 464, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252708.112711)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252742.244763)
('Worker processing elapsed time: ', 34.132051944732666, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[464]', 'EPOCH LOSS:', 0.025822510559020951, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 464, ']')


'Epoch [465] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 386,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005100176974436026,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 465,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 465, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252742.249581)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252775.474196)
('Worker processing elapsed time: ', 33.22461485862732, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[465]', 'EPOCH LOSS:', 1.152719680174344, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 465, ']')


'Epoch [466] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 328,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00038577305468902735,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 8, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 466,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 466, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252775.479524)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252810.170703)
('Worker processing elapsed time: ', 34.691179037094116, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[466]', 'EPOCH LOSS:', 0.015353285679854134, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 466, ']')


'Epoch [467] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 492,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009258259468503006,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 467,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 467, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252810.175842)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252843.436319)
('Worker processing elapsed time: ', 33.26047706604004, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[467]', 'EPOCH LOSS:', 0.023925201631770782, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 467, ']')


'Epoch [468] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 568,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00025004159853067305,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 8, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 468,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 468, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252843.442473)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252876.970891)
('Worker processing elapsed time: ', 33.52841806411743, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[468]', 'EPOCH LOSS:', 0.0030985973780280995, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 468, ']')


'Epoch [469] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 430,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002971345631026831,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 7, 8, 3, 1, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 469,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 469, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252876.976306)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252915.338696)
('Worker processing elapsed time: ', 38.36239004135132, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[469]', 'EPOCH LOSS:', 0.010424141062321857, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 469, ']')


'Epoch [470] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 601,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001459982726409086,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 1, 5, 1, 7, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 470,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 470, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252915.34386)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252951.141074)
('Worker processing elapsed time: ', 35.79721403121948, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[470]', 'EPOCH LOSS:', 0.022805005798897122, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 470, ']')


'Epoch [471] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 437,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004884631116959926,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 471,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 471, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252951.146911)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494252984.660532)
('Worker processing elapsed time: ', 33.51362109184265, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[471]', 'EPOCH LOSS:', 0.025006024881979203, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 471, ']')


'Epoch [472] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 511,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00045042376584432983,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 3, 6, 3, 2, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 472,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 472, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494252984.666035)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253018.833725)
('Worker processing elapsed time: ', 34.16769003868103, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[472]', 'EPOCH LOSS:', 0.025972712177197873, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 472, ']')


'Epoch [473] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 613,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00024362828145404645,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 473,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 473, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253018.839579)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253052.698104)
('Worker processing elapsed time: ', 33.858524799346924, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[473]', 'EPOCH LOSS:', 0.02535082464363261, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 473, ']')


'Epoch [474] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 246,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007500392022308667,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 474,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 474, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253052.70355)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253085.999669)
('Worker processing elapsed time: ', 33.29611897468567, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[474]', 'EPOCH LOSS:', 0.0253331496796846, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 474, ']')


'Epoch [475] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 246,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006742564835523474,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 2, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 475,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 475, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253086.005556)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253119.645144)
('Worker processing elapsed time: ', 33.63958787918091, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[475]', 'EPOCH LOSS:', 0.026659913702991159, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 475, ']')


'Epoch [476] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 494,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00044502377873631524,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 8, 4, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 476,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 476, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253119.650692)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253153.525026)
('Worker processing elapsed time: ', 33.87433409690857, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[476]', 'EPOCH LOSS:', 0.023970285494284559, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 476, ']')


'Epoch [477] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 578,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004458686310956974,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 477,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 477, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253153.530634)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253186.642098)
('Worker processing elapsed time: ', 33.11146402359009, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[477]', 'EPOCH LOSS:', 0.02623947321720204, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 477, ']')


'Epoch [478] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 267,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002392518705532906,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 4, 5, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 478,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 478, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253186.647306)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253220.858168)
('Worker processing elapsed time: ', 34.210861921310425, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[478]', 'EPOCH LOSS:', 1390.8275817610565, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 478, ']')


'Epoch [479] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 213,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009067435917229414,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 4, 4, 6, 5, 1, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 479,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 479, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253220.864181)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253255.024034)
('Worker processing elapsed time: ', 34.15985298156738, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[479]', 'EPOCH LOSS:', 0.024566490462724726, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 479, ']')


'Epoch [480] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 517,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000668914308417264,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 480,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 480, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253255.02892)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253289.177714)
('Worker processing elapsed time: ', 34.148794174194336, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[480]', 'EPOCH LOSS:', 0.036565717362602841, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 480, ']')


'Epoch [481] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 249,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007009792980654677,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 481,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 481, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253289.182812)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253323.590566)
('Worker processing elapsed time: ', 34.40775394439697, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[481]', 'EPOCH LOSS:', 0.024287588943702105, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 481, ']')


'Epoch [482] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 604,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005273217118121952,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 482,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 482, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253323.595541)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253357.235057)
('Worker processing elapsed time: ', 33.6395161151886, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[482]', 'EPOCH LOSS:', 0.061924178125948803, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 482, ']')


'Epoch [483] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 238,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00015710037226876765,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 7, 7, 3, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 483,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 483, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253357.240784)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253391.672525)
('Worker processing elapsed time: ', 34.4317409992218, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[483]', 'EPOCH LOSS:', 0.1425040352233386, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 483, ']')


'Epoch [484] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 560,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00034484706426660507,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 484,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 484, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253391.678498)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253425.544363)
('Worker processing elapsed time: ', 33.865864992141724, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[484]', 'EPOCH LOSS:', 0.25478344279643711, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 484, ']')


'Epoch [485] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 403,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005274729769330113,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1, 1, 4, 3, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 485,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 485, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253425.550348)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253459.487724)
('Worker processing elapsed time: ', 33.93737602233887, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[485]', 'EPOCH LOSS:', 0.023054947341252584, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 485, ']')


'Epoch [486] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 388,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007811731163159466,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 486,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 486, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253459.49274)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253492.694165)
('Worker processing elapsed time: ', 33.201425075531006, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[486]', 'EPOCH LOSS:', 0.023497363439370304, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 486, ']')


'Epoch [487] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 228,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000479291093653445,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 487,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 487, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253492.69955)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253525.803392)
('Worker processing elapsed time: ', 33.10384202003479, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[487]', 'EPOCH LOSS:', 0.0085192423843994669, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 487, ']')


'Epoch [488] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 497,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006373823463255205,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 488,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 488, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253525.809129)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253559.691548)
('Worker processing elapsed time: ', 33.88241910934448, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[488]', 'EPOCH LOSS:', 4.6526471435423318, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 488, ']')


'Epoch [489] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 356,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008033210969521062,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 489,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 489, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253559.696771)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253593.53669)
('Worker processing elapsed time: ', 33.839919090270996, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[489]', 'EPOCH LOSS:', 0.018122691275912879, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 489, ']')


'Epoch [490] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 611,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004566394872518693,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 5, 1, 7, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 490,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 490, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253593.541614)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253628.921111)
('Worker processing elapsed time: ', 35.379497051239014, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[490]', 'EPOCH LOSS:', 0.021831446884721249, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 490, ']')


'Epoch [491] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 330,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000384486337496447,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 491,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 491, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253628.926051)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253662.694698)
('Worker processing elapsed time: ', 33.76864719390869, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[491]', 'EPOCH LOSS:', 1.2655307120905952, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 491, ']')


'Epoch [492] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 300,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006349795504226386,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 492,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 492, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253662.700008)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253696.113497)
('Worker processing elapsed time: ', 33.41348910331726, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[492]', 'EPOCH LOSS:', 0.40886427034123152, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 492, ']')


'Epoch [493] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 284,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005159627340982173,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 493,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 493, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253696.119319)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253729.409354)
('Worker processing elapsed time: ', 33.290035009384155, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[493]', 'EPOCH LOSS:', 0.023100037052524718, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 493, ']')


'Epoch [494] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 578,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008471522811651537,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 6, 7, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 494,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 494, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253729.415422)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253763.434605)
('Worker processing elapsed time: ', 34.01918292045593, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[494]', 'EPOCH LOSS:', 2.8727001113546784, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 494, ']')


'Epoch [495] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 553,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005969560383886762,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 495,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 495, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253763.439477)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253796.658193)
('Worker processing elapsed time: ', 33.21871614456177, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[495]', 'EPOCH LOSS:', 0.0052208470224596572, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 495, ']')


'Epoch [496] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 247,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00048274267540759377,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 496,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 496, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253796.663583)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253830.5154)
('Worker processing elapsed time: ', 33.8518168926239, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[496]', 'EPOCH LOSS:', 0.65314847200897896, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 496, ']')


'Epoch [497] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 374,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00021330900434585567,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 7, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 497,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 497, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253830.520398)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253864.914936)
('Worker processing elapsed time: ', 34.394538164138794, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[497]', 'EPOCH LOSS:', 0.023491068667990066, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 497, ']')


'Epoch [498] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 412,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00043330654606885675,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 4, 5, 5, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 498,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 498, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253864.921104)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253899.337436)
('Worker processing elapsed time: ', 34.41633200645447, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[498]', 'EPOCH LOSS:', 0.1158576448388068, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 498, ']')


'Epoch [499] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 229,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001907965665718614,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 6, 4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 499,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 499, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253899.342756)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253933.195724)
('Worker processing elapsed time: ', 33.852967977523804, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[499]', 'EPOCH LOSS:', 0.025244569929697938, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 499, ']')


'Epoch [500] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 593,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004761980342451508,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 500,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 500, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253933.200887)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253966.504866)
('Worker processing elapsed time: ', 33.30397891998291, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[500]', 'EPOCH LOSS:', 0.67512569578127479, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 500, ']')


'Epoch [501] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 452,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007639439202112636,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 501,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 501, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253966.510299)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494253999.949551)
('Worker processing elapsed time: ', 33.43925213813782, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[501]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 501, ']')


'Epoch [502] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 554,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00036916148104552374,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 8, 3, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 502,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 502, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494253999.954438)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254034.562758)
('Worker processing elapsed time: ', 34.608319997787476, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[502]', 'EPOCH LOSS:', 0.0264592001828368, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 502, ']')


'Epoch [503] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 472,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005669559755498521,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 503,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 503, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254034.567875)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254068.987361)
('Worker processing elapsed time: ', 34.4194860458374, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[503]', 'EPOCH LOSS:', 0.023410939193049832, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 503, ']')


'Epoch [504] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 264,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008767311602407735,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 504,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 504, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254068.992478)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254102.408006)
('Worker processing elapsed time: ', 33.41552805900574, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[504]', 'EPOCH LOSS:', 0.021966828823897611, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 504, ']')


'Epoch [505] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 513,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00012764930793810728,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 4, 2, 6, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 505,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 505, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254102.414261)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254139.910873)
('Worker processing elapsed time: ', 37.49661183357239, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[505]', 'EPOCH LOSS:', 0.019593834734009309, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 505, ']')


'Epoch [506] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 386,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008083050646814245,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 506,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 506, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254139.915862)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254173.366379)
('Worker processing elapsed time: ', 33.45051693916321, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[506]', 'EPOCH LOSS:', 1.6640441142221751, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 506, ']')


'Epoch [507] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 396,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003516391700151269,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 507,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 507, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254173.37253)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254206.813197)
('Worker processing elapsed time: ', 33.440666913986206, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[507]', 'EPOCH LOSS:', 0.023799761518130815, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 507, ']')


'Epoch [508] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 409,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000542448360170067,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 3, 2, 2, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 508,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 508, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254206.819439)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254251.786504)
('Worker processing elapsed time: ', 44.96706509590149, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[508]', 'EPOCH LOSS:', 0.018413339751318491, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 508, ']')


'Epoch [509] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 328,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009978845200788336,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 509,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 509, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254251.792289)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254285.561273)
('Worker processing elapsed time: ', 33.76898407936096, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[509]', 'EPOCH LOSS:', 0.012834976020877667, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 509, ']')


'Epoch [510] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 607,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00029699894918894037,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 3, 1, 3, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 510,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 510, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254285.566773)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254323.526758)
('Worker processing elapsed time: ', 37.95998501777649, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[510]', 'EPOCH LOSS:', 0.020184591508099266, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 510, ']')


'Epoch [511] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 573,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003848004281371642,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 4, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 511,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 511, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254323.533101)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254358.326637)
('Worker processing elapsed time: ', 34.79353594779968, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[511]', 'EPOCH LOSS:', 0.021810401505278326, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 511, ']')


'Epoch [512] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 558,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009923012221282322,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 1, 8, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 512,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 512, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254358.331974)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254399.352118)
('Worker processing elapsed time: ', 41.02014398574829, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[512]', 'EPOCH LOSS:', 5.0873610575265761e-05, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 512, ']')


'Epoch [513] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 507,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006945960059997302,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 513,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 513, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254399.357562)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254433.067577)
('Worker processing elapsed time: ', 33.71001482009888, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[513]', 'EPOCH LOSS:', 16.101579719226052, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 513, ']')


'Epoch [514] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 418,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008415833498862301,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 6, 5, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 514,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 514, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254433.072723)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254466.85862)
('Worker processing elapsed time: ', 33.78589701652527, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[514]', 'EPOCH LOSS:', 0.024558709724715581, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 514, ']')


'Epoch [515] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 568,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005710751145516913,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 7, 2, 4, 4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 515,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 515, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254466.863664)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254500.952453)
('Worker processing elapsed time: ', 34.088788986206055, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[515]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 515, ']')


'Epoch [516] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 323,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009117555827078515,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 1, 7, 3, 5, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 516,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 516, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254500.958331)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254538.821669)
('Worker processing elapsed time: ', 37.863337993621826, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[516]', 'EPOCH LOSS:', 0.017973906110908027, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 516, ']')


'Epoch [517] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 333,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00013095198077836827,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 517,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 517, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254538.827461)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254572.074056)
('Worker processing elapsed time: ', 33.24659490585327, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[517]', 'EPOCH LOSS:', 1.8778461035460841, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 517, ']')


'Epoch [518] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 484,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009379958750931139,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 518,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 518, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254572.079455)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254605.843664)
('Worker processing elapsed time: ', 33.764209032058716, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[518]', 'EPOCH LOSS:', 0.02411495083430662, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 518, ']')


'Epoch [519] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 615,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009481607980788424,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 7, 1, 8, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 519,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 519, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254605.849851)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254642.185116)
('Worker processing elapsed time: ', 36.335265159606934, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[519]', 'EPOCH LOSS:', 0.038798611614194622, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 519, ']')


'Epoch [520] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 315,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007361783108500509,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 6, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 520,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 520, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254642.190503)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254676.86648)
('Worker processing elapsed time: ', 34.67597723007202, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[520]', 'EPOCH LOSS:', 0.025465949686279542, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 520, ']')


'Epoch [521] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 539,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000348722244111624,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 521,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 521, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254676.871438)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254710.007713)
('Worker processing elapsed time: ', 33.13627505302429, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[521]', 'EPOCH LOSS:', 0.016587099541677041, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 521, ']')


'Epoch [522] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 233,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006544703329784074,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 7, 7, 5, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 522,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 522, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254710.013394)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254745.174688)
('Worker processing elapsed time: ', 35.16129398345947, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[522]', 'EPOCH LOSS:', 0.025388536434534717, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 522, ']')


'Epoch [523] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 567,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009913165497047716,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 8, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 523,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 523, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254745.179598)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254779.28395)
('Worker processing elapsed time: ', 34.10435199737549, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[523]', 'EPOCH LOSS:', 0.040575791926328882, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 523, ']')


'Epoch [524] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 280,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00036665386797045006,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 524,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 524, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254779.289024)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254812.756082)
('Worker processing elapsed time: ', 33.467057943344116, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[524]', 'EPOCH LOSS:', 0.0081546030816332382, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 524, ']')


'Epoch [525] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 262,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00034710140099926553,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 525,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 525, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254812.761695)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254845.888375)
('Worker processing elapsed time: ', 33.12668013572693, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[525]', 'EPOCH LOSS:', 0.016234709587482087, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 525, ']')


'Epoch [526] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 576,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007328704323206061,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 526,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 526, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254845.894044)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254879.153339)
('Worker processing elapsed time: ', 33.25929498672485, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[526]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 526, ']')


'Epoch [527] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 218,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002766105250869066,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 7, 6, 2, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 527,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 527, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254879.158207)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254912.985583)
('Worker processing elapsed time: ', 33.82737612724304, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[527]', 'EPOCH LOSS:', 0.024562073178292787, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 527, ']')


'Epoch [528] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 341,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007653647079120186,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 528,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 528, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254912.990675)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254946.065298)
('Worker processing elapsed time: ', 33.074623107910156, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[528]', 'EPOCH LOSS:', 0.014429324948204097, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 528, ']')


'Epoch [529] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 545,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00012283775205975723,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 529,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 529, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254946.070305)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494254979.706828)
('Worker processing elapsed time: ', 33.63652300834656, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[529]', 'EPOCH LOSS:', 11.263021543633551, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 529, ']')


'Epoch [530] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 323,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005426583368623038,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 3, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 530,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 530, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494254979.711894)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255013.383719)
('Worker processing elapsed time: ', 33.67182493209839, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[530]', 'EPOCH LOSS:', 0.024562645782775228, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 530, ']')


'Epoch [531] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 478,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001621291460579073,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 531,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 531, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255013.388643)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255046.914585)
('Worker processing elapsed time: ', 33.52594208717346, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[531]', 'EPOCH LOSS:', 0.044679950068242512, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 531, ']')


'Epoch [532] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 562,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000511953319649455,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 8, 3, 7, 6, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 532,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 532, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255046.919453)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255082.233385)
('Worker processing elapsed time: ', 35.31393218040466, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[532]', 'EPOCH LOSS:', 0.076320874422216162, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 532, ']')


'Epoch [533] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 589,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000629669134489917,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 5, 7, 2, 5, 8, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 533,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 533, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255082.239085)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255121.761329)
('Worker processing elapsed time: ', 39.52224397659302, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[533]', 'EPOCH LOSS:', 0.00069579498427517483, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 533, ']')


'Epoch [534] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 224,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008215893518555363,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 534,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 534, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255121.766738)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255155.247029)
('Worker processing elapsed time: ', 33.48029112815857, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[534]', 'EPOCH LOSS:', 1.3291562470412774, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 534, ']')


'Epoch [535] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 502,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008581768188357633,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 2, 2, 5, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 535,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 535, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255155.25343)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255192.223842)
('Worker processing elapsed time: ', 36.97041201591492, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[535]', 'EPOCH LOSS:', 0.018061464320364387, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 535, ']')


'Epoch [536] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 613,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00016737917438740106,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 536,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 536, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255192.228622)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255225.482227)
('Worker processing elapsed time: ', 33.253605127334595, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[536]', 'EPOCH LOSS:', 0.017345366789149094, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 536, ']')


'Epoch [537] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 499,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007778845733025276,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 3, 6, 2, 3, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 537,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 537, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255225.487051)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255259.835996)
('Worker processing elapsed time: ', 34.348944902420044, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[537]', 'EPOCH LOSS:', 0.016401324125165512, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 537, ']')


'Epoch [538] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 411,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009146811753194639,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 538,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 538, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255259.84169)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255293.338431)
('Worker processing elapsed time: ', 33.49674081802368, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[538]', 'EPOCH LOSS:', 0.041088200882609657, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 538, ']')


'Epoch [539] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 359,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008115943927226716,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 539,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 539, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255293.344271)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255327.195558)
('Worker processing elapsed time: ', 33.85128712654114, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[539]', 'EPOCH LOSS:', 0.0035647633571561218, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 539, ']')


'Epoch [540] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 289,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008317968780746966,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 540,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 540, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255327.200925)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255360.677987)
('Worker processing elapsed time: ', 33.47706198692322, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[540]', 'EPOCH LOSS:', 0.020503706992212753, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 540, ']')


'Epoch [541] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 322,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007553464500397254,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 541,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 541, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255360.682999)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255394.109638)
('Worker processing elapsed time: ', 33.42663908004761, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[541]', 'EPOCH LOSS:', 0.024603290499671936, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 541, ']')


'Epoch [542] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 338,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006889125989322608,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 6, 6, 8, 2, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 542,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 542, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255394.114611)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255430.872703)
('Worker processing elapsed time: ', 36.758092164993286, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[542]', 'EPOCH LOSS:', 0.04885143608430835, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 542, ']')


'Epoch [543] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 264,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005579540831854757,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 4, 8, 5, 3, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 543,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 543, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255430.878362)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255465.600091)
('Worker processing elapsed time: ', 34.721729040145874, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[543]', 'EPOCH LOSS:', 3.5306530145925481, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 543, ']')


'Epoch [544] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 388,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003954492121275298,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 544,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 544, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255465.604988)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255499.02107)
('Worker processing elapsed time: ', 33.41608190536499, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[544]', 'EPOCH LOSS:', 0.023362622056777308, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 544, ']')


'Epoch [545] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 372,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00044031250084708353,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 3, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 545,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 545, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255499.026193)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255532.934095)
('Worker processing elapsed time: ', 33.907902002334595, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[545]', 'EPOCH LOSS:', 9.3101324519077924, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 545, ']')


'Epoch [546] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 514,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009918691935584055,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1, 5, 7, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 546,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 546, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255532.939927)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255567.203096)
('Worker processing elapsed time: ', 34.263168811798096, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[546]', 'EPOCH LOSS:', 0.71428102669005422, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 546, ']')


'Epoch [547] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 401,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00016127637168525106,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 547,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 547, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255567.208766)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255600.476088)
('Worker processing elapsed time: ', 33.267322063446045, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[547]', 'EPOCH LOSS:', 0.38826529160150564, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 547, ']')


'Epoch [548] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 301,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003474187435659931,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 1, 4, 5, 5, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 548,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 548, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255600.481871)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255634.845283)
('Worker processing elapsed time: ', 34.36341214179993, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[548]', 'EPOCH LOSS:', 0.021858023787671668, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 548, ']')


'Epoch [549] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 606,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008943721630897357,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 1, 4, 2, 4, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 549,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 549, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255634.850691)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255669.045604)
('Worker processing elapsed time: ', 34.194912910461426, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[549]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 549, ']')


'Epoch [550] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 532,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009348073401940746,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 550,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 550, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255669.050896)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255702.891647)
('Worker processing elapsed time: ', 33.84075117111206, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[550]', 'EPOCH LOSS:', 0.20532182837667762, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 550, ']')


'Epoch [551] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 392,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007572822910779401,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 5, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 551,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 551, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255702.897435)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255736.969428)
('Worker processing elapsed time: ', 34.07199311256409, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[551]', 'EPOCH LOSS:', 1.5378959938947372, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 551, ']')


'Epoch [552] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 344,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00014128706123878643,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 552,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 552, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255736.975253)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255772.285932)
('Worker processing elapsed time: ', 35.31067895889282, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[552]', 'EPOCH LOSS:', 0.032690149822642484, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 552, ']')


'Epoch [553] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 309,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00035085348553389384,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 553,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 553, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255772.290835)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255806.659024)
('Worker processing elapsed time: ', 34.368189096450806, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[553]', 'EPOCH LOSS:', 0.019597079245702607, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 553, ']')


'Epoch [554] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 462,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005858566131023406,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 554,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 554, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255806.663843)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255845.337763)
('Worker processing elapsed time: ', 38.67392015457153, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[554]', 'EPOCH LOSS:', 0.024782290792941784, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 554, ']')


'Epoch [555] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 505,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006212260406578777,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 8, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 555,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 555, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255845.343919)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255879.446679)
('Worker processing elapsed time: ', 34.10276007652283, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[555]', 'EPOCH LOSS:', 0.59515874404725388, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 555, ']')


'Epoch [556] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 339,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005906473137896347,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 556,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 556, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255879.451758)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255912.899595)
('Worker processing elapsed time: ', 33.447837114334106, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[556]', 'EPOCH LOSS:', 0.024316344635290737, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 556, ']')


'Epoch [557] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 411,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006565438101062075,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 8, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 557,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 557, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255912.905538)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255946.459016)
('Worker processing elapsed time: ', 33.55347800254822, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[557]', 'EPOCH LOSS:', 0.023114266399587104, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 557, ']')


'Epoch [558] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 266,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00032660139431573906,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 558,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 558, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255946.463962)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494255979.664104)
('Worker processing elapsed time: ', 33.20014190673828, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[558]', 'EPOCH LOSS:', 0.024192038137626423, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 558, ']')


'Epoch [559] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 222,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007011051462165323,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 559,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 559, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494255979.669641)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256012.936925)
('Worker processing elapsed time: ', 33.26728391647339, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[559]', 'EPOCH LOSS:', 0.024052690585883649, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 559, ']')


'Epoch [560] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 459,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00019155711969126186,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 5, 7, 7, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 560,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 560, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256012.941917)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256047.48266)
('Worker processing elapsed time: ', 34.54074311256409, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[560]', 'EPOCH LOSS:', 21.351305617294507, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 560, ']')


'Epoch [561] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 446,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008190345342172613,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 7, 8, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 561,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 561, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256047.489003)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256081.258246)
('Worker processing elapsed time: ', 33.769243001937866, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[561]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 561, ']')


'Epoch [562] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 483,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00017327851725300043,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 562,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 562, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256081.263257)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256114.737155)
('Worker processing elapsed time: ', 33.47389793395996, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[562]', 'EPOCH LOSS:', 0.02410222366206902, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 562, ']')


'Epoch [563] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 221,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00034859379731831685,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 3, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 563,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 563, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256114.742711)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256148.872578)
('Worker processing elapsed time: ', 34.1298668384552, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[563]', 'EPOCH LOSS:', 2.1535548623612994, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 563, ']')


'Epoch [564] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 553,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005825587088679473,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 564,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 564, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256148.877954)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256182.150756)
('Worker processing elapsed time: ', 33.272801876068115, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[564]', 'EPOCH LOSS:', 0.027056056978540945, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 564, ']')


'Epoch [565] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 289,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00010130566074131506,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 6, 3, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 565,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 565, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256182.155956)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256216.012373)
('Worker processing elapsed time: ', 33.85641694068909, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[565]', 'EPOCH LOSS:', 0.023263864865903258, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 565, ']')


'Epoch [566] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 460,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002346227466994928,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 6, 4, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 566,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 566, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256216.017956)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256249.818872)
('Worker processing elapsed time: ', 33.80091595649719, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[566]', 'EPOCH LOSS:', 0.023891690572441841, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 566, ']')


'Epoch [567] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 390,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00035653341716529983,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 6, 5, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 567,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 567, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256249.82499)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256284.144384)
('Worker processing elapsed time: ', 34.31939387321472, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[567]', 'EPOCH LOSS:', 1.8539494876058782, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 567, ']')


'Epoch [568] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 570,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007237332114824169,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 568,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 568, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256284.150321)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256317.73654)
('Worker processing elapsed time: ', 33.58621907234192, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[568]', 'EPOCH LOSS:', 0.17962894873725174, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 568, ']')


'Epoch [569] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 587,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008801150209858632,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 569,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 569, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256317.742023)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256351.360583)
('Worker processing elapsed time: ', 33.61856007575989, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[569]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 569, ']')


'Epoch [570] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 567,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00026510193345691084,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 570,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 570, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256351.365666)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256384.912534)
('Worker processing elapsed time: ', 33.546868085861206, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[570]', 'EPOCH LOSS:', 0.030406320066741339, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 570, ']')


'Epoch [571] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 305,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007792353335884897,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 571,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 571, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256384.918156)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256418.564486)
('Worker processing elapsed time: ', 33.64633011817932, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[571]', 'EPOCH LOSS:', 6.3295386471534263, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 571, ']')


'Epoch [572] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 480,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00034721184324861,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 5, 4, 5, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 572,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 572, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256418.56992)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256455.687472)
('Worker processing elapsed time: ', 37.117552042007446, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[572]', 'EPOCH LOSS:', 0.012519038248838935, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 572, ']')


'Epoch [573] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 322,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00044667157785115955,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 7, 5, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 573,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 573, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256455.693607)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256489.532671)
('Worker processing elapsed time: ', 33.83906388282776, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[573]', 'EPOCH LOSS:', 0.024630627661652936, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 573, ']')


'Epoch [574] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 245,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000397489237582531,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 8, 8, 8, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 574,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 574, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256489.53814)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256523.357508)
('Worker processing elapsed time: ', 33.8193678855896, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[574]', 'EPOCH LOSS:', 0.0047952410449213032, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 574, ']')


'Epoch [575] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 332,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004506609749429356,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 575,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 575, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256523.362471)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256556.524305)
('Worker processing elapsed time: ', 33.16183400154114, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[575]', 'EPOCH LOSS:', 0.013573602028585652, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 575, ']')


'Epoch [576] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 538,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009397711076143439,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 576,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 576, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256556.529842)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256589.999476)
('Worker processing elapsed time: ', 33.46963405609131, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[576]', 'EPOCH LOSS:', 0.93117634732424404, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 576, ']')


'Epoch [577] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 408,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008133702425002537,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 7, 6, 2, 5, 2, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 577,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 577, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256590.004639)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256625.091753)
('Worker processing elapsed time: ', 35.087114095687866, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[577]', 'EPOCH LOSS:', 0.095346754617572971, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 577, ']')


'Epoch [578] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 244,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007274463475379224,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 7, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 578,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 578, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256625.096993)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256658.947697)
('Worker processing elapsed time: ', 33.850703954696655, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[578]', 'EPOCH LOSS:', 6.1305047520287408, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 578, ']')


'Epoch [579] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 267,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008994134609930467,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 579,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 579, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256658.953179)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256692.719609)
('Worker processing elapsed time: ', 33.766430139541626, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[579]', 'EPOCH LOSS:', 0.031058382938813217, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 579, ']')


'Epoch [580] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 420,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002510814074918494,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 3, 3, 6, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 580,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 580, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256692.72491)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256729.665593)
('Worker processing elapsed time: ', 36.940682888031006, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[580]', 'EPOCH LOSS:', 0.015970322274531663, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 580, ']')


'Epoch [581] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 257,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008472526338040224,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 4, 3, 5, 8, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 581,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 581, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256729.67126)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256764.394436)
('Worker processing elapsed time: ', 34.72317576408386, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[581]', 'EPOCH LOSS:', 0.033069459537187554, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 581, ']')


'Epoch [582] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 324,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006587236657015567,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 6, 2, 1, 7, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 582,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 582, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256764.400835)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256802.265157)
('Worker processing elapsed time: ', 37.86432194709778, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[582]', 'EPOCH LOSS:', 0.023247104876902982, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 582, ']')


'Epoch [583] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 423,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00040142396432752856,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 6, 6, 5, 3, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 583,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 583, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256802.270362)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256839.270502)
('Worker processing elapsed time: ', 37.00014019012451, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[583]', 'EPOCH LOSS:', 0.015960429148420441, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 583, ']')


'Epoch [584] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 433,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006546585791066315,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 584,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 584, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256839.275828)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256872.552522)
('Worker processing elapsed time: ', 33.27669405937195, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[584]', 'EPOCH LOSS:', 0.0012887330742980068, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 584, ']')


'Epoch [585] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 500,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004566783676604513,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 5, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 585,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 585, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256872.558196)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256906.687641)
('Worker processing elapsed time: ', 34.12944483757019, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[585]', 'EPOCH LOSS:', 45.089194811611712, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 585, ']')


'Epoch [586] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 616,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00017011240937612408,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 586,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 586, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256906.693384)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256940.508872)
('Worker processing elapsed time: ', 33.81548810005188, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[586]', 'EPOCH LOSS:', 0.081729636537805614, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 586, ']')


'Epoch [587] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 584,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006140623231016735,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 587,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 587, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256940.513663)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494256974.933327)
('Worker processing elapsed time: ', 34.41966390609741, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[587]', 'EPOCH LOSS:', 0.027989117681256195, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 587, ']')


'Epoch [588] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 588,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005628578709335158,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 588,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 588, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494256974.938353)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257008.201674)
('Worker processing elapsed time: ', 33.26332092285156, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[588]', 'EPOCH LOSS:', 1.2190034888020309, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 588, ']')


'Epoch [589] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 290,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006741578138761458,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 4, 1, 4, 4, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 589,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 589, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257008.208105)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257043.2248)
('Worker processing elapsed time: ', 35.01669502258301, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[589]', 'EPOCH LOSS:', 14.891200873937303, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 589, ']')


'Epoch [590] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 283,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000935464245652759,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 8, 2, 5, 7, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 590,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 590, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257043.230115)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257078.150305)
('Worker processing elapsed time: ', 34.92019009590149, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[590]', 'EPOCH LOSS:', 0.11721763232008948, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 590, ']')


'Epoch [591] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 540,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007395644014766333,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 591,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 591, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257078.155509)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257111.602531)
('Worker processing elapsed time: ', 33.44702196121216, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[591]', 'EPOCH LOSS:', 0.19418847089434574, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 591, ']')


'Epoch [592] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 313,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00010832254220408523,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 592,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 592, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257111.608522)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257145.450714)
('Worker processing elapsed time: ', 33.84219217300415, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[592]', 'EPOCH LOSS:', 3.743180216258196, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 592, ']')


'Epoch [593] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 339,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000713811597796659,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 8, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 593,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 593, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257145.456387)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257180.156605)
('Worker processing elapsed time: ', 34.700217962265015, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[593]', 'EPOCH LOSS:', 0.00079247675536510181, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 593, ']')


'Epoch [594] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 290,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008084536511755462,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 594,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 594, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257180.162192)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257213.896884)
('Worker processing elapsed time: ', 33.734691858291626, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[594]', 'EPOCH LOSS:', 0.070722722235896027, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 594, ']')


'Epoch [595] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 358,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007012988065692775,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 595,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 595, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257213.902004)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257247.729535)
('Worker processing elapsed time: ', 33.82753109931946, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[595]', 'EPOCH LOSS:', 0.006790861331953852, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 595, ']')


'Epoch [596] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 284,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00036642408816009084,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 3, 2, 3, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 596,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 596, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257247.734916)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257282.214237)
('Worker processing elapsed time: ', 34.479321002960205, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[596]', 'EPOCH LOSS:', 2.517788242393451, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 596, ']')


'Epoch [597] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 566,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003928356145882925,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 597,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 597, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257282.219196)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257315.964029)
('Worker processing elapsed time: ', 33.74483299255371, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[597]', 'EPOCH LOSS:', 0.024388388028342967, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 597, ']')


'Epoch [598] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 432,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00019448298145550297,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 598,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 598, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257315.969351)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257350.354653)
('Worker processing elapsed time: ', 34.3853018283844, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[598]', 'EPOCH LOSS:', 0.025000938895103304, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 598, ']')


'Epoch [599] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 503,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005904906917940419,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 599,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 599, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257350.360376)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257383.744513)
('Worker processing elapsed time: ', 33.38413715362549, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[599]', 'EPOCH LOSS:', 0.3527575206349014, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 599, ']')


'Epoch [600] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 538,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007225994953520344,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 7, 1, 3, 1, 7, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 600,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 600, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257383.750284)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257420.872185)
('Worker processing elapsed time: ', 37.12190103530884, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[600]', 'EPOCH LOSS:', 0.020645548872584692, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 600, ']')


'Epoch [601] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 296,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005257903739195278,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 601,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 601, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257420.877275)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257453.961179)
('Worker processing elapsed time: ', 33.08390402793884, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[601]', 'EPOCH LOSS:', 0.011678552408131615, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 601, ']')


'Epoch [602] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 436,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008748804839362765,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 8, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 602,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 602, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257453.966123)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257487.677154)
('Worker processing elapsed time: ', 33.71103096008301, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[602]', 'EPOCH LOSS:', 0.024925472236389801, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 602, ']')


'Epoch [603] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 324,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002131980560661198,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 1, 1, 1, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 603,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 603, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257487.682495)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257522.83229)
('Worker processing elapsed time: ', 35.149794816970825, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[603]', 'EPOCH LOSS:', 0.022900060123303929, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 603, ']')


'Epoch [604] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 380,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007028498260844812,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 604,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 604, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257522.83776)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257556.331374)
('Worker processing elapsed time: ', 33.493613958358765, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[604]', 'EPOCH LOSS:', 0.015711383152006663, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 604, ']')


'Epoch [605] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 514,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008200285141052453,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 3, 8, 6, 7, 1, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 605,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 605, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257556.33656)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257595.087257)
('Worker processing elapsed time: ', 38.750696897506714, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[605]', 'EPOCH LOSS:', 0.009537487425238177, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 605, ']')


'Epoch [606] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 360,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00024240749028320372,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 2, 1, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 606,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 606, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257595.092596)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257629.762525)
('Worker processing elapsed time: ', 34.66992902755737, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[606]', 'EPOCH LOSS:', 0.022349307671106322, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 606, ']')


'Epoch [607] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 493,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000293311711790952,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 607,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 607, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257629.767655)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257663.576701)
('Worker processing elapsed time: ', 33.809046030044556, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[607]', 'EPOCH LOSS:', 1.2804641579861755, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 607, ']')


'Epoch [608] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 224,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00034804272138912,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 7, 6, 3, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 608,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 608, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257663.582181)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257698.018017)
('Worker processing elapsed time: ', 34.43583607673645, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[608]', 'EPOCH LOSS:', 1.3205484078510457, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 608, ']')


'Epoch [609] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 393,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008144716518011028,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 7, 1, 8, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 609,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 609, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257698.023005)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257732.484968)
('Worker processing elapsed time: ', 34.461962938308716, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[609]', 'EPOCH LOSS:', 0.55636125668802883, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 609, ']')


'Epoch [610] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 610,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009736550247570915,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 3, 5, 1, 8, 4, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 610,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 610, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257732.490059)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257766.613387)
('Worker processing elapsed time: ', 34.12332820892334, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[610]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 610, ']')


'Epoch [611] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 237,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005630809933471381,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 4, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 611,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 611, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257766.618252)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257800.285642)
('Worker processing elapsed time: ', 33.66738986968994, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[611]', 'EPOCH LOSS:', 0.023491990452621979, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 611, ']')


'Epoch [612] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 241,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00021702840655277395,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 612,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 612, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257800.291624)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257833.942241)
('Worker processing elapsed time: ', 33.65061688423157, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[612]', 'EPOCH LOSS:', 1.2598849611027207, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 612, ']')


'Epoch [613] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 274,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002807811466407768,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 613,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 613, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257833.94713)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257867.439405)
('Worker processing elapsed time: ', 33.49227499961853, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[613]', 'EPOCH LOSS:', 0.024066770314261559, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 613, ']')


'Epoch [614] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 221,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006064031162626798,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 8, 6, 8, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 614,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 614, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257867.445303)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257901.778712)
('Worker processing elapsed time: ', 34.33340907096863, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[614]', 'EPOCH LOSS:', 230.96973151026941, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 614, ']')


'Epoch [615] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 321,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001413530295389005,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 615,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 615, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257901.784886)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257935.095688)
('Worker processing elapsed time: ', 33.31080222129822, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[615]', 'EPOCH LOSS:', 0.024691641422809184, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 615, ']')


'Epoch [616] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 588,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00036139251510751836,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 616,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 616, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257935.101454)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494257969.025115)
('Worker processing elapsed time: ', 33.92366099357605, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[616]', 'EPOCH LOSS:', 0.62446596117059794, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 616, ']')


'Epoch [617] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 488,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005990929275682694,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 8, 2, 8, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 617,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 617, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494257969.030135)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258003.242043)
('Worker processing elapsed time: ', 34.21190810203552, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[617]', 'EPOCH LOSS:', 0.18190919058357191, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 617, ']')


'Epoch [618] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 419,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00036115652889750214,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 7, 2, 4, 8, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 618,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 618, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258003.247188)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258041.557484)
('Worker processing elapsed time: ', 38.310295820236206, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[618]', 'EPOCH LOSS:', 0.019887473459527662, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 618, ']')


'Epoch [619] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 341,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008574407441475798,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 5, 2, 7, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 619,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 619, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258041.563499)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258075.996398)
('Worker processing elapsed time: ', 34.4328989982605, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[619]', 'EPOCH LOSS:', 0.025776551184168633, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 619, ']')


'Epoch [620] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 382,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003829323717631141,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 620,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 620, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258076.002362)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258109.489902)
('Worker processing elapsed time: ', 33.48754000663757, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[620]', 'EPOCH LOSS:', 0.023118097833137612, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 620, ']')


'Epoch [621] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 586,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005516759132877744,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 8, 8, 8, 1, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 621,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 621, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258109.495189)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258145.846227)
('Worker processing elapsed time: ', 36.35103797912598, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[621]', 'EPOCH LOSS:', 0.03107078778205781, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 621, ']')


'Epoch [622] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 544,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009576281467043404,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 622,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 622, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258145.8514)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258179.584239)
('Worker processing elapsed time: ', 33.73283910751343, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[622]', 'EPOCH LOSS:', 0.012956472422523345, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 622, ']')


'Epoch [623] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 428,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003495047766253393,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 623,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 623, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258179.589745)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258213.259766)
('Worker processing elapsed time: ', 33.670021057128906, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[623]', 'EPOCH LOSS:', 6.6176195950722558, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 623, ']')


'Epoch [624] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 296,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007868380933827617,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 5, 1, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 624,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 624, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258213.264981)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258247.563163)
('Worker processing elapsed time: ', 34.298182010650635, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[624]', 'EPOCH LOSS:', 46.264175997229017, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 624, ']')


'Epoch [625] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 241,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006628926782088996,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 7, 5, 5, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 625,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 625, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258247.569392)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258282.761532)
('Worker processing elapsed time: ', 35.192140102386475, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[625]', 'EPOCH LOSS:', 0.082108058636637005, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 625, ']')


'Epoch [626] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 252,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00040599232008712324,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 626,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 626, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258282.767221)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258316.221115)
('Worker processing elapsed time: ', 33.45389413833618, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[626]', 'EPOCH LOSS:', 0.024867672830534215, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 626, ']')


'Epoch [627] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 607,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006035037434379172,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 4, 7, 3, 8, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 627,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 627, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258316.226188)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258352.654292)
('Worker processing elapsed time: ', 36.42810416221619, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[627]', 'EPOCH LOSS:', 0.36598234157418463, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 627, ']')


'Epoch [628] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 342,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00030031725210587614,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 628,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 628, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258352.670364)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258385.920281)
('Worker processing elapsed time: ', 33.24991703033447, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[628]', 'EPOCH LOSS:', 0.022594688752950672, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 628, ']')


'Epoch [629] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 344,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00040055871587934633,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 4, 6, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 629,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 629, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258385.926014)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258421.187224)
('Worker processing elapsed time: ', 35.2612099647522, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[629]', 'EPOCH LOSS:', 0.024999794102295098, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 629, ']')


'Epoch [630] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 240,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00056930299822308,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 2, 2, 7, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 630,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 630, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258421.19334)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258456.336474)
('Worker processing elapsed time: ', 35.143133878707886, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[630]', 'EPOCH LOSS:', 0.023889136833776099, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 630, ']')


'Epoch [631] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 559,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002192706142346031,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 631,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 631, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258456.341599)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258489.636004)
('Worker processing elapsed time: ', 33.29440498352051, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[631]', 'EPOCH LOSS:', 0.36902083769206673, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 631, ']')


'Epoch [632] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 316,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00022550698202994195,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 3, 5, 4, 2, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 632,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 632, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258489.641103)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258523.534364)
('Worker processing elapsed time: ', 33.89326095581055, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[632]', 'EPOCH LOSS:', 0.023292519684966984, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 632, ']')


'Epoch [633] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 217,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007645152290920042,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 633,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 633, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258523.539708)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258557.347587)
('Worker processing elapsed time: ', 33.80787920951843, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[633]', 'EPOCH LOSS:', 0.0051085290701848342, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 633, ']')


'Epoch [634] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 365,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005218118055881132,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 8, 1, 6, 2, 4, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 634,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 634, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258557.353695)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258592.038241)
('Worker processing elapsed time: ', 34.68454599380493, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[634]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 634, ']')


'Epoch [635] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 278,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005708694615924976,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 8, 7, 2, 7, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 635,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 635, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258592.044618)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258629.768638)
('Worker processing elapsed time: ', 37.72402000427246, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[635]', 'EPOCH LOSS:', 0.014884065071262447, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 635, ']')


'Epoch [636] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 526,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005519049718869669,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 6, 4, 8, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 636,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 636, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258629.775268)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258664.087284)
('Worker processing elapsed time: ', 34.312016010284424, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[636]', 'EPOCH LOSS:', 1.8403544112916426, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 636, ']')


'Epoch [637] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 569,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004988471704801069,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 5, 5, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 637,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 637, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258664.092275)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258698.284594)
('Worker processing elapsed time: ', 34.19231915473938, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[637]', 'EPOCH LOSS:', 3.3348016851519429, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 637, ']')


'Epoch [638] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 305,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003556661777729428,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 638,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 638, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258698.289736)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258732.708841)
('Worker processing elapsed time: ', 34.419105052948, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[638]', 'EPOCH LOSS:', 0.027204486001790561, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 638, ']')


'Epoch [639] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 357,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00021543937948050637,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 1, 6, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 639,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 639, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258732.713936)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258767.035281)
('Worker processing elapsed time: ', 34.32134485244751, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[639]', 'EPOCH LOSS:', 2.1270551118824601, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 639, ']')


'Epoch [640] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 346,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006343418667523926,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 6, 8, 2, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 640,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 640, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258767.040414)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258801.028683)
('Worker processing elapsed time: ', 33.98826885223389, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[640]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 640, ']')


'Epoch [641] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 314,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007333951602719369,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 641,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 641, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258801.034225)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258834.889859)
('Worker processing elapsed time: ', 33.85563397407532, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[641]', 'EPOCH LOSS:', 239.41816490389706, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 641, ']')


'Epoch [642] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 266,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007150647424563851,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 642,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 642, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258834.895713)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258868.749953)
('Worker processing elapsed time: ', 33.85423994064331, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[642]', 'EPOCH LOSS:', 3.6469687129991764, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 642, ']')


'Epoch [643] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 375,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007163414072379337,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 643,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 643, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258868.754877)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258902.024201)
('Worker processing elapsed time: ', 33.26932382583618, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[643]', 'EPOCH LOSS:', 0.81449939939688587, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 643, ']')


'Epoch [644] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 471,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00024625255155351496,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 3, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 644,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 644, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258902.029642)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258936.100619)
('Worker processing elapsed time: ', 34.070976972579956, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[644]', 'EPOCH LOSS:', 1.246146133250924, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 644, ']')


'Epoch [645] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 547,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00048531455374339254,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 645,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 645, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258936.106567)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494258969.607081)
('Worker processing elapsed time: ', 33.50051403045654, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[645]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 645, ']')


'Epoch [646] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 426,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00012218105816620977,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 646,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 646, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494258969.61192)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259003.735802)
('Worker processing elapsed time: ', 34.123881816864014, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[646]', 'EPOCH LOSS:', 0.033847041210443234, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 646, ']')


'Epoch [647] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 365,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006296561631309134,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 3, 7, 3, 7, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 647,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 647, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259003.741304)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259041.726251)
('Worker processing elapsed time: ', 37.984946966171265, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[647]', 'EPOCH LOSS:', 0.017543782144780709, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 647, ']')


'Epoch [648] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 428,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003318908526591275,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 7, 4, 2, 4, 3, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 648,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 648, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259041.731512)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259076.791149)
('Worker processing elapsed time: ', 35.05963683128357, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[648]', 'EPOCH LOSS:', 17.990384549717501, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 648, ']')


'Epoch [649] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 213,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006877824313918294,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 7, 1, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 649,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 649, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259076.796358)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259111.129461)
('Worker processing elapsed time: ', 34.33310294151306, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[649]', 'EPOCH LOSS:', 1.7950330114415343, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 649, ']')


'Epoch [650] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 409,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00016679108989172743,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 650,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 650, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259111.134541)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259144.35336)
('Worker processing elapsed time: ', 33.21881890296936, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[650]', 'EPOCH LOSS:', 2.9112503104563752, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 650, ']')


'Epoch [651] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 301,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006064538942545971,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 651,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 651, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259144.358252)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259177.904324)
('Worker processing elapsed time: ', 33.546072006225586, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[651]', 'EPOCH LOSS:', 0.015754289535720313, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 651, ']')


'Epoch [652] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 472,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009246083538383145,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 652,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 652, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259177.91013)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259211.944044)
('Worker processing elapsed time: ', 34.03391408920288, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[652]', 'EPOCH LOSS:', 0.023579243274211042, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 652, ']')


'Epoch [653] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 527,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00040418168329626504,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 653,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 653, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259211.949624)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259245.383493)
('Worker processing elapsed time: ', 33.43386888504028, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[653]', 'EPOCH LOSS:', 0.52202547126871446, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 653, ']')


'Epoch [654] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 330,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005327861055292122,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 7, 1, 4, 6, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 654,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 654, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259245.389419)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259282.051104)
('Worker processing elapsed time: ', 36.6616849899292, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[654]', 'EPOCH LOSS:', 0.02011920897194085, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 654, ']')


'Epoch [655] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 374,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005978184671528673,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 1, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 655,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 655, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259282.056808)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259315.620157)
('Worker processing elapsed time: ', 33.56334900856018, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[655]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 655, ']')


'Epoch [656] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 443,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003938500106429972,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 7, 7, 7, 6, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 656,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 656, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259315.625737)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259354.372094)
('Worker processing elapsed time: ', 38.74635696411133, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[656]', 'EPOCH LOSS:', 0.021139009390020315, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 656, ']')


'Epoch [657] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 247,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003432351499679747,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1, 3, 3, 5, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 657,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 657, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259354.377879)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259388.934354)
('Worker processing elapsed time: ', 34.5564751625061, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[657]', 'EPOCH LOSS:', 534.41016307327311, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 657, ']')


'Epoch [658] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 277,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003383905330082167,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 3, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 658,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 658, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259388.941174)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259422.674441)
('Worker processing elapsed time: ', 33.733267068862915, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[658]', 'EPOCH LOSS:', 0.02388665091645386, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 658, ']')


'Epoch [659] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 505,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000973593436538594,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 659,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 659, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259422.679348)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259456.201736)
('Worker processing elapsed time: ', 33.522387981414795, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[659]', 'EPOCH LOSS:', 0.016085252115724452, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 659, ']')


'Epoch [660] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 582,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008448840787355361,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 5, 1, 7, 8, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 660,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 660, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259456.207049)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259491.735449)
('Worker processing elapsed time: ', 35.528400182724, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[660]', 'EPOCH LOSS:', 0.026463292314892296, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 660, ']')


'Epoch [661] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 517,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009033441927408457,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 8, 2, 2, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 661,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 661, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259491.740957)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259525.621425)
('Worker processing elapsed time: ', 33.880467891693115, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[661]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 661, ']')


'Epoch [662] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 333,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002224433521993295,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 662,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 662, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259525.62651)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259559.359953)
('Worker processing elapsed time: ', 33.73344302177429, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[662]', 'EPOCH LOSS:', 0.44073877624625069, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 662, ']')


'Epoch [663] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 543,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00032064222400034756,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 6, 7, 7, 7, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 663,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 663, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259559.365239)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259596.705544)
('Worker processing elapsed time: ', 37.34030508995056, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[663]', 'EPOCH LOSS:', 0.026683524067363688, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 663, ']')


'Epoch [664] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 296,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006491293786224049,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 3, 7, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 664,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 664, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259596.710691)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259631.861525)
('Worker processing elapsed time: ', 35.15083408355713, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[664]', 'EPOCH LOSS:', 0.023581645805996643, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 664, ']')


'Epoch [665] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 511,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001164013896568359,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 6, 3, 8, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 665,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 665, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259631.867779)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259666.084222)
('Worker processing elapsed time: ', 34.21644306182861, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[665]', 'EPOCH LOSS:', 10.979624338556068, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 665, ']')


'Epoch [666] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 376,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00040437632554888854,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 5, 7, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 666,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 666, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259666.090447)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259701.642298)
('Worker processing elapsed time: ', 35.55185103416443, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[666]', 'EPOCH LOSS:', 0.023795543478872583, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 666, ']')


'Epoch [667] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 588,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001650201319645197,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 667,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 667, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259701.648268)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259735.21067)
('Worker processing elapsed time: ', 33.56240200996399, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[667]', 'EPOCH LOSS:', 0.026195611939384668, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 667, ']')


'Epoch [668] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 217,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004742514981193605,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 668,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 668, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259735.215454)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259768.718231)
('Worker processing elapsed time: ', 33.502776861190796, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[668]', 'EPOCH LOSS:', 0.022132603990146955, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 668, ']')


'Epoch [669] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 298,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00036237227968630213,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 669,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 669, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259768.723211)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259801.851663)
('Worker processing elapsed time: ', 33.12845206260681, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[669]', 'EPOCH LOSS:', 0.014097671632269528, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 669, ']')


'Epoch [670] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 619,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008585329838368784,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 6, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 670,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 670, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259801.857238)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259835.47842)
('Worker processing elapsed time: ', 33.62118196487427, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[670]', 'EPOCH LOSS:', 0.025429736811026869, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 670, ']')


'Epoch [671] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 458,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000318546198800369,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 8, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 671,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 671, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259835.485527)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259869.314086)
('Worker processing elapsed time: ', 33.828558921813965, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[671]', 'EPOCH LOSS:', 2.2530751202189343, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 671, ']')


'Epoch [672] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 474,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006291026907549882,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 7, 8, 1, 7, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 672,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 672, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259869.319066)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259903.599339)
('Worker processing elapsed time: ', 34.28027296066284, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[672]', 'EPOCH LOSS:', 0.026424065923273749, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 672, ']')


'Epoch [673] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 565,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006916275356114729,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 1, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 673,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 673, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259903.604281)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259937.237831)
('Worker processing elapsed time: ', 33.63355016708374, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[673]', 'EPOCH LOSS:', 0.026583996619341098, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 673, ']')


'Epoch [674] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 311,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002956133728755665,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 674,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 674, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259937.242583)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494259970.74012)
('Worker processing elapsed time: ', 33.4975368976593, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[674]', 'EPOCH LOSS:', 0.023031780165034606, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 674, ']')


'Epoch [675] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 388,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000882675047479901,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 8, 4, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 675,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 675, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494259970.745657)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260006.535959)
('Worker processing elapsed time: ', 35.79030203819275, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[675]', 'EPOCH LOSS:', 0.023243480147864284, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 675, ']')


'Epoch [676] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 616,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004918088429225398,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 4, 4, 6, 8, 7, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 676,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 676, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260006.541823)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260043.203559)
('Worker processing elapsed time: ', 36.66173601150513, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[676]', 'EPOCH LOSS:', 2.5414415739274117, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 676, ']')


'Epoch [677] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 547,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005310272272362457,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 1, 2, 7, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 677,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 677, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260043.209335)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260077.100699)
('Worker processing elapsed time: ', 33.891363859176636, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[677]', 'EPOCH LOSS:', 0.026006249579248367, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 677, ']')


'Epoch [678] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 493,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00020060823579261158,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 6, 3, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 678,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 678, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260077.105942)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260110.853584)
('Worker processing elapsed time: ', 33.747642040252686, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[678]', 'EPOCH LOSS:', 0.024004133140097975, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 678, ']')


'Epoch [679] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 459,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005485657807515354,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 7, 2, 3, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 679,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 679, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260110.85891)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260144.781989)
('Worker processing elapsed time: ', 33.92307901382446, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[679]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 679, ']')


'Epoch [680] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 602,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006492348931032307,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 1, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 680,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 680, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260144.787737)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260179.132295)
('Worker processing elapsed time: ', 34.344558000564575, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[680]', 'EPOCH LOSS:', 0.027421733318935021, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 680, ']')


'Epoch [681] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 275,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006378723743430886,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 681,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 681, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260179.137176)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260212.563446)
('Worker processing elapsed time: ', 33.42627000808716, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[681]', 'EPOCH LOSS:', 0.0082469444437280014, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 681, ']')


'Epoch [682] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 617,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008705778077378558,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 1, 6, 8, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 682,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 682, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260212.569005)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260249.127952)
('Worker processing elapsed time: ', 36.55894708633423, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[682]', 'EPOCH LOSS:', 0.018654433459964592, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 682, ']')


'Epoch [683] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 512,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000317084054064463,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 683,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 683, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260249.133618)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260282.777826)
('Worker processing elapsed time: ', 33.64420795440674, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[683]', 'EPOCH LOSS:', 4.5386405244043235, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 683, ']')


'Epoch [684] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 218,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00045444998470184643,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 684,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 684, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260282.783425)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260315.879386)
('Worker processing elapsed time: ', 33.09596085548401, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[684]', 'EPOCH LOSS:', 0.015612854144205223, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 684, ']')


'Epoch [685] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 517,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0002587374632978647,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 7, 1, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 685,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 685, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260315.884356)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260350.115198)
('Worker processing elapsed time: ', 34.230841875076294, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[685]', 'EPOCH LOSS:', 1067.7944581856682, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 685, ']')


'Epoch [686] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 353,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005257269132992622,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 686,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 686, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260350.120397)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260383.574781)
('Worker processing elapsed time: ', 33.454383850097656, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[686]', 'EPOCH LOSS:', 0.69303142623154212, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 686, ']')


'Epoch [687] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 376,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004375086141722676,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 2, 4, 6, 3, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 687,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 687, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260383.579787)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260421.451759)
('Worker processing elapsed time: ', 37.87197208404541, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[687]', 'EPOCH LOSS:', 0.021857819405182406, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 687, ']')


'Epoch [688] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 306,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000982107870259492,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 688,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 688, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260421.457414)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260455.86574)
('Worker processing elapsed time: ', 34.408326148986816, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[688]', 'EPOCH LOSS:', 0.023560729397477446, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 688, ']')


'Epoch [689] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 302,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00047722712157260375,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 689,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 689, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260455.871782)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260459.831024)
('Worker processing elapsed time: ', 3.9592418670654297, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[689]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 689, ']')


'Epoch [690] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 435,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001876262214377082,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 7, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 690,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 690, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260459.835907)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260493.473085)
('Worker processing elapsed time: ', 33.63717794418335, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[690]', 'EPOCH LOSS:', 0.025020044405063176, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 690, ']')


'Epoch [691] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 505,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009299903098778894,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 7, 2, 7, 7, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 691,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 691, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260493.479053)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260529.465808)
('Worker processing elapsed time: ', 35.98675489425659, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[691]', 'EPOCH LOSS:', 0.062224554858015334, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 691, ']')


'Epoch [692] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 519,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005147661561494469,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 1, 3, 2, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 692,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 692, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260529.471094)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260563.554683)
('Worker processing elapsed time: ', 34.08358907699585, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[692]', 'EPOCH LOSS:', 0.026235878102851145, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 692, ']')


'Epoch [693] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 231,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007947448085179399,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 3, 1, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 693,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 693, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260563.559718)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260598.243779)
('Worker processing elapsed time: ', 34.68406105041504, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[693]', 'EPOCH LOSS:', 0.024821232928931233, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 693, ']')


'Epoch [694] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 487,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00029585880184577877,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 694,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 694, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260598.249431)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260631.860738)
('Worker processing elapsed time: ', 33.61130714416504, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[694]', 'EPOCH LOSS:', 72.719298855408354, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 694, ']')


'Epoch [695] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 589,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003672722817346743,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 7, 7, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 695,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 695, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260631.865935)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260665.627778)
('Worker processing elapsed time: ', 33.76184296607971, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[695]', 'EPOCH LOSS:', 0.026156503813845173, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 695, ']')


'Epoch [696] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 466,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009858247639245876,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 7, 8, 2, 2, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 696,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 696, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260665.633416)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260701.418347)
('Worker processing elapsed time: ', 35.78493094444275, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[696]', 'EPOCH LOSS:', 0.96292248338110953, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 696, ']')


'Epoch [697] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 590,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007533305901719547,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 8, 2, 2, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 697,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 697, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260701.423877)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260735.196509)
('Worker processing elapsed time: ', 33.772631883621216, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[697]', 'EPOCH LOSS:', 0.026090841118588289, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 697, ']')


'Epoch [698] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 615,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009038536945042966,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 698,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 698, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260735.201665)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260768.820207)
('Worker processing elapsed time: ', 33.618542194366455, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[698]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 698, ']')


'Epoch [699] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 248,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00048103548485403116,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 699,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 699, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260768.826309)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260802.3985)
('Worker processing elapsed time: ', 33.57219099998474, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[699]', 'EPOCH LOSS:', 0.0025322380302325608, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 699, ']')


'Epoch [700] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 495,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00026354369537243637,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 7, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 700,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 700, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260802.403657)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260837.070973)
('Worker processing elapsed time: ', 34.66731595993042, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[700]', 'EPOCH LOSS:', 0.024922723281211884, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 700, ']')


'Epoch [701] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 562,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00017929614390117745,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 701,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 701, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260837.075925)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260870.096149)
('Worker processing elapsed time: ', 33.02022385597229, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[701]', 'EPOCH LOSS:', 0.016940862066951886, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 701, ']')


'Epoch [702] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 321,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001832120680206221,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 7, 2, 5, 4, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 702,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 702, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260870.101787)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260904.518872)
('Worker processing elapsed time: ', 34.41708493232727, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[702]', 'EPOCH LOSS:', 0.13848384650495069, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 702, ']')


'Epoch [703] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 371,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006648237875174738,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 703,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 703, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260904.524244)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260938.526067)
('Worker processing elapsed time: ', 34.00182294845581, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[703]', 'EPOCH LOSS:', 0.51355603790260962, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 703, ']')


'Epoch [704] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 480,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003462141864523715,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 7, 1, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 704,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 704, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260938.531056)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494260972.594995)
('Worker processing elapsed time: ', 34.06393909454346, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[704]', 'EPOCH LOSS:', 11.323290692815526, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 704, ']')


'Epoch [705] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 531,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003366231324944532,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 705,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 705, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494260972.600147)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261006.716523)
('Worker processing elapsed time: ', 34.11637592315674, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[705]', 'EPOCH LOSS:', 0.026791224501063741, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 705, ']')


'Epoch [706] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 550,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009089919402989547,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 4, 7, 7, 5, 5, 2, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 706,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 706, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261006.721787)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261040.858137)
('Worker processing elapsed time: ', 34.13634991645813, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[706]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 706, ']')


'Epoch [707] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 346,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00023295797940528615,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 4, 5, 8, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 707,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 707, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261040.863647)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261074.755375)
('Worker processing elapsed time: ', 33.891727924346924, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[707]', 'EPOCH LOSS:', 0.023924167883044262, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 707, ']')


'Epoch [708] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 578,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00023979357997110594,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 4, 5, 8, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 708,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 708, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261074.761056)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261110.819828)
('Worker processing elapsed time: ', 36.05877208709717, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[708]', 'EPOCH LOSS:', 0.022415810428634664, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 708, ']')


'Epoch [709] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 266,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00041250061531247946,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 709,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 709, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261110.825914)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261144.54488)
('Worker processing elapsed time: ', 33.718966007232666, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[709]', 'EPOCH LOSS:', 0.021274796381057622, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 709, ']')


'Epoch [710] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 575,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000209760426247647,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 4, 2, 8, 3, 1, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 710,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 710, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261144.5503)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261180.27547)
('Worker processing elapsed time: ', 35.72517013549805, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[710]', 'EPOCH LOSS:', 0.01834567686226948, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 710, ']')


'Epoch [711] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 262,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009967749294707722,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 711,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 711, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261180.281077)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261213.76582)
('Worker processing elapsed time: ', 33.48474311828613, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[711]', 'EPOCH LOSS:', 0.04710757451842159, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 711, ']')


'Epoch [712] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 468,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009291673844029505,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 712,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 712, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261213.77076)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261247.132719)
('Worker processing elapsed time: ', 33.3619589805603, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[712]', 'EPOCH LOSS:', 0.023699392406147973, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 712, ']')


'Epoch [713] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 408,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000751936982807501,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 713,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 713, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261247.137988)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261280.612588)
('Worker processing elapsed time: ', 33.474599838256836, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[713]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 713, ']')


'Epoch [714] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 236,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00034587392849073353,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 3, 3, 3, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 714,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 714, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261280.618703)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261316.962098)
('Worker processing elapsed time: ', 36.34339499473572, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[714]', 'EPOCH LOSS:', 0.027193150481715566, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 714, ']')


'Epoch [715] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 469,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00043685469405692606,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 715,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 715, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261316.967022)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261350.593912)
('Worker processing elapsed time: ', 33.62688994407654, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[715]', 'EPOCH LOSS:', 5.0921603259270452, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 715, ']')


'Epoch [716] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 461,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003113320300780197,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 716,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 716, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261350.599191)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261384.003999)
('Worker processing elapsed time: ', 33.404808044433594, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[716]', 'EPOCH LOSS:', 0.25459385992284822, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 716, ']')


'Epoch [717] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 590,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004676769479589896,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 717,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 717, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261384.009985)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261418.323959)
('Worker processing elapsed time: ', 34.313974142074585, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[717]', 'EPOCH LOSS:', 0.026338465729706027, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 717, ']')


'Epoch [718] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 316,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005673864563061502,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 6, 6, 8, 6, 5, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 718,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 718, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261418.329991)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261453.473701)
('Worker processing elapsed time: ', 35.143709897994995, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[718]', 'EPOCH LOSS:', 13.856175762299829, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 718, ']')


'Epoch [719] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 396,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00010239705819293534,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 719,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 719, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261453.47871)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261486.751475)
('Worker processing elapsed time: ', 33.272765159606934, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[719]', 'EPOCH LOSS:', 0.43051221084815111, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 719, ']')


'Epoch [720] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 493,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007068671672532925,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 4, 8, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 720,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 720, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261486.757184)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261520.85516)
('Worker processing elapsed time: ', 34.097975969314575, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[720]', 'EPOCH LOSS:', 0.29561090864728223, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 720, ']')


'Epoch [721] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 492,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00018204108892383116,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 721,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 721, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261520.860996)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261554.36448)
('Worker processing elapsed time: ', 33.50348401069641, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[721]', 'EPOCH LOSS:', 38.903135600300075, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 721, ']')


'Epoch [722] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 503,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005733439793940675,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 1, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 722,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 722, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261554.36997)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261588.452087)
('Worker processing elapsed time: ', 34.0821168422699, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[722]', 'EPOCH LOSS:', 4.6701390412480697, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 722, ']')


'Epoch [723] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 499,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0001798298135241601,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 2, 1, 5, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 723,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 723, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261588.457988)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261624.378333)
('Worker processing elapsed time: ', 35.920345067977905, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[723]', 'EPOCH LOSS:', 0.022923896737199551, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 723, ']')


'Epoch [724] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 391,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008002285364857773,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 8, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 724,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 724, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261624.383841)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261658.764106)
('Worker processing elapsed time: ', 34.3802649974823, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[724]', 'EPOCH LOSS:', 0.024452799770209893, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 724, ']')


'Epoch [725] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 370,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008706752652264439,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 725,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 725, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261658.769643)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261692.247235)
('Worker processing elapsed time: ', 33.47759199142456, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[725]', 'EPOCH LOSS:', 0.0142791217631154, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 725, ']')


'Epoch [726] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 305,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006828051111444355,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 726,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 726, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261692.252122)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261725.673888)
('Worker processing elapsed time: ', 33.42176604270935, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[726]', 'EPOCH LOSS:', 0.023600218109357606, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 726, ']')


'Epoch [727] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 283,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005994225314349509,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 727,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 727, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261725.678822)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261758.830094)
('Worker processing elapsed time: ', 33.15127205848694, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[727]', 'EPOCH LOSS:', 0.023393581288383865, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 727, ']')


'Epoch [728] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 299,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00012344749877898366,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 3, 8, 5, 5, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 728,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 728, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261758.835187)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261792.793332)
('Worker processing elapsed time: ', 33.95814514160156, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[728]', 'EPOCH LOSS:', 0.022694617232643502, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 728, ']')


'Epoch [729] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 240,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007461486343044099,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 6, 1, 3, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 729,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 729, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261792.799394)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261826.790032)
('Worker processing elapsed time: ', 33.99063801765442, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[729]', 'EPOCH LOSS:', 0.023476299429434612, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 729, ']')


'Epoch [730] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 561,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00015983863059591764,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 730,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 730, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261826.794894)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261860.240891)
('Worker processing elapsed time: ', 33.4459969997406, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[730]', 'EPOCH LOSS:', 0.080723957729430237, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 730, ']')


'Epoch [731] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 346,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009279595888069382,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 4, 6, 5, 1, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 731,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 731, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261860.246534)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261898.102551)
('Worker processing elapsed time: ', 37.856016874313354, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[731]', 'EPOCH LOSS:', 0.021704615591212947, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 731, ']')


'Epoch [732] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 448,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005268827521845805,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 6, 5, 1, 4, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 732,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 732, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261898.109323)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261935.042856)
('Worker processing elapsed time: ', 36.93353295326233, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[732]', 'EPOCH LOSS:', 0.018754646907628326, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 732, ']')


'Epoch [733] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 568,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0006397727164152627,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 733,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 733, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261935.047735)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494261968.66732)
('Worker processing elapsed time: ', 33.619585037231445, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[733]', 'EPOCH LOSS:', 0.6119553151054552, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 733, ']')


'Epoch [734] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 555,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009571854200701599,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 7, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 734,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 734, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494261968.672459)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262002.282137)
('Worker processing elapsed time: ', 33.60967803001404, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[734]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 734, ']')


'Epoch [735] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 469,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0003591752618008103,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 735,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 735, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262002.287568)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262035.618294)
('Worker processing elapsed time: ', 33.33072590827942, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[735]', 'EPOCH LOSS:', 0.022387663417813893, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 735, ']')


'Epoch [736] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 293,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004273500365080549,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 7, 7, 1, 2, 1, 7, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 736,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 736, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262035.623925)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262069.911782)
('Worker processing elapsed time: ', 34.28785705566406, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[736]', 'EPOCH LOSS:', 0.022916984693191304, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 736, ']')


'Epoch [737] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 596,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000869449208396586,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 5, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 737,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 737, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262069.917584)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262103.711206)
('Worker processing elapsed time: ', 33.79362201690674, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[737]', 'EPOCH LOSS:', 0.094793408357405073, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 737, ']')


'Epoch [738] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 577,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009085513005397266,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 8, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 738,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 738, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262103.716235)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262137.221108)
('Worker processing elapsed time: ', 33.50487303733826, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[738]', 'EPOCH LOSS:', 15.459063829931862, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 738, ']')


'Epoch [739] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 364,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0005260719407818681,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 7, 3, 5, 6, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 739,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 739, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262137.226387)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262172.725087)
('Worker processing elapsed time: ', 35.49869990348816, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[739]', 'EPOCH LOSS:', 0.028284450870573425, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 739, ']')


'Epoch [740] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 335,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000354026686338158,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 740,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 740, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262172.730253)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262205.963198)
('Worker processing elapsed time: ', 33.23294496536255, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[740]', 'EPOCH LOSS:', 0.011045857175008379, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 740, ']')


'Epoch [741] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 409,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009365674143005764,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 741,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 741, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262205.967904)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262238.961235)
('Worker processing elapsed time: ', 32.99333095550537, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[741]', 'EPOCH LOSS:', 0.024111931487759745, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 741, ']')


'Epoch [742] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 233,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00010475363192951053,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 7, 3, 3, 7, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 742,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 742, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262238.96638)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262274.052638)
('Worker processing elapsed time: ', 35.08625817298889, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[742]', 'EPOCH LOSS:', 0.026129405549784563, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 742, ']')


'Epoch [743] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 560,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0004598140612625736,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 6, 8, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 743,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 743, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262274.058181)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262308.75928)
('Worker processing elapsed time: ', 34.701098918914795, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[743]', 'EPOCH LOSS:', 0.022298785969797643, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 743, ']')


'Epoch [744] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 372,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0009716973432225011,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 744,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 744, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262308.765318)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262342.511186)
('Worker processing elapsed time: ', 33.74586796760559, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[744]', 'EPOCH LOSS:', 0.010994381143655889, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 744, ']')


'Epoch [745] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 405,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007763446366183824,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 6, 3, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 745,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 745, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262342.51645)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262376.415845)
('Worker processing elapsed time: ', 33.89939498901367, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[745]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 745, ']')


'Epoch [746] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 293,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.00022699232777872427,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 6, 6, 1, 8, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 746,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 746, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262376.421153)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262411.16119)
('Worker processing elapsed time: ', 34.740036964416504, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[746]', 'EPOCH LOSS:', 16.360394693766789, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 746, ']')


'Epoch [747] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 442,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0008247092357458711,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 4, 5, 4, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 747,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 747, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262411.166412)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262445.735846)
('Worker processing elapsed time: ', 34.56943392753601, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[747]', 'EPOCH LOSS:', 0.3356980920592792, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 747, ']')


'Epoch [748] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 477,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000827064170807121,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 2, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 748,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 748, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262445.740997)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262480.454406)
('Worker processing elapsed time: ', 34.71340894699097, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[748]', 'EPOCH LOSS:', 0.02412714105533225, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 748, ']')


'Epoch [749] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 338,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.000316262782076407,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 749,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 749, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262480.459372)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262514.020372)
('Worker processing elapsed time: ', 33.56099987030029, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[749]', 'EPOCH LOSS:', 0.023379877298665162, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 749, ']')


'Epoch [750] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 545,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 9],
 'input_dim': 4,
 'learning_rate': 0.0007846272434400199,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 8, 5, 6, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 750,
 'opt_epoch_iter': 750,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 2,
 'sync_replicas': False,
 'train_epoch': 5000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 750, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262514.026149)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262547.706087)
('Worker processing elapsed time: ', 33.679938077926636, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[750]', 'EPOCH LOSS:', 0.022993055331907781, 'BEST LOSS:', 2.5544787520616208e-05)
('END OF Optimizer EPOCH =====================>>[', 750, ']')


END OF STAGED EPOCH #######################>>[ 2 ]
START OF STAGED EPOCH #######################>> [ 3 ]
'Epoch [1] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 403,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007266053474594405,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 1,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 1, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262547.711793)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262584.705056)
('Worker processing elapsed time: ', 36.99326300621033, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[1]', 'EPOCH LOSS:', 0.37717358727630396, 'BEST LOSS:', 0.37717358727630396)
('END OF Optimizer EPOCH =====================>>[', 1, ']')


'Epoch [2] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 257,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00029768369722364067,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 2,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 2, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262584.710521)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262628.738008)
('Worker processing elapsed time: ', 44.02748703956604, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[2]', 'EPOCH LOSS:', 0.021143276841268391, 'BEST LOSS:', 0.021143276841268391)
('END OF Optimizer EPOCH =====================>>[', 2, ']')


'Epoch [3] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 513,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00032252687759940293,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 3,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 3, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262628.743347)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262666.044253)
('Worker processing elapsed time: ', 37.30090618133545, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[3]', 'EPOCH LOSS:', 0.27591269658017198, 'BEST LOSS:', 0.021143276841268391)
('END OF Optimizer EPOCH =====================>>[', 3, ']')


'Epoch [4] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 299,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000355906038970971,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 3, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 4,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 4, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262666.049244)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262708.158617)
('Worker processing elapsed time: ', 42.10937309265137, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[4]', 'EPOCH LOSS:', 2.1459827873739412, 'BEST LOSS:', 0.021143276841268391)
('END OF Optimizer EPOCH =====================>>[', 4, ']')


'Epoch [5] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 381,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005472285061991312,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 5,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 5, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262708.163944)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262750.572573)
('Worker processing elapsed time: ', 42.408628940582275, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[5]', 'EPOCH LOSS:', 0.00043354534277560849, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 5, ']')


'Epoch [6] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 621,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005048624302955147,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 6,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 6, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262750.578575)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262789.952399)
('Worker processing elapsed time: ', 39.37382411956787, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[6]', 'EPOCH LOSS:', 0.020310319128294889, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 6, ']')


'Epoch [7] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 382,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003679304347490028,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 7,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 7, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262789.957478)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262827.626083)
('Worker processing elapsed time: ', 37.66860485076904, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[7]', 'EPOCH LOSS:', 0.0093404973458064144, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 7, ']')


'Epoch [8] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 431,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00047878652484365403,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 8,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 8, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262827.631792)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262872.425068)
('Worker processing elapsed time: ', 44.79327583312988, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[8]', 'EPOCH LOSS:', 0.001388569169329555, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 8, ']')


'Epoch [9] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 458,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00028831454707022627,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 1, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 9,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 9, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262872.430114)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262914.73965)
('Worker processing elapsed time: ', 42.30953598022461, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[9]', 'EPOCH LOSS:', 0.014554423611708838, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 9, ']')


'Epoch [10] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 378,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006435375619079322,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 10,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 10, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262914.745285)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494262954.789624)
('Worker processing elapsed time: ', 40.0443389415741, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[10]', 'EPOCH LOSS:', 0.016400266252764646, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 10, ']')


'Epoch [11] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 298,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00018422942992087164,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 11,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 11, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494262954.794889)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263001.256928)
('Worker processing elapsed time: ', 46.46203899383545, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[11]', 'EPOCH LOSS:', 0.0090398750314043593, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 11, ']')


'Epoch [12] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 433,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009199740105763564,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 12,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 12, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263001.26237)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263036.259214)
('Worker processing elapsed time: ', 34.99684381484985, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[12]', 'EPOCH LOSS:', 0.31436078261408495, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 12, ']')


'Epoch [13] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 373,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00023289443309623768,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 13,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 13, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263036.264318)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263076.571479)
('Worker processing elapsed time: ', 40.30716109275818, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[13]', 'EPOCH LOSS:', 0.021933795263688032, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 13, ']')


'Epoch [14] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 471,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000220240864430297,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 3, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 14,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 14, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263076.577697)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263120.024028)
('Worker processing elapsed time: ', 43.44633102416992, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[14]', 'EPOCH LOSS:', 3.0724680500035371, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 14, ']')


'Epoch [15] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 269,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00032702099149286906,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 15,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 15, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263120.02994)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263154.51501)
('Worker processing elapsed time: ', 34.48507022857666, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[15]', 'EPOCH LOSS:', 0.013123776777849434, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 15, ']')


'Epoch [16] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 427,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007114841240223861,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 16,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 16, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263154.520971)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263191.660038)
('Worker processing elapsed time: ', 37.13906693458557, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[16]', 'EPOCH LOSS:', 0.92737663099798107, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 16, ']')


'Epoch [17] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 463,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00013543600178811985,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 17,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 17, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263191.665389)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263229.492692)
('Worker processing elapsed time: ', 37.82730293273926, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[17]', 'EPOCH LOSS:', 0.011658744880309248, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 17, ']')


'Epoch [18] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 533,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00026425465144832726,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 1, 1, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 18,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 18, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263229.497592)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263278.910287)
('Worker processing elapsed time: ', 49.41269493103027, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[18]', 'EPOCH LOSS:', 0.024642082135347822, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 18, ']')


'Epoch [19] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 446,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006971362463646462,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 19,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 19, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263278.915633)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263315.727247)
('Worker processing elapsed time: ', 36.81161403656006, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[19]', 'EPOCH LOSS:', 0.022540823751353505, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 19, ']')


'Epoch [20] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 513,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000755874651591658,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 20,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 20, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263315.732361)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263355.456414)
('Worker processing elapsed time: ', 39.72405290603638, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[20]', 'EPOCH LOSS:', 0.66355623819365195, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 20, ']')


'Epoch [21] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 354,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005053724012475339,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 4, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 21,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 21, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263355.462512)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263405.035874)
('Worker processing elapsed time: ', 49.57336187362671, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[21]', 'EPOCH LOSS:', 0.0018511923235140337, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 21, ']')


'Epoch [22] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 485,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003901952109544764,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 22,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 22, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263405.041521)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263445.942752)
('Worker processing elapsed time: ', 40.901230812072754, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[22]', 'EPOCH LOSS:', 0.59195522021674829, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 22, ']')


'Epoch [23] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 361,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00015179229319677555,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 1, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 23,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 23, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263445.947817)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263487.341022)
('Worker processing elapsed time: ', 41.39320492744446, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[23]', 'EPOCH LOSS:', 0.018722628051176016, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 23, ']')


'Epoch [24] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 502,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006572643483846506,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 24,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 24, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263487.346751)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263526.123857)
('Worker processing elapsed time: ', 38.777106046676636, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[24]', 'EPOCH LOSS:', 0.023076032724156696, 'BEST LOSS:', 0.00043354534277560849)
('END OF Optimizer EPOCH =====================>>[', 24, ']')


'Epoch [25] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 478,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008101800475723625,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 6, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 25,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 25, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263526.129985)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263573.592763)
('Worker processing elapsed time: ', 47.462777853012085, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[25]', 'EPOCH LOSS:', 0.00041036488118147298, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 25, ']')


'Epoch [26] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 548,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00017119927055691978,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 26,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 26, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263573.598269)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263611.025396)
('Worker processing elapsed time: ', 37.42712712287903, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[26]', 'EPOCH LOSS:', 0.0021447822266814766, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 26, ']')


'Epoch [27] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 333,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007478338205946273,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 27,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 27, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263611.031033)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263651.088142)
('Worker processing elapsed time: ', 40.057108879089355, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[27]', 'EPOCH LOSS:', 0.00097925277384610645, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 27, ']')


'Epoch [28] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 557,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009940597266151915,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 28,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 28, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263651.094044)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263684.332088)
('Worker processing elapsed time: ', 33.238044023513794, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[28]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 28, ']')


'Epoch [29] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 506,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007029489922333902,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 29,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 29, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263684.337929)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263725.54433)
('Worker processing elapsed time: ', 41.206400871276855, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[29]', 'EPOCH LOSS:', 0.026044725357106821, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 29, ']')


'Epoch [30] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 302,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009264457282166419,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 30,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 30, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263725.550311)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263764.401721)
('Worker processing elapsed time: ', 38.851409912109375, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[30]', 'EPOCH LOSS:', 0.24818621738903948, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 30, ']')


'Epoch [31] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 300,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007742114602529962,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 31,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 31, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263764.407321)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263808.855698)
('Worker processing elapsed time: ', 44.44837713241577, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[31]', 'EPOCH LOSS:', 0.020539169379465219, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 31, ']')


'Epoch [32] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 618,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00039489992328337986,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 5, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 32,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 32, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263808.861168)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263844.662605)
('Worker processing elapsed time: ', 35.80143713951111, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[32]', 'EPOCH LOSS:', nan, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 32, ']')


'Epoch [33] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 461,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007884816604373563,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 33,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 33, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263844.667519)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263885.336266)
('Worker processing elapsed time: ', 40.66874694824219, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[33]', 'EPOCH LOSS:', 0.074705344908965868, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 33, ']')


'Epoch [34] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 618,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00034266476313313135,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 34,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 34, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263885.341371)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263928.452949)
('Worker processing elapsed time: ', 43.1115779876709, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[34]', 'EPOCH LOSS:', 0.010054954798960887, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 34, ']')


'Epoch [35] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 447,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00015499337179783186,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 35,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 35, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263928.45823)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494263969.733318)
('Worker processing elapsed time: ', 41.27508807182312, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[35]', 'EPOCH LOSS:', 4.7818615404202793, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 35, ']')


'Epoch [36] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 543,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00010730154365409155,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 36,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 36, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494263969.738514)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264005.245667)
('Worker processing elapsed time: ', 35.507153034210205, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[36]', 'EPOCH LOSS:', 1.8072206299401881, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 36, ']')


'Epoch [37] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 537,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009003516533467993,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 37,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 37, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264005.250562)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264040.088084)
('Worker processing elapsed time: ', 34.83752202987671, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[37]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 37, ']')


'Epoch [38] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 281,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00037246717590411183,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 38,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 38, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264040.093058)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264082.190963)
('Worker processing elapsed time: ', 42.097904920578, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[38]', 'EPOCH LOSS:', 0.002332197624680077, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 38, ']')


'Epoch [39] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 583,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003297319250724779,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 39,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 39, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264082.196242)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264119.439017)
('Worker processing elapsed time: ', 37.242774963378906, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[39]', 'EPOCH LOSS:', 0.0022456360757578276, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 39, ']')


'Epoch [40] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 499,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002426660545668427,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 40,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 40, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264119.444979)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264157.312997)
('Worker processing elapsed time: ', 37.86801815032959, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[40]', 'EPOCH LOSS:', 0.013161224443273504, 'BEST LOSS:', 0.00041036488118147298)
('END OF Optimizer EPOCH =====================>>[', 40, ']')


'Epoch [41] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 375,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009233924655534873,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 2, 2, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 41,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 41, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264157.318357)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264207.108002)
('Worker processing elapsed time: ', 49.789644956588745, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[41]', 'EPOCH LOSS:', 6.3920782384329833e-05, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 41, ']')


'Epoch [42] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 451,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004504416039224834,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 42,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 42, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264207.11352)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264241.960332)
('Worker processing elapsed time: ', 34.8468120098114, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[42]', 'EPOCH LOSS:', 0.0036985366783225376, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 42, ']')


'Epoch [43] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 355,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006383093401118162,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 43,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 43, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264241.965707)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264276.748138)
('Worker processing elapsed time: ', 34.78243088722229, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[43]', 'EPOCH LOSS:', 0.41468488159841316, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 43, ']')


'Epoch [44] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 608,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00047988050785745153,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 2, 3, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 44,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 44, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264276.753856)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264327.371168)
('Worker processing elapsed time: ', 50.61731195449829, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[44]', 'EPOCH LOSS:', 0.00083418726392847555, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 44, ']')


'Epoch [45] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 295,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000824577789916484,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 45,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 45, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264327.376953)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264367.335994)
('Worker processing elapsed time: ', 39.959041118621826, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[45]', 'EPOCH LOSS:', 0.019639510660508942, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 45, ']')


'Epoch [46] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 515,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005683990108141959,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 46,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 46, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264367.341966)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264407.689993)
('Worker processing elapsed time: ', 40.3480269908905, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[46]', 'EPOCH LOSS:', 0.0030949192230582934, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 46, ']')


'Epoch [47] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 493,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00010237719628478146,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 47,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 47, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264407.696038)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264442.96311)
('Worker processing elapsed time: ', 35.26707196235657, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[47]', 'EPOCH LOSS:', 1.7600748966079827, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 47, ']')


'Epoch [48] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 516,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00019853840929638666,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 48,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 48, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264442.968381)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264482.525164)
('Worker processing elapsed time: ', 39.556782960891724, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[48]', 'EPOCH LOSS:', 0.29099505288133626, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 48, ']')


'Epoch [49] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 567,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006925151407171986,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 49,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 49, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264482.530477)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264516.081798)
('Worker processing elapsed time: ', 33.551321029663086, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[49]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 49, ']')


'Epoch [50] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 366,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00027108539192595513,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 2, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 50,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 50, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264516.087892)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264562.59912)
('Worker processing elapsed time: ', 46.51122784614563, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[50]', 'EPOCH LOSS:', 0.0019072296862289149, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 50, ']')


'Epoch [51] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 481,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005556788560215165,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 51,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 51, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264562.604881)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264600.483443)
('Worker processing elapsed time: ', 37.87856197357178, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[51]', 'EPOCH LOSS:', 0.0024902650435230061, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 51, ']')


'Epoch [52] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 420,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00011733851586915656,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 52,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 52, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264600.489573)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264639.505396)
('Worker processing elapsed time: ', 39.015822887420654, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[52]', 'EPOCH LOSS:', 19.520170373611794, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 52, ']')


'Epoch [53] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 258,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007886300530569814,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 53,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 53, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264639.511519)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264676.59756)
('Worker processing elapsed time: ', 37.08604097366333, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[53]', 'EPOCH LOSS:', nan, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 53, ']')


'Epoch [54] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 463,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008508281100508176,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 5, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 54,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 54, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264676.60293)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264719.065394)
('Worker processing elapsed time: ', 42.46246385574341, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[54]', 'EPOCH LOSS:', 0.020068287338330944, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 54, ']')


'Epoch [55] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 559,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004519646600896815,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 1, 2, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 55,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 55, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264719.070755)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264768.542057)
('Worker processing elapsed time: ', 49.4713020324707, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[55]', 'EPOCH LOSS:', 0.019646561786662128, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 55, ']')


'Epoch [56] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 492,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006415945792255293,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 56,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 56, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264768.547348)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264809.402724)
('Worker processing elapsed time: ', 40.85537600517273, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[56]', 'EPOCH LOSS:', 0.024107995343027964, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 56, ']')


'Epoch [57] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 319,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009175901335520307,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 5, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 57,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 57, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264809.408994)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264851.924952)
('Worker processing elapsed time: ', 42.515958070755005, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[57]', 'EPOCH LOSS:', 0.24798146917027306, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 57, ']')


'Epoch [58] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 262,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007416273821704913,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 58,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 58, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264851.93033)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264885.662973)
('Worker processing elapsed time: ', 33.73264288902283, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[58]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 58, ']')


'Epoch [59] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 590,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009589485377297209,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 59,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 59, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264885.667989)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264923.089054)
('Worker processing elapsed time: ', 37.42106509208679, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[59]', 'EPOCH LOSS:', 0.019845997745994725, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 59, ']')


'Epoch [60] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 421,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00030505074025254954,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 60,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 60, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264923.09434)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494264960.020681)
('Worker processing elapsed time: ', 36.92634081840515, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[60]', 'EPOCH LOSS:', 0.28924619744638475, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 60, ']')


'Epoch [61] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 588,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009395284406778673,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 61,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 61, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494264960.0269)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265003.362371)
('Worker processing elapsed time: ', 43.3354709148407, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[61]', 'EPOCH LOSS:', 0.00061591387249524646, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 61, ']')


'Epoch [62] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 282,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009084566685775546,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 62,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 62, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265003.367862)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265037.839306)
('Worker processing elapsed time: ', 34.47144412994385, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[62]', 'EPOCH LOSS:', 0.002911830949836353, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 62, ']')


'Epoch [63] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 264,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0001834307270423334,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 63,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 63, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265037.844167)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265075.342587)
('Worker processing elapsed time: ', 37.498420000076294, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[63]', 'EPOCH LOSS:', 0.016084801947418806, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 63, ']')


'Epoch [64] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 384,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000757821944081769,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 64,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 64, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265075.34889)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265122.245491)
('Worker processing elapsed time: ', 46.89660096168518, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[64]', 'EPOCH LOSS:', 0.00013597975281157932, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 64, ']')


'Epoch [65] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 518,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00027519009459192525,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 65,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 65, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265122.251286)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265165.727484)
('Worker processing elapsed time: ', 43.476197957992554, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[65]', 'EPOCH LOSS:', 0.013972895302078363, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 65, ']')


'Epoch [66] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 477,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004203422069582819,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 66,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 66, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265165.733239)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265200.765896)
('Worker processing elapsed time: ', 35.03265714645386, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[66]', 'EPOCH LOSS:', 0.6575137102680475, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 66, ']')


'Epoch [67] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 490,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002904338551792306,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 67,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 67, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265200.771695)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265240.655678)
('Worker processing elapsed time: ', 39.88398313522339, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[67]', 'EPOCH LOSS:', 3.9338926338983797, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 67, ']')


'Epoch [68] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 540,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004391775176681653,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 68,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 68, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265240.661154)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265278.213441)
('Worker processing elapsed time: ', 37.552286863327026, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[68]', 'EPOCH LOSS:', 0.00082557401145074132, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 68, ']')


'Epoch [69] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 587,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00013066616921021267,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 69,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 69, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265278.219543)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265319.982153)
('Worker processing elapsed time: ', 41.76260995864868, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[69]', 'EPOCH LOSS:', 1.5334543459284871, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 69, ']')


'Epoch [70] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 306,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00041771423555416094,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 70,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 70, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265319.988276)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265359.933779)
('Worker processing elapsed time: ', 39.9455029964447, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[70]', 'EPOCH LOSS:', 0.015127080167583497, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 70, ']')


'Epoch [71] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 492,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005245744610705809,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 1, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 71,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 71, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265359.939242)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265403.397306)
('Worker processing elapsed time: ', 43.45806407928467, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[71]', 'EPOCH LOSS:', 2.8658068565140193, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 71, ']')


'Epoch [72] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 256,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008706167627219081,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 6, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 72,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 72, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265403.402613)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265445.237637)
('Worker processing elapsed time: ', 41.83502411842346, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[72]', 'EPOCH LOSS:', 0.051537670267952966, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 72, ']')


'Epoch [73] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 360,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0001372445371120389,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 73,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 73, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265445.243608)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265485.148415)
('Worker processing elapsed time: ', 39.90480709075928, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[73]', 'EPOCH LOSS:', 0.019348590080746193, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 73, ']')


'Epoch [74] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 617,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007968897924373166,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 74,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 74, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265485.154363)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265533.677377)
('Worker processing elapsed time: ', 48.523014068603516, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[74]', 'EPOCH LOSS:', 0.045022610648436545, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 74, ']')


'Epoch [75] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 530,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007450397745050026,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 75,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 75, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265533.682594)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265570.997268)
('Worker processing elapsed time: ', 37.31467390060425, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[75]', 'EPOCH LOSS:', 0.59908039693711657, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 75, ']')


'Epoch [76] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 271,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00021340677078606493,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 76,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 76, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265571.003246)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265607.269902)
('Worker processing elapsed time: ', 36.266655921936035, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[76]', 'EPOCH LOSS:', 0.027241122008493303, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 76, ']')


'Epoch [77] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 381,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00011229496559668462,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 77,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 77, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265607.275955)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265647.881402)
('Worker processing elapsed time: ', 40.6054470539093, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[77]', 'EPOCH LOSS:', 86.854450409795945, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 77, ']')


'Epoch [78] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 439,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007175233338716596,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 3, 5, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 78,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 78, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265647.887558)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265697.146613)
('Worker processing elapsed time: ', 49.2590548992157, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[78]', 'EPOCH LOSS:', 0.001352258134715715, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 78, ']')


'Epoch [79] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 593,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003632399269836602,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 79,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 79, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265697.15186)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265734.679466)
('Worker processing elapsed time: ', 37.52760601043701, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[79]', 'EPOCH LOSS:', 2.144619701084006, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 79, ']')


'Epoch [80] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 296,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008188450952362017,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 80,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 80, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265734.68461)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265772.915138)
('Worker processing elapsed time: ', 38.230528116226196, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[80]', 'EPOCH LOSS:', 0.024110324188018659, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 80, ']')


'Epoch [81] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 440,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00030963489276831764,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 81,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 81, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265772.921116)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265811.504694)
('Worker processing elapsed time: ', 38.58357787132263, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[81]', 'EPOCH LOSS:', 0.021949735142819889, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 81, ']')


'Epoch [82] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 264,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000495855496671933,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 2, 6, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 82,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 82, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265811.509947)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265859.649749)
('Worker processing elapsed time: ', 48.13980197906494, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[82]', 'EPOCH LOSS:', 0.020950139427057757, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 82, ']')


'Epoch [83] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 544,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006619091314244321,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 83,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 83, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265859.654958)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265897.507087)
('Worker processing elapsed time: ', 37.852128982543945, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[83]', 'EPOCH LOSS:', 0.010283366815705224, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 83, ']')


'Epoch [84] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 444,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000993267639839075,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 3, 3, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 84,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 84, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265897.512617)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265946.648747)
('Worker processing elapsed time: ', 49.13612985610962, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[84]', 'EPOCH LOSS:', 0.021342307186697655, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 84, ']')


'Epoch [85] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 446,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006766090016720465,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 4, 6, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 85,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 85, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265946.654206)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494265996.264632)
('Worker processing elapsed time: ', 49.61042594909668, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[85]', 'EPOCH LOSS:', 0.001163658261903282, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 85, ']')


'Epoch [86] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 384,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005154674644249901,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 86,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 86, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494265996.27003)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266040.500664)
('Worker processing elapsed time: ', 44.23063397407532, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[86]', 'EPOCH LOSS:', 0.021767402212421471, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 86, ']')


'Epoch [87] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 611,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00040330010593817706,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 87,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 87, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266040.505744)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266079.881622)
('Worker processing elapsed time: ', 39.37587809562683, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[87]', 'EPOCH LOSS:', 0.017488476872910542, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 87, ']')


'Epoch [88] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 592,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009070539348606403,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 88,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 88, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266079.886598)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266113.377129)
('Worker processing elapsed time: ', 33.4905309677124, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[88]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 88, ']')


'Epoch [89] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 462,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004206080199004715,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 5, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 89,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 89, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266113.382098)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266160.547801)
('Worker processing elapsed time: ', 47.1657030582428, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[89]', 'EPOCH LOSS:', 0.0048048281789019893, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 89, ']')


'Epoch [90] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 566,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002474012205936094,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 90,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 90, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266160.55317)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266200.921412)
('Worker processing elapsed time: ', 40.368242025375366, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[90]', 'EPOCH LOSS:', 0.011413067323710124, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 90, ']')


'Epoch [91] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 525,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007785705747242743,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 6, 5, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 91,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 91, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266200.9278)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266234.764567)
('Worker processing elapsed time: ', 33.836766958236694, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[91]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 91, ']')


'Epoch [92] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 578,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007154976072814307,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 92,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 92, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266234.770021)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266279.355766)
('Worker processing elapsed time: ', 44.585745096206665, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[92]', 'EPOCH LOSS:', 0.0105946347068762, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 92, ']')


'Epoch [93] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 325,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00024291156250884815,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 5, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 93,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 93, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266279.361241)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266321.128768)
('Worker processing elapsed time: ', 41.76752686500549, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[93]', 'EPOCH LOSS:', 0.021581252495075118, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 93, ']')


'Epoch [94] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 296,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006625528692186695,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 94,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 94, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266321.134713)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266361.873338)
('Worker processing elapsed time: ', 40.738625049591064, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[94]', 'EPOCH LOSS:', 0.102692295417622, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 94, ']')


'Epoch [95] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 287,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009015534995901963,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 95,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 95, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266361.878691)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266395.669958)
('Worker processing elapsed time: ', 33.79126715660095, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[95]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 95, ']')


'Epoch [96] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 449,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000957222839131371,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 1, 4, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 96,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 96, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266395.675881)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266444.801601)
('Worker processing elapsed time: ', 49.12572002410889, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[96]', 'EPOCH LOSS:', 0.00099499292133171371, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 96, ']')


'Epoch [97] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 294,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005600737349397494,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 6, 4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 97,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 97, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266444.807139)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266486.786344)
('Worker processing elapsed time: ', 41.97920513153076, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[97]', 'EPOCH LOSS:', 7.0979632199488618, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 97, ']')


'Epoch [98] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 400,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00037346172529027007,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 98,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 98, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266486.79142)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266521.685999)
('Worker processing elapsed time: ', 34.89457893371582, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[98]', 'EPOCH LOSS:', 0.4599963654152584, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 98, ']')


'Epoch [99] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 300,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008920617315424334,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 99,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 99, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266521.691185)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266556.04483)
('Worker processing elapsed time: ', 34.35364508628845, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[99]', 'EPOCH LOSS:', 0.014096995334528533, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 99, ']')


'Epoch [100] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 282,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009017920851557959,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 1, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 100,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 100, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266556.050507)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266597.912262)
('Worker processing elapsed time: ', 41.86175489425659, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[100]', 'EPOCH LOSS:', 2.247588831174983, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 100, ']')


'Epoch [101] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 259,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009276583044756523,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 4, 5, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 101,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 101, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266597.91773)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266631.744325)
('Worker processing elapsed time: ', 33.826594829559326, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[101]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 101, ']')


'Epoch [102] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 483,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008191589450784385,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 4, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 102,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 102, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266631.750175)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266665.522925)
('Worker processing elapsed time: ', 33.77274990081787, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[102]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 102, ']')


'Epoch [103] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 489,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00038318019056023773,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 103,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 103, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266665.527943)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266710.570562)
('Worker processing elapsed time: ', 45.04261898994446, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[103]', 'EPOCH LOSS:', 0.0015086305680550335, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 103, ']')


'Epoch [104] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 347,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008712326890697392,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 6, 4, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 104,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 104, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266710.576064)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266755.091145)
('Worker processing elapsed time: ', 44.51508092880249, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[104]', 'EPOCH LOSS:', 0.028051366340322733, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 104, ']')


'Epoch [105] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 390,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008106689587706817,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 105,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 105, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266755.09639)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266795.902387)
('Worker processing elapsed time: ', 40.805996894836426, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[105]', 'EPOCH LOSS:', 3.6571011486938234, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 105, ']')


'Epoch [106] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 529,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004508091515567171,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 106,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 106, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266795.908033)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266834.903981)
('Worker processing elapsed time: ', 38.99594807624817, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[106]', 'EPOCH LOSS:', 0.023039994117030959, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 106, ']')


'Epoch [107] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 504,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008338982982466873,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 4, 4, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 107,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 107, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266834.910336)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266879.938313)
('Worker processing elapsed time: ', 45.027976989746094, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[107]', 'EPOCH LOSS:', 0.019018641002683891, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 107, ']')


'Epoch [108] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 603,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003878331044880306,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 108,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 108, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266879.944324)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266917.588703)
('Worker processing elapsed time: ', 37.644378900527954, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[108]', 'EPOCH LOSS:', 0.42997285018961506, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 108, ']')


'Epoch [109] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 582,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006006077328040104,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 109,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 109, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266917.594516)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266956.560186)
('Worker processing elapsed time: ', 38.96566987037659, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[109]', 'EPOCH LOSS:', 0.020685278958340172, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 109, ']')


'Epoch [110] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 490,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008586412397512586,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 3, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 110,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 110, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266956.565204)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494266999.607205)
('Worker processing elapsed time: ', 43.04200100898743, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[110]', 'EPOCH LOSS:', 0.024073022415073388, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 110, ']')


'Epoch [111] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 430,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002815222432678853,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 111,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 111, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494266999.61282)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267036.418886)
('Worker processing elapsed time: ', 36.806066036224365, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[111]', 'EPOCH LOSS:', 0.017145954308745, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 111, ']')


'Epoch [112] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 377,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007839918474703817,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 112,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 112, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267036.42424)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267074.160926)
('Worker processing elapsed time: ', 37.73668599128723, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[112]', 'EPOCH LOSS:', 0.0097709014802606145, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 112, ']')


'Epoch [113] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 573,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009290315173659303,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 113,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 113, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267074.166983)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267113.349804)
('Worker processing elapsed time: ', 39.18282103538513, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[113]', 'EPOCH LOSS:', 0.020492695565908731, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 113, ']')


'Epoch [114] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 337,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007329536629056162,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 1, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 114,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 114, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267113.35501)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267155.549379)
('Worker processing elapsed time: ', 42.194369077682495, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[114]', 'EPOCH LOSS:', 3.5927322965493294, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 114, ']')


'Epoch [115] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 314,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000958299869490019,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 6, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 115,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 115, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267155.554817)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267202.103483)
('Worker processing elapsed time: ', 46.54866600036621, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[115]', 'EPOCH LOSS:', 0.018951945301908593, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 115, ']')


'Epoch [116] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 423,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005513001008482939,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 4, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 116,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 116, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267202.108961)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267248.882266)
('Worker processing elapsed time: ', 46.77330493927002, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[116]', 'EPOCH LOSS:', 0.013577532310244063, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 116, ']')


'Epoch [117] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 598,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007518848809841079,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 2, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 117,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 117, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267248.888233)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267292.829064)
('Worker processing elapsed time: ', 43.94083094596863, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[117]', 'EPOCH LOSS:', 0.14121934900485089, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 117, ']')


'Epoch [118] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 456,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000790106327873482,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 118,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 118, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267292.834043)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267332.900495)
('Worker processing elapsed time: ', 40.06645202636719, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[118]', 'EPOCH LOSS:', 0.016410646808906762, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 118, ']')


'Epoch [119] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 254,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00029752385945531286,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 119,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 119, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267332.905857)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267372.746256)
('Worker processing elapsed time: ', 39.84039902687073, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[119]', 'EPOCH LOSS:', 0.022066262180724359, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 119, ']')


'Epoch [120] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 307,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007596133569028275,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 120,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 120, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267372.751425)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267410.355454)
('Worker processing elapsed time: ', 37.604028940200806, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[120]', 'EPOCH LOSS:', 0.0086973025788281248, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 120, ']')


'Epoch [121] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 320,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008596887865462605,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 121,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 121, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267410.360481)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267450.427448)
('Worker processing elapsed time: ', 40.06696701049805, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[121]', 'EPOCH LOSS:', 0.019128787393954655, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 121, ']')


'Epoch [122] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 498,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009116056584005373,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 122,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 122, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267450.43308)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267489.738975)
('Worker processing elapsed time: ', 39.30589509010315, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[122]', 'EPOCH LOSS:', 0.17197649841008381, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 122, ']')


'Epoch [123] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 377,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009157843897523089,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 123,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 123, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267489.744481)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267524.434185)
('Worker processing elapsed time: ', 34.689703941345215, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[123]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 123, ']')


'Epoch [124] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 495,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006843080099890798,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 124,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 124, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267524.439206)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267557.890519)
('Worker processing elapsed time: ', 33.45131301879883, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[124]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 124, ']')


'Epoch [125] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 304,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002474464633426038,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 125,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 125, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267557.896409)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267592.430467)
('Worker processing elapsed time: ', 34.53405785560608, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[125]', 'EPOCH LOSS:', 0.84032561234031034, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 125, ']')


'Epoch [126] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 390,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009378434761889733,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 126,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 126, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267592.435525)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267625.957544)
('Worker processing elapsed time: ', 33.522019147872925, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[126]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 126, ']')


'Epoch [127] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 408,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006032442926156207,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 127,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 127, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267625.963337)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267664.899192)
('Worker processing elapsed time: ', 38.93585515022278, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[127]', 'EPOCH LOSS:', 5.6295340730408752, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 127, ']')


'Epoch [128] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 318,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005225351980300071,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 5, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 128,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 128, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267664.905365)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267706.616179)
('Worker processing elapsed time: ', 41.710813999176025, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[128]', 'EPOCH LOSS:', 0.016546047651567447, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 128, ']')


'Epoch [129] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 501,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009833975419327983,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 129,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 129, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267706.621208)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267743.61629)
('Worker processing elapsed time: ', 36.99508213996887, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[129]', 'EPOCH LOSS:', 1.3527895504638807, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 129, ']')


'Epoch [130] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 398,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002543513164016926,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 130,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 130, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267743.621576)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267780.745777)
('Worker processing elapsed time: ', 37.12420082092285, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[130]', 'EPOCH LOSS:', 0.49995000022599789, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 130, ']')


'Epoch [131] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 503,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008043308784876839,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 5, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 131,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 131, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267780.751169)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267827.727175)
('Worker processing elapsed time: ', 46.97600603103638, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[131]', 'EPOCH LOSS:', 0.0092347253366298586, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 131, ']')


'Epoch [132] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 597,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008236031677156234,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 6, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 132,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 132, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267827.732415)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267877.689116)
('Worker processing elapsed time: ', 49.956701040267944, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[132]', 'EPOCH LOSS:', 0.0024434245040078294, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 132, ']')


'Epoch [133] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 444,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00010112291374984503,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 133,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 133, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267877.694079)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267912.662355)
('Worker processing elapsed time: ', 34.968276023864746, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[133]', 'EPOCH LOSS:', 0.176568154836656, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 133, ']')


'Epoch [134] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 464,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00020985718915914162,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 134,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 134, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267912.667651)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494267955.201057)
('Worker processing elapsed time: ', 42.533406019210815, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[134]', 'EPOCH LOSS:', 0.018484034419097319, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 134, ']')


'Epoch [135] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 515,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004898268865368996,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 135,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 135, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494267955.20644)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268000.753087)
('Worker processing elapsed time: ', 45.54664707183838, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[135]', 'EPOCH LOSS:', 0.00084673263385700291, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 135, ']')


'Epoch [136] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 557,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009218357567870389,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 136,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 136, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268000.758579)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268040.609067)
('Worker processing elapsed time: ', 39.85048794746399, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[136]', 'EPOCH LOSS:', 11.981940197276083, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 136, ']')


'Epoch [137] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 415,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002465686100003951,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 137,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 137, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268040.614042)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268079.563014)
('Worker processing elapsed time: ', 38.94897198677063, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[137]', 'EPOCH LOSS:', 0.12819035744033866, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 137, ']')


'Epoch [138] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 508,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00036457251819784777,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 2, 1, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 138,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 138, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268079.568125)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268113.452064)
('Worker processing elapsed time: ', 33.883939027786255, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[138]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 138, ']')


'Epoch [139] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 581,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00010444694629679788,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 139,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 139, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268113.458128)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268154.022875)
('Worker processing elapsed time: ', 40.56474709510803, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[139]', 'EPOCH LOSS:', 0.016803499096391509, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 139, ']')


'Epoch [140] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 303,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008066459827352528,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 140,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 140, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268154.028054)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268194.520429)
('Worker processing elapsed time: ', 40.492374897003174, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[140]', 'EPOCH LOSS:', 0.90171986992174336, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 140, ']')


'Epoch [141] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 386,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002216477402927462,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 141,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 141, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268194.525696)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268233.546051)
('Worker processing elapsed time: ', 39.020354986190796, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[141]', 'EPOCH LOSS:', 6.137606670898311, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 141, ']')


'Epoch [142] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 305,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000380693704262292,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 4, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 142,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 142, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268233.55109)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268276.253982)
('Worker processing elapsed time: ', 42.70289206504822, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[142]', 'EPOCH LOSS:', 1.6032540851667434, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 142, ']')


'Epoch [143] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 265,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009147235726280342,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 143,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 143, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268276.25939)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268325.903379)
('Worker processing elapsed time: ', 49.643988847732544, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[143]', 'EPOCH LOSS:', 0.00065573129026870545, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 143, ']')


'Epoch [144] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 525,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000994298625144334,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 144,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 144, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268325.908456)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268363.740312)
('Worker processing elapsed time: ', 37.83185601234436, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[144]', 'EPOCH LOSS:', 0.0090866314772928818, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 144, ']')


'Epoch [145] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 281,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009148436076869998,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 1, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 145,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 145, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268363.74602)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268405.856652)
('Worker processing elapsed time: ', 42.11063194274902, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[145]', 'EPOCH LOSS:', 0.022924652602824905, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 145, ']')


'Epoch [146] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 366,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008369528622753842,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 146,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 146, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268405.861797)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268443.66105)
('Worker processing elapsed time: ', 37.79925298690796, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[146]', 'EPOCH LOSS:', 0.0023038487066612563, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 146, ']')


'Epoch [147] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 444,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00016062119761992,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 2, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 147,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 147, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268443.666272)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268486.820719)
('Worker processing elapsed time: ', 43.154447078704834, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[147]', 'EPOCH LOSS:', 4.6218113158426197, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 147, ']')


'Epoch [148] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 510,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00010536076205390379,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 148,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 148, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268486.826555)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268522.28684)
('Worker processing elapsed time: ', 35.460284948349, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[148]', 'EPOCH LOSS:', 1.199425829968644, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 148, ']')


'Epoch [149] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 393,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004135669259504273,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 149,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 149, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268522.2926)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268559.247405)
('Worker processing elapsed time: ', 36.95480513572693, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[149]', 'EPOCH LOSS:', 0.18897830376213595, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 149, ']')


'Epoch [150] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 366,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008913159685113366,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 2, 2, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 150,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 150, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268559.252975)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268607.863409)
('Worker processing elapsed time: ', 48.61043405532837, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[150]', 'EPOCH LOSS:', 0.019540832334278006, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 150, ']')


'Epoch [151] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 611,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006704061497118278,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 3, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 151,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 151, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268607.869383)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268651.641644)
('Worker processing elapsed time: ', 43.772260904312134, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[151]', 'EPOCH LOSS:', 0.0743306127183396, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 151, ']')


'Epoch [152] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 536,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009579345918010718,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 152,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 152, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268651.646928)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268685.261932)
('Worker processing elapsed time: ', 33.61500382423401, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[152]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 152, ']')


'Epoch [153] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 399,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007093260131157943,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 153,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 153, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268685.267656)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268722.986444)
('Worker processing elapsed time: ', 37.71878790855408, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[153]', 'EPOCH LOSS:', 0.010123550058555943, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 153, ']')


'Epoch [154] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 316,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00032927851009727433,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 1, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 154,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 154, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268722.991826)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268765.112582)
('Worker processing elapsed time: ', 42.12075591087341, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[154]', 'EPOCH LOSS:', 14.729934779863534, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 154, ']')


'Epoch [155] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 544,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009599536807645263,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 5, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 155,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 155, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268765.118532)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268807.577008)
('Worker processing elapsed time: ', 42.458476066589355, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[155]', 'EPOCH LOSS:', 0.020248965392318317, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 155, ']')


'Epoch [156] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 447,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002153287118776458,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 156,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 156, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268807.581965)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268846.211138)
('Worker processing elapsed time: ', 38.629173040390015, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[156]', 'EPOCH LOSS:', 0.017734750419732458, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 156, ']')


'Epoch [157] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 581,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007467794050664629,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 3, 3, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 157,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 157, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268846.222654)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268895.645593)
('Worker processing elapsed time: ', 49.42293882369995, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[157]', 'EPOCH LOSS:', 0.024197587500012545, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 157, ']')


'Epoch [158] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 603,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00012704983662515485,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 158,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 158, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268895.65073)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268936.218479)
('Worker processing elapsed time: ', 40.5677490234375, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[158]', 'EPOCH LOSS:', 0.0237435102709967, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 158, ']')


'Epoch [159] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 506,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006691235803209039,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 159,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 159, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268936.224048)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494268976.712879)
('Worker processing elapsed time: ', 40.48883104324341, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[159]', 'EPOCH LOSS:', 0.0026515403402552202, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 159, ']')


'Epoch [160] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 403,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007799424559811751,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 160,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 160, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494268976.718289)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269021.350609)
('Worker processing elapsed time: ', 44.632320165634155, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[160]', 'EPOCH LOSS:', 0.0010304801484822903, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 160, ']')


'Epoch [161] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 474,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00025335672148545855,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 161,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 161, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269021.356645)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269058.484823)
('Worker processing elapsed time: ', 37.128177881240845, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[161]', 'EPOCH LOSS:', 0.91240242661036797, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 161, ']')


'Epoch [162] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 617,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006690170994713255,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 162,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 162, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269058.489998)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269096.21566)
('Worker processing elapsed time: ', 37.72566199302673, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[162]', 'EPOCH LOSS:', 0.29616072963374779, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 162, ']')


'Epoch [163] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 278,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008674337281646167,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 163,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 163, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269096.221538)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269138.319121)
('Worker processing elapsed time: ', 42.09758281707764, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[163]', 'EPOCH LOSS:', 0.0043048731894095877, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 163, ']')


'Epoch [164] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 532,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00043254882761654815,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 164,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 164, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269138.324142)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269175.758369)
('Worker processing elapsed time: ', 37.434226989746094, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[164]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 164, ']')


'Epoch [165] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 508,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00013429368600101842,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 1, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 165,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 165, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269175.764106)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269222.994426)
('Worker processing elapsed time: ', 47.23031997680664, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[165]', 'EPOCH LOSS:', 0.020087159797601877, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 165, ']')


'Epoch [166] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 377,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00023125461743105145,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 166,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 166, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269223.000788)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269263.85688)
('Worker processing elapsed time: ', 40.85609197616577, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[166]', 'EPOCH LOSS:', 195.54552225971494, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 166, ']')


'Epoch [167] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 556,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009225707486532702,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 3, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 167,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 167, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269263.862591)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269297.581279)
('Worker processing elapsed time: ', 33.718688011169434, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[167]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 167, ']')


'Epoch [168] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 330,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006756139891614182,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 6, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 168,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 168, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269297.586406)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269343.788853)
('Worker processing elapsed time: ', 46.202446937561035, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[168]', 'EPOCH LOSS:', 0.02616786241995938, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 168, ']')


'Epoch [169] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 321,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007537619939178165,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 4, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 169,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 169, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269343.794184)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269390.459771)
('Worker processing elapsed time: ', 46.665586948394775, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[169]', 'EPOCH LOSS:', 0.00019281058472490138, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 169, ']')


'Epoch [170] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 599,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00027662971140662016,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 170,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 170, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269390.465317)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269432.074034)
('Worker processing elapsed time: ', 41.60871696472168, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[170]', 'EPOCH LOSS:', 0.016588494690800131, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 170, ']')


'Epoch [171] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 323,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002967779958068723,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 171,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 171, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269432.07908)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269472.066693)
('Worker processing elapsed time: ', 39.98761296272278, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[171]', 'EPOCH LOSS:', 0.025655118049546333, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 171, ']')


'Epoch [172] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 592,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000578991779005679,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 172,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 172, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269472.072658)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269507.524928)
('Worker processing elapsed time: ', 35.45227003097534, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[172]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 172, ']')


'Epoch [173] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 548,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00016636765092560622,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 173,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 173, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269507.530275)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269545.003191)
('Worker processing elapsed time: ', 37.47291588783264, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[173]', 'EPOCH LOSS:', 3.3725186520421682, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 173, ']')


'Epoch [174] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 447,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005902693866629719,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 174,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 174, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269545.009253)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269579.819493)
('Worker processing elapsed time: ', 34.810240030288696, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[174]', 'EPOCH LOSS:', 0.016623998055234884, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 174, ']')


'Epoch [175] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 298,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00036215778816556243,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 175,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 175, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269579.824846)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269616.662351)
('Worker processing elapsed time: ', 36.837504863739014, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[175]', 'EPOCH LOSS:', 1.4044805837505077, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 175, ']')


'Epoch [176] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 452,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007178327432734002,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 176,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 176, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269616.668189)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269657.859194)
('Worker processing elapsed time: ', 41.19100499153137, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[176]', 'EPOCH LOSS:', 0.12438073282548297, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 176, ']')


'Epoch [177] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 301,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00010844100311189413,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 177,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 177, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269657.86488)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269697.780094)
('Worker processing elapsed time: ', 39.91521382331848, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[177]', 'EPOCH LOSS:', 0.015063912206623039, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 177, ']')


'Epoch [178] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 540,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006964721802759477,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 5, 6, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 178,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 178, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269697.786081)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269748.228982)
('Worker processing elapsed time: ', 50.44290089607239, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[178]', 'EPOCH LOSS:', 0.0016356903364282435, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 178, ']')


'Epoch [179] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 287,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008297538766272372,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 4, 2, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 179,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 179, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269748.234203)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269791.913023)
('Worker processing elapsed time: ', 43.67881989479065, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[179]', 'EPOCH LOSS:', 13.091440368463571, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 179, ']')


'Epoch [180] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 457,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006448582674343296,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 180,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 180, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269791.918058)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269832.113612)
('Worker processing elapsed time: ', 40.19555401802063, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[180]', 'EPOCH LOSS:', 0.012759316202455072, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 180, ']')


'Epoch [181] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 466,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00011646539797657439,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 181,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 181, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269832.120389)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269872.842544)
('Worker processing elapsed time: ', 40.72215509414673, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[181]', 'EPOCH LOSS:', 0.023228841580691481, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 181, ']')


'Epoch [182] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 320,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008766962493596401,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 182,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 182, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269872.847584)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269917.598065)
('Worker processing elapsed time: ', 44.75048089027405, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[182]', 'EPOCH LOSS:', 0.019671728931923045, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 182, ']')


'Epoch [183] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 332,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00024628448945352994,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 4, 6, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 183,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 183, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269917.603287)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494269967.700465)
('Worker processing elapsed time: ', 50.09717798233032, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[183]', 'EPOCH LOSS:', 0.01271119338302963, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 183, ']')


'Epoch [184] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 489,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00030622038795903226,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 184,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 184, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494269967.705561)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270008.792888)
('Worker processing elapsed time: ', 41.087327003479004, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[184]', 'EPOCH LOSS:', 0.022692239816318818, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 184, ']')


'Epoch [185] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 621,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004548568113092935,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 4, 6, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 185,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 185, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270008.79834)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270057.08083)
('Worker processing elapsed time: ', 48.28249001502991, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[185]', 'EPOCH LOSS:', 2.3765255409627302, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 185, ']')


'Epoch [186] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 387,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000970103307137784,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 1, 6, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 186,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 186, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270057.086229)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270106.726895)
('Worker processing elapsed time: ', 49.640666007995605, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[186]', 'EPOCH LOSS:', 0.00015229004232083444, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 186, ']')


'Epoch [187] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 254,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007228785484842339,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 1, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 187,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 187, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270106.731799)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270149.076174)
('Worker processing elapsed time: ', 42.344375133514404, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[187]', 'EPOCH LOSS:', 0.031150829136417684, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 187, ']')


'Epoch [188] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 458,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0001996493300279817,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 188,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 188, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270149.082368)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270186.129899)
('Worker processing elapsed time: ', 37.04753112792969, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[188]', 'EPOCH LOSS:', 0.015021669930719486, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 188, ']')


'Epoch [189] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 323,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005809337199818904,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 6, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 189,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 189, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270186.136441)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270228.381508)
('Worker processing elapsed time: ', 42.24506711959839, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[189]', 'EPOCH LOSS:', 0.00015434820790388864, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 189, ']')


'Epoch [190] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 598,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002778297790710857,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 190,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 190, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270228.386905)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270269.9525)
('Worker processing elapsed time: ', 41.5655951499939, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[190]', 'EPOCH LOSS:', 10.936784721818714, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 190, ']')


'Epoch [191] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 347,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004111660917626075,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 191,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 191, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270269.958529)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270310.745245)
('Worker processing elapsed time: ', 40.78671598434448, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[191]', 'EPOCH LOSS:', 0.019319511485949563, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 191, ']')


'Epoch [192] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 551,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00040825844186935143,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 192,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 192, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270310.75045)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270347.846245)
('Worker processing elapsed time: ', 37.09579515457153, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[192]', 'EPOCH LOSS:', 0.023925326604855333, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 192, ']')


'Epoch [193] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 479,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005412281359537139,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 193,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 193, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270347.851373)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270384.842318)
('Worker processing elapsed time: ', 36.9909451007843, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[193]', 'EPOCH LOSS:', 0.00039600151334469742, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 193, ']')


'Epoch [194] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 584,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008361563397221568,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 6, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 194,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 194, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270384.848475)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270428.990304)
('Worker processing elapsed time: ', 44.14182901382446, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[194]', 'EPOCH LOSS:', 0.62559964755959363, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 194, ']')


'Epoch [195] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 580,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00022930735604346196,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 195,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 195, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270428.996166)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270488.21165)
('Worker processing elapsed time: ', 59.21548390388489, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[195]', 'EPOCH LOSS:', 0.00060545446483536893, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 195, ']')


'Epoch [196] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 463,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006669490507356815,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 196,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 196, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270488.216792)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270523.491399)
('Worker processing elapsed time: ', 35.27460694313049, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[196]', 'EPOCH LOSS:', 0.016596533334501552, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 196, ']')


'Epoch [197] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 383,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00016918122844139906,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 197,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 197, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270523.497528)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270562.582313)
('Worker processing elapsed time: ', 39.08478498458862, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[197]', 'EPOCH LOSS:', 0.76901682251144265, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 197, ']')


'Epoch [198] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 303,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00035102335435704415,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 198,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 198, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270562.587381)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270601.101403)
('Worker processing elapsed time: ', 38.5140221118927, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[198]', 'EPOCH LOSS:', 2.0600085708562332, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 198, ']')


'Epoch [199] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 566,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000440548552743656,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 199,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 199, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270601.106867)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270642.907114)
('Worker processing elapsed time: ', 41.80024695396423, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[199]', 'EPOCH LOSS:', 0.062148794667034886, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 199, ']')


'Epoch [200] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 401,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000636213398881877,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 2, 3, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 200,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 200, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270642.912357)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270686.486032)
('Worker processing elapsed time: ', 43.57367491722107, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[200]', 'EPOCH LOSS:', 0.021414055949894623, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 200, ']')


'Epoch [201] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 453,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005114950058097058,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 201,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 201, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270686.491408)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270725.509263)
('Worker processing elapsed time: ', 39.01785492897034, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[201]', 'EPOCH LOSS:', 21.493623024055147, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 201, ']')


'Epoch [202] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 599,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00021330477849096732,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 202,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 202, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270725.514793)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270765.022174)
('Worker processing elapsed time: ', 39.507380962371826, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[202]', 'EPOCH LOSS:', 11.534310621992882, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 202, ']')


'Epoch [203] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 575,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004470258030081737,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 203,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 203, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270765.027275)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270810.169914)
('Worker processing elapsed time: ', 45.14263892173767, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[203]', 'EPOCH LOSS:', 0.021464134542673347, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 203, ']')


'Epoch [204] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 321,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004856499586859865,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 204,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 204, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270810.175733)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270852.429152)
('Worker processing elapsed time: ', 42.253418922424316, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[204]', 'EPOCH LOSS:', 0.023090983081494598, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 204, ']')


'Epoch [205] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 590,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009374975823077541,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 205,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 205, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270852.435267)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270893.847394)
('Worker processing elapsed time: ', 41.41212701797485, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[205]', 'EPOCH LOSS:', 0.53822726744800442, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 205, ']')


'Epoch [206] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 520,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00034855322083973336,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 206,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 206, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270893.852767)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270939.238026)
('Worker processing elapsed time: ', 45.38525891304016, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[206]', 'EPOCH LOSS:', 0.0010008343223911976, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 206, ']')


'Epoch [207] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 383,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006736401422080528,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 6, 3, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 207,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 207, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270939.243929)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494270983.24086)
('Worker processing elapsed time: ', 43.996931076049805, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[207]', 'EPOCH LOSS:', 0.020558051602935699, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 207, ']')


'Epoch [208] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 581,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003848091954029516,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 1, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 208,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 208, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494270983.245792)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271026.729465)
('Worker processing elapsed time: ', 43.483673095703125, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[208]', 'EPOCH LOSS:', 0.29071024415118357, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 208, ']')


'Epoch [209] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 549,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006037240575410912,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 209,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 209, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271026.734795)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271063.97036)
('Worker processing elapsed time: ', 37.235564947128296, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[209]', 'EPOCH LOSS:', 0.017740841468201079, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 209, ']')


'Epoch [210] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 537,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004422624475068542,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 1, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 210,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 210, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271063.976035)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271107.225204)
('Worker processing elapsed time: ', 43.24916887283325, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[210]', 'EPOCH LOSS:', 3.9013503710930917, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 210, ']')


'Epoch [211] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 332,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00010040431231522003,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 1, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 211,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 211, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271107.230762)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271149.56382)
('Worker processing elapsed time: ', 42.33305788040161, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[211]', 'EPOCH LOSS:', 2.4428723729363977, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 211, ']')


'Epoch [212] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 491,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004502928896453533,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 2, 2, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 212,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 212, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271149.569959)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271198.833427)
('Worker processing elapsed time: ', 49.26346802711487, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[212]', 'EPOCH LOSS:', 0.019063148965473529, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 212, ']')


'Epoch [213] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 618,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008630906855610757,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 213,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 213, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271198.83935)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271232.530312)
('Worker processing elapsed time: ', 33.690962076187134, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[213]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 213, ']')


'Epoch [214] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 315,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00043076130004710087,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 214,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 214, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271232.53576)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271272.6177)
('Worker processing elapsed time: ', 40.08194017410278, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[214]', 'EPOCH LOSS:', 0.022168319515640723, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 214, ']')


'Epoch [215] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 509,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006236493510025007,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 215,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 215, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271272.623236)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271315.025318)
('Worker processing elapsed time: ', 42.40208196640015, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[215]', 'EPOCH LOSS:', 0.23102285420102234, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 215, ']')


'Epoch [216] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 397,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009988809733292106,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 216,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 216, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271315.030444)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271349.665902)
('Worker processing elapsed time: ', 34.63545799255371, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[216]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 216, ']')


'Epoch [217] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 394,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007824002764579353,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 6, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 217,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 217, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271349.672024)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271385.974969)
('Worker processing elapsed time: ', 36.30294489860535, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[217]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 217, ']')


'Epoch [218] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 565,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00030481547326802834,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 218,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 218, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271385.980172)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271421.426721)
('Worker processing elapsed time: ', 35.4465491771698, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[218]', 'EPOCH LOSS:', 0.26193254707568225, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 218, ']')


'Epoch [219] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 509,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00034470754204910426,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 219,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 219, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271421.432404)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271458.866127)
('Worker processing elapsed time: ', 37.43372297286987, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[219]', 'EPOCH LOSS:', 25.968154802622429, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 219, ']')


'Epoch [220] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 464,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00015572297058703615,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 1, 4, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 220,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 220, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271458.871629)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271507.654913)
('Worker processing elapsed time: ', 48.783283948898315, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[220]', 'EPOCH LOSS:', 0.019132786453174733, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 220, ']')


'Epoch [221] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 490,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000827860395482066,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 221,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 221, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271507.661)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271547.980426)
('Worker processing elapsed time: ', 40.3194260597229, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[221]', 'EPOCH LOSS:', 0.0011391252703942287, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 221, ']')


'Epoch [222] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 376,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008454844895861423,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 222,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 222, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271547.986808)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271590.596511)
('Worker processing elapsed time: ', 42.609702825546265, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[222]', 'EPOCH LOSS:', 0.0021998649078745374, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 222, ']')


'Epoch [223] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 555,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00014735423536015968,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 223,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 223, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271590.602641)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271627.826614)
('Worker processing elapsed time: ', 37.2239727973938, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[223]', 'EPOCH LOSS:', 0.018075062538843036, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 223, ']')


'Epoch [224] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 521,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009858517979469744,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 224,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 224, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271627.833035)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271664.509991)
('Worker processing elapsed time: ', 36.67695593833923, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[224]', 'EPOCH LOSS:', nan, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 224, ']')


'Epoch [225] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 440,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00063357234344109,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 3, 2, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 225,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 225, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271664.514833)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271708.066799)
('Worker processing elapsed time: ', 43.551965951919556, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[225]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 225, ']')


'Epoch [226] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 379,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002901480299524761,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 226,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 226, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271708.072106)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271748.286934)
('Worker processing elapsed time: ', 40.21482801437378, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[226]', 'EPOCH LOSS:', 0.026066042929716478, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 226, ']')


'Epoch [227] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 551,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009702091391839092,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 227,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 227, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271748.292281)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271781.502227)
('Worker processing elapsed time: ', 33.209946155548096, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[227]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 227, ']')


'Epoch [228] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 392,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005908347144971357,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 228,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 228, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271781.507132)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271819.234527)
('Worker processing elapsed time: ', 37.72739505767822, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[228]', 'EPOCH LOSS:', 0.0026223155165595586, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 228, ']')


'Epoch [229] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 448,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009583350525650686,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 229,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 229, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271819.239758)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271853.90618)
('Worker processing elapsed time: ', 34.66642189025879, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[229]', 'EPOCH LOSS:', 0.024581292456312027, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 229, ']')


'Epoch [230] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 600,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008002571713094768,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 230,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 230, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271853.911121)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271887.50832)
('Worker processing elapsed time: ', 33.59719920158386, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[230]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 230, ']')


'Epoch [231] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 369,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007636667652831035,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 231,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 231, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271887.51442)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271925.292135)
('Worker processing elapsed time: ', 37.77771496772766, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[231]', 'EPOCH LOSS:', 0.0023028206704862717, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 231, ']')


'Epoch [232] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 262,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000697948216609794,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 232,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 232, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271925.298097)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494271965.025672)
('Worker processing elapsed time: ', 39.727575063705444, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[232]', 'EPOCH LOSS:', 0.020959169048359833, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 232, ']')


'Epoch [233] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 535,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00040563098414927296,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 5, 3, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 233,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 233, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494271965.030986)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272009.041657)
('Worker processing elapsed time: ', 44.01067090034485, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[233]', 'EPOCH LOSS:', 0.022541284742908865, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 233, ']')


'Epoch [234] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 396,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004633469223345049,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 234,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 234, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272009.04745)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272048.957141)
('Worker processing elapsed time: ', 39.909690856933594, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[234]', 'EPOCH LOSS:', 0.021053934843455725, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 234, ']')


'Epoch [235] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 319,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004917524403645035,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 1, 2, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 235,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 235, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272048.96326)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272092.50316)
('Worker processing elapsed time: ', 43.539900064468384, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[235]', 'EPOCH LOSS:', 1.9391189519743441, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 235, ']')


'Epoch [236] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 258,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006733994151607303,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 236,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 236, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272092.508682)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272132.849795)
('Worker processing elapsed time: ', 40.34111309051514, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[236]', 'EPOCH LOSS:', 0.91456677609685622, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 236, ']')


'Epoch [237] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 436,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006099113123333957,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 237,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 237, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272132.85505)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272170.649863)
('Worker processing elapsed time: ', 37.79481291770935, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[237]', 'EPOCH LOSS:', 0.016093006027554962, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 237, ']')


'Epoch [238] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 565,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00014297814040249203,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 238,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 238, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272170.655541)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272208.148074)
('Worker processing elapsed time: ', 37.49253296852112, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[238]', 'EPOCH LOSS:', 0.36377075633062389, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 238, ']')


'Epoch [239] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 537,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006957069527906592,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 239,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 239, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272208.153857)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272244.144044)
('Worker processing elapsed time: ', 35.99018692970276, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[239]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 239, ']')


'Epoch [240] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 611,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007562356845567052,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 4, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 240,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 240, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272244.149833)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272280.468128)
('Worker processing elapsed time: ', 36.31829500198364, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[240]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 240, ']')


'Epoch [241] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 348,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005553367493392145,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 241,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 241, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272280.473929)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272317.348876)
('Worker processing elapsed time: ', 36.87494707107544, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[241]', 'EPOCH LOSS:', 0.19011660239670483, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 241, ']')


'Epoch [242] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 259,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00034649540906566236,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 1, 3, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 242,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 242, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272317.354197)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272359.859498)
('Worker processing elapsed time: ', 42.505300998687744, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[242]', 'EPOCH LOSS:', 0.0225566297109011, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 242, ']')


'Epoch [243] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 462,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006246784415101537,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 243,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 243, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272359.864978)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272394.780435)
('Worker processing elapsed time: ', 34.915457010269165, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[243]', 'EPOCH LOSS:', 0.020118671398493018, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 243, ']')


'Epoch [244] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 563,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006667354311412527,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 244,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 244, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272394.785907)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272437.212387)
('Worker processing elapsed time: ', 42.42648005485535, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[244]', 'EPOCH LOSS:', 0.39594802270704238, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 244, ']')


'Epoch [245] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 435,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00031416579745912167,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 245,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 245, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272437.217878)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272472.276048)
('Worker processing elapsed time: ', 35.05816984176636, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[245]', 'EPOCH LOSS:', 0.33955352282671347, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 245, ']')


'Epoch [246] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 605,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009533473672389611,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 5, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 246,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 246, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272472.281855)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272515.203453)
('Worker processing elapsed time: ', 42.921597957611084, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[246]', 'EPOCH LOSS:', 2.8281167124954609, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 246, ']')


'Epoch [247] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 269,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008136183032471596,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 247,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 247, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272515.208588)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272559.563494)
('Worker processing elapsed time: ', 44.35490608215332, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[247]', 'EPOCH LOSS:', 0.0012304705479378518, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 247, ']')


'Epoch [248] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 571,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009264195985043213,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 248,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 248, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272559.568741)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272598.520445)
('Worker processing elapsed time: ', 38.951704025268555, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[248]', 'EPOCH LOSS:', 0.022761704676810889, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 248, ']')


'Epoch [249] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 286,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005786759473889167,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 5, 1, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 249,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 249, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272598.526082)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272642.575248)
('Worker processing elapsed time: ', 44.04916596412659, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[249]', 'EPOCH LOSS:', 0.016013416356875872, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 249, ']')


'Epoch [250] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 414,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005892025120387306,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 250,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 250, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272642.580741)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272685.21684)
('Worker processing elapsed time: ', 42.636099100112915, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[250]', 'EPOCH LOSS:', 0.0025642721480000378, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 250, ']')


'Epoch [251] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 558,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005062732061492914,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 251,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 251, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272685.222273)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272731.351292)
('Worker processing elapsed time: ', 46.129018783569336, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[251]', 'EPOCH LOSS:', 0.017915299380667136, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 251, ']')


'Epoch [252] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 550,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00032690151437402633,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 3, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 252,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 252, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272731.356122)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272774.498238)
('Worker processing elapsed time: ', 43.1421160697937, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[252]', 'EPOCH LOSS:', 0.36649945065946204, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 252, ']')


'Epoch [253] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 476,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006792495170087984,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 253,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 253, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272774.504396)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272811.387734)
('Worker processing elapsed time: ', 36.88333797454834, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[253]', 'EPOCH LOSS:', 0.00066038368812251975, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 253, ']')


'Epoch [254] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 291,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004050996530484271,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 254,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 254, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272811.392877)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272848.826955)
('Worker processing elapsed time: ', 37.434077978134155, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[254]', 'EPOCH LOSS:', 0.01090255754471149, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 254, ']')


'Epoch [255] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 401,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008033416405461356,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 255,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 255, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272848.832452)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272883.726518)
('Worker processing elapsed time: ', 34.894065856933594, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[255]', 'EPOCH LOSS:', 0.38368382089174718, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 255, ']')


'Epoch [256] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 379,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004292993108651154,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 5, 5, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 256,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 256, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272883.732598)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272928.166217)
('Worker processing elapsed time: ', 44.433619022369385, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[256]', 'EPOCH LOSS:', 0.11638323381174741, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 256, ']')


'Epoch [257] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 282,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00020805968834398557,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 257,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 257, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272928.171062)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272964.522516)
('Worker processing elapsed time: ', 36.35145401954651, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[257]', 'EPOCH LOSS:', 0.020096621297717727, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 257, ']')


'Epoch [258] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 462,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002688410490781955,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 258,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 258, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272964.527833)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494272999.460855)
('Worker processing elapsed time: ', 34.933022022247314, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[258]', 'EPOCH LOSS:', 0.0090970883560775422, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 258, ']')


'Epoch [259] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 383,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00039244281714453314,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 3, 4, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 259,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 259, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494272999.466579)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273043.659005)
('Worker processing elapsed time: ', 44.19242596626282, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[259]', 'EPOCH LOSS:', 0.020654987665461869, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 259, ']')


'Epoch [260] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 383,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00032809061083713463,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 260,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 260, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273043.664076)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273078.689064)
('Worker processing elapsed time: ', 35.0249879360199, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[260]', 'EPOCH LOSS:', 1.2091660085847904, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 260, ']')


'Epoch [261] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 453,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008628859825906421,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 261,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 261, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273078.694052)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273113.630854)
('Worker processing elapsed time: ', 34.93680191040039, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[261]', 'EPOCH LOSS:', 0.38660879131623693, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 261, ']')


'Epoch [262] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 411,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004789792815784392,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 1, 1, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 262,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 262, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273113.637067)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273161.949407)
('Worker processing elapsed time: ', 48.31234002113342, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[262]', 'EPOCH LOSS:', 0.022674368552047117, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 262, ']')


'Epoch [263] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 363,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005395198870183535,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 3, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 263,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 263, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273161.954691)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273203.666345)
('Worker processing elapsed time: ', 41.7116539478302, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[263]', 'EPOCH LOSS:', 0.023153919855692134, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 263, ']')


'Epoch [264] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 562,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005827327240527473,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 264,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 264, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273203.672078)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273248.723862)
('Worker processing elapsed time: ', 45.0517840385437, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[264]', 'EPOCH LOSS:', 0.017808031114435768, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 264, ']')


'Epoch [265] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 595,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002983024078342144,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 265,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 265, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273248.729443)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273289.244534)
('Worker processing elapsed time: ', 40.51509094238281, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[265]', 'EPOCH LOSS:', 0.0070489250551373029, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 265, ']')


'Epoch [266] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 301,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008220856814235289,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 266,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 266, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273289.250585)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273329.123292)
('Worker processing elapsed time: ', 39.8727068901062, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[266]', 'EPOCH LOSS:', 0.019482819363772042, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 266, ']')


'Epoch [267] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 294,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00034357790139879357,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 267,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 267, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273329.128698)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273363.49936)
('Worker processing elapsed time: ', 34.37066197395325, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[267]', 'EPOCH LOSS:', 0.012392588906631051, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 267, ']')


'Epoch [268] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 407,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007678000394703843,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 268,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 268, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273363.504687)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273405.992832)
('Worker processing elapsed time: ', 42.488144874572754, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[268]', 'EPOCH LOSS:', 0.024499254439385068, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 268, ']')


'Epoch [269] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 481,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00026321351315525493,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 269,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 269, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273405.998685)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273443.009175)
('Worker processing elapsed time: ', 37.01049017906189, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[269]', 'EPOCH LOSS:', 0.0080149403783047555, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 269, ']')


'Epoch [270] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 258,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008501559735602385,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 270,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 270, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273443.014369)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273477.238499)
('Worker processing elapsed time: ', 34.22412991523743, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[270]', 'EPOCH LOSS:', 0.0040410723639178964, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 270, ']')


'Epoch [271] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 334,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007419582049278636,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 1, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 271,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 271, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273477.244132)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273523.773575)
('Worker processing elapsed time: ', 46.52944302558899, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[271]', 'EPOCH LOSS:', 0.0011621194503673739, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 271, ']')


'Epoch [272] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 350,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00028946212198954237,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 1, 6, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 272,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 272, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273523.778601)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273572.762402)
('Worker processing elapsed time: ', 48.9838011264801, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[272]', 'EPOCH LOSS:', 0.009288114191641713, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 272, ']')


'Epoch [273] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 345,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007024903865453882,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 273,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 273, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273572.767914)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273613.033047)
('Worker processing elapsed time: ', 40.265132904052734, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[273]', 'EPOCH LOSS:', 0.021653800913461169, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 273, ']')


'Epoch [274] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 612,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00030280177797975885,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 274,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 274, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273613.038168)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273650.7596)
('Worker processing elapsed time: ', 37.72143197059631, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[274]', 'EPOCH LOSS:', 0.83957221306652696, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 274, ']')


'Epoch [275] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 334,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00028418228071328433,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 3, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 275,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 275, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273650.765403)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273692.825707)
('Worker processing elapsed time: ', 42.060303926467896, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[275]', 'EPOCH LOSS:', 0.02256543971224452, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 275, ']')


'Epoch [276] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 256,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003542836575821949,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 276,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 276, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273692.831033)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273732.730511)
('Worker processing elapsed time: ', 39.8994779586792, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[276]', 'EPOCH LOSS:', 0.0023844073142957186, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 276, ']')


'Epoch [277] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 513,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002682117174752217,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 277,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 277, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273732.735415)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273773.28292)
('Worker processing elapsed time: ', 40.547504901885986, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[277]', 'EPOCH LOSS:', 0.017848524718324914, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 277, ']')


'Epoch [278] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 528,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005580838433248152,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 278,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 278, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273773.288961)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273812.21677)
('Worker processing elapsed time: ', 38.92780900001526, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[278]', 'EPOCH LOSS:', 0.022885387408448352, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 278, ']')


'Epoch [279] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 375,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004192068513762784,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 6, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 279,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 279, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273812.221755)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273854.897455)
('Worker processing elapsed time: ', 42.675699949264526, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[279]', 'EPOCH LOSS:', 112.59223988680957, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 279, ']')


'Epoch [280] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 440,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005331823678623077,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 280,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 280, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273854.903021)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273895.846103)
('Worker processing elapsed time: ', 40.943081855773926, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[280]', 'EPOCH LOSS:', 3.7977235105512319, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 280, ']')


'Epoch [281] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 569,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008558478746784964,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 281,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 281, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273895.851032)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273935.616267)
('Worker processing elapsed time: ', 39.76523494720459, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[281]', 'EPOCH LOSS:', 6.5586403010657781, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 281, ']')


'Epoch [282] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 410,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0001987260171736453,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 282,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 282, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273935.621143)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494273974.178698)
('Worker processing elapsed time: ', 38.557554960250854, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[282]', 'EPOCH LOSS:', 0.02262156979459911, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 282, ']')


'Epoch [283] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 469,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000893024914734369,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 283,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 283, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494273974.184379)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274007.719455)
('Worker processing elapsed time: ', 33.53507590293884, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[283]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 283, ']')


'Epoch [284] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 517,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005645122682063216,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 284,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 284, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274007.724488)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274049.00036)
('Worker processing elapsed time: ', 41.275871992111206, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[284]', 'EPOCH LOSS:', 0.14866472043944637, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 284, ']')


'Epoch [285] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 564,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004442786984715976,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 6, 5, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 285,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 285, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274049.006692)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274093.898389)
('Worker processing elapsed time: ', 44.89169716835022, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[285]', 'EPOCH LOSS:', 0.022099365760038299, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 285, ']')


'Epoch [286] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 585,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002363607364229825,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 6, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 286,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 286, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274093.903714)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274142.686601)
('Worker processing elapsed time: ', 48.78288698196411, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[286]', 'EPOCH LOSS:', 0.015001086850221576, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 286, ']')


'Epoch [287] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 343,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007428212133381677,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 287,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 287, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274142.692438)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274177.250953)
('Worker processing elapsed time: ', 34.5585150718689, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[287]', 'EPOCH LOSS:', 1.5833596401401788, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 287, ']')


'Epoch [288] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 315,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009984863438221702,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 4, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 288,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 288, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274177.256564)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274222.737085)
('Worker processing elapsed time: ', 45.4805212020874, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[288]', 'EPOCH LOSS:', 8.4081353241807221e-05, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 288, ']')


'Epoch [289] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 561,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009852970290308144,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 5, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 289,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 289, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274222.742505)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274266.303423)
('Worker processing elapsed time: ', 43.56091785430908, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[289]', 'EPOCH LOSS:', 0.19631487293627556, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 289, ']')


'Epoch [290] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 302,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007870923612310636,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 290,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 290, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274266.309401)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274303.81001)
('Worker processing elapsed time: ', 37.500608921051025, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[290]', 'EPOCH LOSS:', 0.0076622093396695509, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 290, ']')


'Epoch [291] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 480,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00043692655563519554,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 291,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 291, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274303.815287)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274344.365477)
('Worker processing elapsed time: ', 40.55018997192383, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[291]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 291, ']')


'Epoch [292] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 424,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00019040597845306156,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 292,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 292, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274344.37098)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274382.078459)
('Worker processing elapsed time: ', 37.70747900009155, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[292]', 'EPOCH LOSS:', 0.013002812309828622, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 292, ']')


'Epoch [293] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 293,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006051974238794109,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 3, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 293,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 293, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274382.084838)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274428.851418)
('Worker processing elapsed time: ', 46.76658010482788, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[293]', 'EPOCH LOSS:', 0.0038386759603907159, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 293, ']')


'Epoch [294] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 367,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006165512137593482,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 5, 4, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 294,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 294, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274428.857699)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274462.71534)
('Worker processing elapsed time: ', 33.857640981674194, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[294]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 294, ']')


'Epoch [295] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 608,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008346827956136474,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 295,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 295, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274462.72068)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274501.823062)
('Worker processing elapsed time: ', 39.10238194465637, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[295]', 'EPOCH LOSS:', 0.020224503190065437, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 295, ']')


'Epoch [296] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 429,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00043853426155393213,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 2, 2, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 296,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 296, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274501.828829)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274545.84986)
('Worker processing elapsed time: ', 44.02103090286255, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[296]', 'EPOCH LOSS:', 0.7513160475241939, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 296, ']')


'Epoch [297] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 567,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004009064432523755,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 297,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 297, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274545.855895)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274589.249992)
('Worker processing elapsed time: ', 43.39409685134888, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[297]', 'EPOCH LOSS:', 0.0010506501856505763, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 297, ']')


'Epoch [298] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 455,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00044922915144648284,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 298,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 298, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274589.255364)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274626.010785)
('Worker processing elapsed time: ', 36.75542116165161, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[298]', 'EPOCH LOSS:', 0.015690206669718432, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 298, ']')


'Epoch [299] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 429,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006069851290563546,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 299,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 299, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274626.022694)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274663.837979)
('Worker processing elapsed time: ', 37.815284967422485, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[299]', 'EPOCH LOSS:', 0.0025256766871285144, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 299, ']')


'Epoch [300] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 538,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002420927412645949,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 300,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 300, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274663.842951)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274705.353855)
('Worker processing elapsed time: ', 41.51090383529663, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[300]', 'EPOCH LOSS:', 0.016641658631774484, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 300, ']')


'Epoch [301] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 458,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000431516408958506,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 301,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 301, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274705.359373)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274750.143657)
('Worker processing elapsed time: ', 44.78428387641907, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[301]', 'EPOCH LOSS:', 0.018864367559575525, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 301, ']')


'Epoch [302] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 296,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006067209812685293,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 302,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 302, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274750.149752)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274790.229552)
('Worker processing elapsed time: ', 40.07980012893677, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[302]', 'EPOCH LOSS:', 0.00059079858576629096, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 302, ']')


'Epoch [303] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 603,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002382388616651817,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 303,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 303, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274790.234903)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274831.281528)
('Worker processing elapsed time: ', 41.04662489891052, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[303]', 'EPOCH LOSS:', 0.0033074265290252026, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 303, ']')


'Epoch [304] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 273,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000977456972625592,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 304,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 304, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274831.287088)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274867.668702)
('Worker processing elapsed time: ', 36.381613969802856, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[304]', 'EPOCH LOSS:', 0.0014210436531579085, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 304, ']')


'Epoch [305] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 311,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009087475716655646,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 305,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 305, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274867.674024)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274907.744149)
('Worker processing elapsed time: ', 40.07012486457825, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[305]', 'EPOCH LOSS:', 0.016456855690073657, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 305, ']')


'Epoch [306] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 316,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009150070173380294,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 2, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 306,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 306, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274907.749616)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274949.495502)
('Worker processing elapsed time: ', 41.7458860874176, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[306]', 'EPOCH LOSS:', 0.025676904702655403, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 306, ']')


'Epoch [307] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 310,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008782168465419991,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 5, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 307,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 307, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274949.50102)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494274996.685112)
('Worker processing elapsed time: ', 47.18409204483032, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[307]', 'EPOCH LOSS:', 0.0002512584778078654, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 307, ']')


'Epoch [308] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 619,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006127737755683298,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 308,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 308, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494274996.68997)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275034.103387)
('Worker processing elapsed time: ', 37.41341710090637, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[308]', 'EPOCH LOSS:', 0.021091630487493011, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 308, ']')


'Epoch [309] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 326,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007459030981452647,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 309,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 309, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275034.108332)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275070.657884)
('Worker processing elapsed time: ', 36.54955196380615, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[309]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 309, ']')


'Epoch [310] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 524,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007950009562783809,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 4, 4, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 310,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 310, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275070.663456)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275115.506847)
('Worker processing elapsed time: ', 44.84339094161987, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[310]', 'EPOCH LOSS:', 0.8711073168385185, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 310, ']')


'Epoch [311] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 569,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009693441691811387,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 311,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 311, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275115.513038)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275152.961061)
('Worker processing elapsed time: ', 37.448023080825806, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[311]', 'EPOCH LOSS:', 0.068259031156937544, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 311, ']')


'Epoch [312] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 313,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00043719997125794374,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 312,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 312, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275152.966402)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275193.056342)
('Worker processing elapsed time: ', 40.08993983268738, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[312]', 'EPOCH LOSS:', 0.015616799182272052, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 312, ']')


'Epoch [313] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 369,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003738211417283107,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 6, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 313,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 313, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275193.061423)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275235.665767)
('Worker processing elapsed time: ', 42.6043438911438, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[313]', 'EPOCH LOSS:', 0.16252653388772115, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 313, ']')


'Epoch [314] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 298,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008197818195134723,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 1, 2, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 314,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 314, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275235.670581)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275283.964148)
('Worker processing elapsed time: ', 48.293566942214966, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[314]', 'EPOCH LOSS:', 0.019605713109006405, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 314, ']')


'Epoch [315] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 527,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000797787829702537,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 315,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 315, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275283.970012)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275324.908954)
('Worker processing elapsed time: ', 40.938941955566406, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[315]', 'EPOCH LOSS:', 0.020314032787188779, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 315, ']')


'Epoch [316] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 502,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00022661903598760586,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 316,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 316, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275324.914027)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275364.103838)
('Worker processing elapsed time: ', 39.18981099128723, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[316]', 'EPOCH LOSS:', 0.022456442101211727, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 316, ']')


'Epoch [317] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 484,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004823625791400889,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 317,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 317, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275364.109357)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275406.466068)
('Worker processing elapsed time: ', 42.35671091079712, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[317]', 'EPOCH LOSS:', 0.0086211220368161198, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 317, ']')


'Epoch [318] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 287,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00021667043490174447,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 318,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 318, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275406.472404)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275446.383832)
('Worker processing elapsed time: ', 39.91142797470093, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[318]', 'EPOCH LOSS:', 0.018960123836339784, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 318, ']')


'Epoch [319] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 507,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002419292456193261,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 319,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 319, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275446.38923)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275484.400942)
('Worker processing elapsed time: ', 38.011712074279785, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[319]', 'EPOCH LOSS:', 0.014252543657448257, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 319, ']')


'Epoch [320] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 273,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008914839950457126,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 320,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 320, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275484.405951)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275526.520232)
('Worker processing elapsed time: ', 42.11428093910217, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[320]', 'EPOCH LOSS:', 0.021378555274301141, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 320, ']')


'Epoch [321] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 395,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006287467631858664,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 321,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 321, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275526.525958)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275561.090551)
('Worker processing elapsed time: ', 34.56459283828735, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[321]', 'EPOCH LOSS:', 0.018528512915696634, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 321, ']')


'Epoch [322] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 267,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008019367800343377,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 6, 5, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 322,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 322, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275561.095864)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275598.122324)
('Worker processing elapsed time: ', 37.02645993232727, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[322]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 322, ']')


'Epoch [323] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 318,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006692468567950957,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 323,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 323, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275598.127767)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275635.822397)
('Worker processing elapsed time: ', 37.69462990760803, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[323]', 'EPOCH LOSS:', 0.0037991729707908396, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 323, ']')


'Epoch [324] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 466,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006277419521757936,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 5, 3, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 324,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 324, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275635.827534)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275674.939892)
('Worker processing elapsed time: ', 39.11235809326172, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[324]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 324, ']')


'Epoch [325] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 417,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00028681864594637776,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 325,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 325, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275674.946095)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275715.541993)
('Worker processing elapsed time: ', 40.595897912979126, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[325]', 'EPOCH LOSS:', 0.13078386864208075, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 325, ']')


'Epoch [326] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 457,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000940251440372629,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 6, 1, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 326,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 326, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275715.547631)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275764.74527)
('Worker processing elapsed time: ', 49.19763898849487, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[326]', 'EPOCH LOSS:', 0.021381025079812828, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 326, ']')


'Epoch [327] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 564,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00040291935928583357,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 327,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 327, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275764.750311)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275807.022439)
('Worker processing elapsed time: ', 42.272128105163574, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[327]', 'EPOCH LOSS:', 0.021200273957048387, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 327, ']')


'Epoch [328] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 312,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007746696610609151,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 328,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 328, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275807.027654)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275846.965591)
('Worker processing elapsed time: ', 39.93793702125549, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[328]', 'EPOCH LOSS:', 0.0036846169104841312, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 328, ']')


'Epoch [329] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 386,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002308313728736082,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 329,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 329, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275846.970777)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275883.851897)
('Worker processing elapsed time: ', 36.88111996650696, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[329]', 'EPOCH LOSS:', 0.016155812679972108, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 329, ']')


'Epoch [330] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 493,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000420167951556861,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 330,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 330, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275883.857964)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275920.99316)
('Worker processing elapsed time: ', 37.13519597053528, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[330]', 'EPOCH LOSS:', 0.16457622428860291, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 330, ']')


'Epoch [331] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 468,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003321158970644413,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 331,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 331, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275920.998465)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275956.099615)
('Worker processing elapsed time: ', 35.101150035858154, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[331]', 'EPOCH LOSS:', 0.61953047624126634, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 331, ']')


'Epoch [332] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 277,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009065958551052455,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 332,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 332, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275956.104866)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494275990.675416)
('Worker processing elapsed time: ', 34.570549964904785, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[332]', 'EPOCH LOSS:', 0.011230125974456663, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 332, ']')


'Epoch [333] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 344,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009753139647406521,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 333,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 333, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494275990.681293)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276031.357914)
('Worker processing elapsed time: ', 40.676620960235596, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[333]', 'EPOCH LOSS:', 15.118179083316754, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 333, ']')


'Epoch [334] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 515,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00036483847617091426,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 334,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 334, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276031.363679)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276074.049339)
('Worker processing elapsed time: ', 42.68566012382507, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[334]', 'EPOCH LOSS:', 0.006348906613005946, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 334, ']')


'Epoch [335] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 422,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007081498711541705,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 335,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 335, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276074.054594)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276111.785867)
('Worker processing elapsed time: ', 37.73127293586731, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[335]', 'EPOCH LOSS:', 0.012228058240886193, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 335, ']')


'Epoch [336] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 402,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00041929590578482917,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 3, 5, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 336,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 336, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276111.791166)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276156.340202)
('Worker processing elapsed time: ', 44.54903602600098, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[336]', 'EPOCH LOSS:', 280.83964214674756, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 336, ']')


'Epoch [337] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 545,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007027594271856565,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 5, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 337,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 337, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276156.346354)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276201.879608)
('Worker processing elapsed time: ', 45.53325390815735, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[337]', 'EPOCH LOSS:', 0.00092522625343413275, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 337, ']')


'Epoch [338] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 453,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008600819556810491,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 338,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 338, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276201.884748)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276243.282765)
('Worker processing elapsed time: ', 41.398016929626465, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[338]', 'EPOCH LOSS:', 0.44409845580870561, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 338, ']')


'Epoch [339] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 318,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008258435347399664,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 339,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 339, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276243.288792)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276277.92258)
('Worker processing elapsed time: ', 34.633788108825684, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[339]', 'EPOCH LOSS:', 0.17460351571911167, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 339, ']')


'Epoch [340] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 411,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009483757015463829,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 5, 2, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 340,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 340, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276277.929051)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276322.255123)
('Worker processing elapsed time: ', 44.326071977615356, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[340]', 'EPOCH LOSS:', 1.2155525674454468, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 340, ']')


'Epoch [341] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 309,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006929994676022306,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 341,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 341, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276322.260429)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276360.824575)
('Worker processing elapsed time: ', 38.56414604187012, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[341]', 'EPOCH LOSS:', 0.024563943884154348, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 341, ']')


'Epoch [342] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 490,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007148332313637238,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 4, 4, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 342,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 342, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276360.829832)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276405.872258)
('Worker processing elapsed time: ', 45.042425870895386, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[342]', 'EPOCH LOSS:', 0.76954415657238373, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 342, ']')


'Epoch [343] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 356,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00042460065429317576,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 343,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 343, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276405.878173)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276446.523493)
('Worker processing elapsed time: ', 40.64531993865967, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[343]', 'EPOCH LOSS:', 1.9279335864669165, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 343, ']')


'Epoch [344] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 533,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009289996731623785,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 3, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 344,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 344, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276446.529034)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276494.039609)
('Worker processing elapsed time: ', 47.51057505607605, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[344]', 'EPOCH LOSS:', 0.018839968330356453, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 344, ']')


'Epoch [345] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 330,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00016425478138874458,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 345,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 345, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276494.045171)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276527.458524)
('Worker processing elapsed time: ', 33.413352966308594, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[345]', 'EPOCH LOSS:', 0.023871879283117126, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 345, ']')


'Epoch [346] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 268,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008416321459572186,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 1, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 346,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 346, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276527.46377)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276568.914008)
('Worker processing elapsed time: ', 41.45023798942566, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[346]', 'EPOCH LOSS:', 0.021116747746119515, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 346, ']')


'Epoch [347] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 581,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00022485897813437372,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 5, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 347,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 347, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276568.918956)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276617.17369)
('Worker processing elapsed time: ', 48.25473403930664, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[347]', 'EPOCH LOSS:', 0.00062145077562150469, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 347, ']')


'Epoch [348] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 300,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003052354430587085,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 5, 5, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 348,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 348, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276617.178864)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276660.838811)
('Worker processing elapsed time: ', 43.65994691848755, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[348]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 348, ']')


'Epoch [349] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 471,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007869953911777692,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 349,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 349, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276660.844523)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276695.593672)
('Worker processing elapsed time: ', 34.74914908409119, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[349]', 'EPOCH LOSS:', 0.019316810691122524, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 349, ']')


'Epoch [350] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 579,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002314542494996737,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 350,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 350, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276695.599357)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276737.469106)
('Worker processing elapsed time: ', 41.86974906921387, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[350]', 'EPOCH LOSS:', 0.47501484071118666, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 350, ']')


'Epoch [351] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 442,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008579272356114784,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 6, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 351,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 351, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276737.474203)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276779.486006)
('Worker processing elapsed time: ', 42.01180291175842, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[351]', 'EPOCH LOSS:', 0.021774619634664198, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 351, ']')


'Epoch [352] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 255,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008535919285852248,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 352,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 352, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276779.491075)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276821.819751)
('Worker processing elapsed time: ', 42.328675985336304, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[352]', 'EPOCH LOSS:', 0.44964019398073141, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 352, ']')


'Epoch [353] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 564,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005107892594351024,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 353,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 353, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276821.824868)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276861.16518)
('Worker processing elapsed time: ', 39.340312004089355, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[353]', 'EPOCH LOSS:', 0.016558729553082498, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 353, ']')


'Epoch [354] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 507,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008122181059166565,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 354,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 354, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276861.170553)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276903.218949)
('Worker processing elapsed time: ', 42.04839611053467, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[354]', 'EPOCH LOSS:', 0.037883914774216994, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 354, ']')


'Epoch [355] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 509,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009465554691574723,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 355,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 355, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276903.224902)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276948.331268)
('Worker processing elapsed time: ', 45.10636615753174, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[355]', 'EPOCH LOSS:', 0.014627020362244525, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 355, ']')


'Epoch [356] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 303,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00041852383516113964,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 356,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 356, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276948.336695)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494276982.980766)
('Worker processing elapsed time: ', 34.644071102142334, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[356]', 'EPOCH LOSS:', 0.58158163772836524, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 356, ']')


'Epoch [357] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 367,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00038791467259435505,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 2, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 357,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 357, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494276982.986888)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277024.600813)
('Worker processing elapsed time: ', 41.613924980163574, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[357]', 'EPOCH LOSS:', 0.021976011535186846, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 357, ']')


'Epoch [358] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 254,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007372927052248779,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 358,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 358, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277024.606472)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277058.863329)
('Worker processing elapsed time: ', 34.25685691833496, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[358]', 'EPOCH LOSS:', 0.0033248202715801733, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 358, ']')


'Epoch [359] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 550,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005193249731647502,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 359,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 359, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277058.869118)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277100.212082)
('Worker processing elapsed time: ', 41.3429639339447, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[359]', 'EPOCH LOSS:', 3.6643897398749385, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 359, ']')


'Epoch [360] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 336,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00019609523468659597,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 360,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 360, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277100.21758)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277140.407899)
('Worker processing elapsed time: ', 40.19031882286072, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[360]', 'EPOCH LOSS:', 0.0081192151666116229, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 360, ']')


'Epoch [361] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 531,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004426670714257092,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 5, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 361,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 361, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277140.413283)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277174.150322)
('Worker processing elapsed time: ', 33.7370388507843, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[361]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 361, ']')


'Epoch [362] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 360,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008708028206117886,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 362,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 362, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277174.155257)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277214.089269)
('Worker processing elapsed time: ', 39.934011936187744, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[362]', 'EPOCH LOSS:', 0.00078904439037935062, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 362, ']')


'Epoch [363] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 585,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00013991810677826022,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 4, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 363,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 363, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277214.094585)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277256.823078)
('Worker processing elapsed time: ', 42.728492975234985, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[363]', 'EPOCH LOSS:', 0.021852057808401094, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 363, ']')


'Epoch [364] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 420,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003717828350757766,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 364,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 364, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277256.827967)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277293.676099)
('Worker processing elapsed time: ', 36.84813213348389, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[364]', 'EPOCH LOSS:', 2.1878169578357896, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 364, ']')


'Epoch [365] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 477,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00026617081957242765,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 365,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 365, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277293.681023)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277334.156965)
('Worker processing elapsed time: ', 40.47594213485718, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[365]', 'EPOCH LOSS:', 0.0082665527567421359, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 365, ']')


'Epoch [366] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 571,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0002189366200045504,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 366,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 366, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277334.162582)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277375.433631)
('Worker processing elapsed time: ', 41.27104902267456, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[366]', 'EPOCH LOSS:', 0.01848868521189246, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 366, ']')


'Epoch [367] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 458,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005148271749678905,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 367,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 367, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277375.438988)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277413.996891)
('Worker processing elapsed time: ', 38.55790305137634, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[367]', 'EPOCH LOSS:', 0.018425977746041877, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 367, ']')


'Epoch [368] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 338,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00041871010339493344,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 2, 2, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 368,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 368, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277414.001955)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277457.090053)
('Worker processing elapsed time: ', 43.08809804916382, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[368]', 'EPOCH LOSS:', 0.019836015031225791, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 368, ']')


'Epoch [369] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 475,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006654206425737669,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 2, 6, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 369,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 369, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277457.09537)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277501.501909)
('Worker processing elapsed time: ', 44.40653896331787, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[369]', 'EPOCH LOSS:', 0.020108671761397943, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 369, ']')


'Epoch [370] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 575,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00015823610482928031,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 370,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 370, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277501.507296)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277542.492854)
('Worker processing elapsed time: ', 40.9855580329895, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[370]', 'EPOCH LOSS:', 0.022487510442332905, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 370, ']')


'Epoch [371] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 538,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006318629030031728,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1, 4, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 371,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 371, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277542.498168)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277589.019562)
('Worker processing elapsed time: ', 46.52139401435852, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[371]', 'EPOCH LOSS:', 0.018056421998214545, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 371, ']')


'Epoch [372] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 560,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003553735356013034,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 372,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 372, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277589.025551)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277624.277829)
('Worker processing elapsed time: ', 35.252277851104736, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[372]', 'EPOCH LOSS:', 0.76030240025351836, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 372, ']')


'Epoch [373] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 484,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004361750876741312,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 373,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 373, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277624.283244)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277659.333735)
('Worker processing elapsed time: ', 35.05049109458923, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[373]', 'EPOCH LOSS:', 0.67347199834399818, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 373, ']')


'Epoch [374] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 486,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006259460295553632,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 374,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 374, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277659.338562)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277694.196769)
('Worker processing elapsed time: ', 34.85820698738098, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[374]', 'EPOCH LOSS:', 0.023328589219944955, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 374, ']')


'Epoch [375] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 358,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004158900391842971,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 375,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 375, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277694.202314)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277729.143639)
('Worker processing elapsed time: ', 34.941325187683105, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[375]', 'EPOCH LOSS:', 0.49401063546496665, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 375, ']')


'Epoch [376] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 270,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00075792235847911,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 4, 2, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 376,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 376, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277729.149345)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277762.989437)
('Worker processing elapsed time: ', 33.840092182159424, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[376]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 376, ']')


'Epoch [377] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 466,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00015444044739256868,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 5, 2, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 377,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 377, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277762.995591)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277811.830925)
('Worker processing elapsed time: ', 48.835334062576294, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[377]', 'EPOCH LOSS:', 0.046946824933756258, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 377, ']')


'Epoch [378] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 266,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00017560556372626208,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 378,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 378, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277811.835897)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277851.550455)
('Worker processing elapsed time: ', 39.714558124542236, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[378]', 'EPOCH LOSS:', 0.021321310116536152, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 378, ']')


'Epoch [379] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 361,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00018708970534755888,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 379,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 379, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277851.556794)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277890.211093)
('Worker processing elapsed time: ', 38.65429902076721, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[379]', 'EPOCH LOSS:', 0.6500882067467012, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 379, ']')


'Epoch [380] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 385,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007208262120803951,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 5, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 380,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 380, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277890.216184)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277933.046252)
('Worker processing elapsed time: ', 42.83006811141968, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[380]', 'EPOCH LOSS:', 0.00029672533023296577, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 380, ']')


'Epoch [381] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 450,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008477128140732702,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 6, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 381,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 381, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277933.051338)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494277980.639105)
('Worker processing elapsed time: ', 47.587767124176025, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[381]', 'EPOCH LOSS:', 0.0010783380397934228, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 381, ']')


'Epoch [382] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 530,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005777760098605062,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 1, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 382,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 382, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494277980.645313)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278023.990367)
('Worker processing elapsed time: ', 43.345053911209106, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[382]', 'EPOCH LOSS:', 4.7673188066934511, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 382, ']')


'Epoch [383] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 558,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006309636970073899,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 383,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 383, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278023.995534)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278057.456226)
('Worker processing elapsed time: ', 33.460692167282104, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[383]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 383, ']')


'Epoch [384] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 258,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006583529469227769,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 384,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 384, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278057.461133)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278097.404044)
('Worker processing elapsed time: ', 39.94291090965271, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[384]', 'EPOCH LOSS:', 0.00062522231809246191, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 384, ']')


'Epoch [385] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 414,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008879838213957023,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 385,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 385, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278097.409347)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278141.730573)
('Worker processing elapsed time: ', 44.32122588157654, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[385]', 'EPOCH LOSS:', 0.024658222461680163, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 385, ']')


'Epoch [386] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 440,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000775516052388936,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 386,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 386, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278141.736509)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278181.956059)
('Worker processing elapsed time: ', 40.219549894332886, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[386]', 'EPOCH LOSS:', 0.0062129116174676619, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 386, ']')


'Epoch [387] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 600,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009107958378325712,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 387,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 387, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278181.962355)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278219.431749)
('Worker processing elapsed time: ', 37.46939420700073, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[387]', 'EPOCH LOSS:', 1.4633939199379935, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 387, ']')


'Epoch [388] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 276,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007095983846731923,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 388,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 388, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278219.437793)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278260.06273)
('Worker processing elapsed time: ', 40.62493705749512, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[388]', 'EPOCH LOSS:', 0.025599413052114647, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 388, ']')


'Epoch [389] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 499,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006612442284477404,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 389,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 389, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278260.068328)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278297.347364)
('Worker processing elapsed time: ', 37.27903604507446, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[389]', 'EPOCH LOSS:', 0.11757269727005658, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 389, ']')


'Epoch [390] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 388,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007200596415933328,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 6, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 390,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 390, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278297.353019)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278344.496416)
('Worker processing elapsed time: ', 47.143397092819214, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[390]', 'EPOCH LOSS:', 0.00069453075126542775, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 390, ']')


'Epoch [391] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 427,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008297821395810495,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 391,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 391, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278344.501733)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278381.594667)
('Worker processing elapsed time: ', 37.092933893203735, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[391]', 'EPOCH LOSS:', 12.659823149509196, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 391, ']')


'Epoch [392] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 266,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006218806543205845,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 392,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 392, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278381.600923)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278423.641516)
('Worker processing elapsed time: ', 42.04059290885925, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[392]', 'EPOCH LOSS:', 0.025799269032197314, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 392, ']')


'Epoch [393] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 428,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003305278512468295,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 1, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 393,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 393, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278423.64657)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278468.028066)
('Worker processing elapsed time: ', 44.3814959526062, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[393]', 'EPOCH LOSS:', 58.360207470502111, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 393, ']')


'Epoch [394] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 485,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009503962698382569,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 3, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 394,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 394, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278468.034553)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278515.375056)
('Worker processing elapsed time: ', 47.340502977371216, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[394]', 'EPOCH LOSS:', 0.003992175364304823, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 394, ']')


'Epoch [395] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 259,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00027883345080000294,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 6, 3, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 395,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 395, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278515.380845)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278558.35661)
('Worker processing elapsed time: ', 42.975764989852905, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[395]', 'EPOCH LOSS:', 0.022564902950563792, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 395, ']')


'Epoch [396] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 418,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00043123474710928596,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 396,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 396, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278558.362219)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278596.286803)
('Worker processing elapsed time: ', 37.92458391189575, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[396]', 'EPOCH LOSS:', 0.0089200251303934355, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 396, ']')


'Epoch [397] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 347,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00043153150982299347,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 397,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 397, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278596.292683)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278630.983785)
('Worker processing elapsed time: ', 34.691102027893066, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[397]', 'EPOCH LOSS:', 0.32543100490294519, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 397, ']')


'Epoch [398] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 286,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00019939940823186732,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 5, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 398,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 398, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278630.989716)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278677.062676)
('Worker processing elapsed time: ', 46.072959899902344, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[398]', 'EPOCH LOSS:', 0.0121389692174533, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 398, ']')


'Epoch [399] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 603,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003111059560370374,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 399,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 399, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278677.068621)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278716.741262)
('Worker processing elapsed time: ', 39.67264103889465, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[399]', 'EPOCH LOSS:', 0.017292747993765629, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 399, ']')


'Epoch [400] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 539,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007428540147925637,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 400,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 400, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278716.746336)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278757.56686)
('Worker processing elapsed time: ', 40.82052397727966, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[400]', 'EPOCH LOSS:', 0.00056018873219682225, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 400, ']')


'Epoch [401] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 314,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003676282443644723,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 401,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 401, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278757.572355)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278794.067197)
('Worker processing elapsed time: ', 36.49484205245972, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[401]', 'EPOCH LOSS:', 0.18926961247340254, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 401, ']')


'Epoch [402] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 460,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007328930582094022,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 3, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 402,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 402, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278794.072331)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278836.33094)
('Worker processing elapsed time: ', 42.25860905647278, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[402]', 'EPOCH LOSS:', 0.021319582759443692, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 402, ']')


'Epoch [403] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 349,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00011339564309238682,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 403,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 403, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278836.336138)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278878.600201)
('Worker processing elapsed time: ', 42.26406288146973, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[403]', 'EPOCH LOSS:', 0.024224770891337559, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 403, ']')


'Epoch [404] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 397,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009556998036965602,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 404,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 404, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278878.605469)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278919.298329)
('Worker processing elapsed time: ', 40.69286012649536, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[404]', 'EPOCH LOSS:', 3.0286090979716769, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 404, ']')


'Epoch [405] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 453,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00034560910542397476,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 405,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 405, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278919.304241)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278952.674364)
('Worker processing elapsed time: ', 33.37012314796448, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[405]', 'EPOCH LOSS:', 0.015041505494314584, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 405, ']')


'Epoch [406] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 340,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008559795300681898,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 406,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 406, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278952.680082)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494278989.2471)
('Worker processing elapsed time: ', 36.567018032073975, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[406]', 'EPOCH LOSS:', 0.021326185090668737, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 406, ']')


'Epoch [407] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 591,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008410105897636255,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 407,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 407, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494278989.252418)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279024.437757)
('Worker processing elapsed time: ', 35.18533897399902, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[407]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 407, ']')


'Epoch [408] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 311,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0001780060600150837,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 4, 6, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 408,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 408, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279024.443538)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279072.817009)
('Worker processing elapsed time: ', 48.37347102165222, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[408]', 'EPOCH LOSS:', 0.013817696166916849, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 408, ']')


'Epoch [409] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 347,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008715506372491153,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 409,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 409, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279072.822902)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279107.515756)
('Worker processing elapsed time: ', 34.692853927612305, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[409]', 'EPOCH LOSS:', 0.17716478935233571, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 409, ']')


'Epoch [410] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 583,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007654772825759478,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 3, 3, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 410,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 410, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279107.521956)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279141.306981)
('Worker processing elapsed time: ', 33.785025119781494, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[410]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 410, ']')


'Epoch [411] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 385,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00048028417601907864,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 2, 3, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 411,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 411, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279141.312189)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279184.620207)
('Worker processing elapsed time: ', 43.30801796913147, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[411]', 'EPOCH LOSS:', 0.016802622752608347, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 411, ']')


'Epoch [412] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 576,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006096856957166434,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 412,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 412, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279184.625925)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279222.009801)
('Worker processing elapsed time: ', 37.38387584686279, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[412]', 'EPOCH LOSS:', 0.16578569629846196, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 412, ']')


'Epoch [413] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 398,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008972477921909638,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 413,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 413, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279222.015212)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279263.015474)
('Worker processing elapsed time: ', 41.00026202201843, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[413]', 'EPOCH LOSS:', 0.16795314463003169, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 413, ']')


'Epoch [414] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 394,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005259859120745885,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 414,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 414, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279263.020602)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279301.701068)
('Worker processing elapsed time: ', 38.68046593666077, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[414]', 'EPOCH LOSS:', 0.015789423353235979, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 414, ']')


'Epoch [415] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 357,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008477018403443619,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 415,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 415, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279301.707006)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279341.85973)
('Worker processing elapsed time: ', 40.15272402763367, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[415]', 'EPOCH LOSS:', 0.0017816762649634644, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 415, ']')


'Epoch [416] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 314,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005304680728816303,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 416,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 416, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279341.865539)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279378.564427)
('Worker processing elapsed time: ', 36.69888782501221, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[416]', 'EPOCH LOSS:', 0.048100864313249531, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 416, ']')


'Epoch [417] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 258,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009051786274701847,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 417,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 417, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279378.570077)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279416.090603)
('Worker processing elapsed time: ', 37.52052617073059, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[417]', 'EPOCH LOSS:', 0.011705327336329507, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 417, ']')


'Epoch [418] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 318,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008810261884550578,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 418,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 418, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279416.096414)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279452.764267)
('Worker processing elapsed time: ', 36.66785287857056, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[418]', 'EPOCH LOSS:', 0.84290101295248665, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 418, ']')


'Epoch [419] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 466,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00042046593281463605,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 5, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 419,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 419, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279452.77007)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279495.814353)
('Worker processing elapsed time: ', 43.04428291320801, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[419]', 'EPOCH LOSS:', 13.986295119420054, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 419, ']')


'Epoch [420] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 407,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000547648356052299,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 420,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 420, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279495.81937)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279538.159202)
('Worker processing elapsed time: ', 42.339832067489624, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[420]', 'EPOCH LOSS:', 0.01397443093252993, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 420, ']')


'Epoch [421] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 486,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009496569060185658,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 3, 4, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 421,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 421, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279538.165726)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279582.194558)
('Worker processing elapsed time: ', 44.02883195877075, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[421]', 'EPOCH LOSS:', 0.021210186598884941, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 421, ']')


'Epoch [422] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 335,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003698911531562083,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 422,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 422, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279582.200339)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279619.161801)
('Worker processing elapsed time: ', 36.96146202087402, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[422]', 'EPOCH LOSS:', 25.916626331437623, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 422, ']')


'Epoch [423] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 344,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00023084065765848144,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 423,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 423, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279619.167613)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279657.989059)
('Worker processing elapsed time: ', 38.82144594192505, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[423]', 'EPOCH LOSS:', 0.16927581215819901, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 423, ']')


'Epoch [424] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 619,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000897775724596535,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 424,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 424, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279657.9942)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279699.049505)
('Worker processing elapsed time: ', 41.05530500411987, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[424]', 'EPOCH LOSS:', 0.023479568759039556, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 424, ']')


'Epoch [425] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 323,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00035923126875903424,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 425,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 425, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279699.055104)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279741.103508)
('Worker processing elapsed time: ', 42.04840397834778, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[425]', 'EPOCH LOSS:', 0.022671602770335673, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 425, ']')


'Epoch [426] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 436,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005623848290159453,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 3, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 426,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 426, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279741.109309)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279781.55518)
('Worker processing elapsed time: ', 40.445871114730835, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[426]', 'EPOCH LOSS:', 0.00097329008460090107, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 426, ']')


'Epoch [427] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 416,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005343215502947604,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 427,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 427, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279781.561114)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279820.242938)
('Worker processing elapsed time: ', 38.68182396888733, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[427]', 'EPOCH LOSS:', 0.00049037731731537651, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 427, ']')


'Epoch [428] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 512,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009088737163181122,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 428,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 428, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279820.247934)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279859.072199)
('Worker processing elapsed time: ', 38.824265003204346, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[428]', 'EPOCH LOSS:', 0.023436773103520856, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 428, ']')


'Epoch [429] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 416,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00018752321312487863,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 6, 2, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 429,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 429, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279859.07708)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279908.035721)
('Worker processing elapsed time: ', 48.958641052246094, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[429]', 'EPOCH LOSS:', 0.015137794147558578, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 429, ']')


'Epoch [430] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 386,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007712891346617326,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 430,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 430, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279908.041699)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279948.652339)
('Worker processing elapsed time: ', 40.61064004898071, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[430]', 'EPOCH LOSS:', 0.13153115308908631, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 430, ']')


'Epoch [431] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 543,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009819995730538063,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 431,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 431, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279948.65842)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494279986.6655)
('Worker processing elapsed time: ', 38.00707983970642, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[431]', 'EPOCH LOSS:', 0.011447404569550545, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 431, ']')


'Epoch [432] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 535,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006275720716470449,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 432,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 432, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494279986.670767)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280026.463396)
('Worker processing elapsed time: ', 39.79262900352478, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[432]', 'EPOCH LOSS:', 2.6919191698424094, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 432, ']')


'Epoch [433] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 521,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000526293212600277,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 433,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 433, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280026.468619)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280066.402271)
('Worker processing elapsed time: ', 39.9336519241333, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[433]', 'EPOCH LOSS:', 0.10075914433696867, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 433, ']')


'Epoch [434] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 273,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005188199576872652,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 434,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 434, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280066.407568)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280100.792647)
('Worker processing elapsed time: ', 34.38507890701294, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[434]', 'EPOCH LOSS:', 0.0096853768901978363, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 434, ']')


'Epoch [435] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 331,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006036268745068527,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 3, 6, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 435,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 435, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280100.797914)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280138.332062)
('Worker processing elapsed time: ', 37.53414797782898, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[435]', 'EPOCH LOSS:', nan, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 435, ']')


'Epoch [436] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 569,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003811284039654469,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 436,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 436, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280138.337443)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280175.601435)
('Worker processing elapsed time: ', 37.263991832733154, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[436]', 'EPOCH LOSS:', 0.019630383802966968, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 436, ']')


'Epoch [437] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 348,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007242717111703128,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 3, 5, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 437,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 437, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280175.607042)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280219.12468)
('Worker processing elapsed time: ', 43.517637968063354, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[437]', 'EPOCH LOSS:', 0.020769708907284214, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 437, ']')


'Epoch [438] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 371,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005180556619504944,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 3, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 438,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 438, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280219.129642)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280265.711313)
('Worker processing elapsed time: ', 46.58167099952698, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[438]', 'EPOCH LOSS:', 0.018117073664371278, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 438, ']')


'Epoch [439] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 349,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009993106954405595,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 439,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 439, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280265.716646)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280307.943895)
('Worker processing elapsed time: ', 42.22724914550781, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[439]', 'EPOCH LOSS:', 0.019298553350541651, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 439, ']')


'Epoch [440] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 561,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000302023360950296,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 440,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 440, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280307.949661)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280352.91913)
('Worker processing elapsed time: ', 44.96946907043457, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[440]', 'EPOCH LOSS:', 0.022360402968327201, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 440, ']')


'Epoch [441] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 400,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003862666838112329,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 441,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 441, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280352.924273)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280395.522622)
('Worker processing elapsed time: ', 42.59834909439087, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[441]', 'EPOCH LOSS:', 0.0063841906560947372, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 441, ']')


'Epoch [442] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 439,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005686288803149532,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 4, 1, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 442,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 442, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280395.528221)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280444.867788)
('Worker processing elapsed time: ', 49.33956718444824, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[442]', 'EPOCH LOSS:', 0.00025122298039556313, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 442, ']')


'Epoch [443] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 333,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00017549773591146115,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 443,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 443, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280444.872926)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280485.074136)
('Worker processing elapsed time: ', 40.201210021972656, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[443]', 'EPOCH LOSS:', 0.017840205411989621, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 443, ']')


'Epoch [444] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 505,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00046955717528974453,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 444,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 444, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280485.080103)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280524.708409)
('Worker processing elapsed time: ', 39.6283061504364, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[444]', 'EPOCH LOSS:', 0.63686652514691655, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 444, ']')


'Epoch [445] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 565,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000688665680220002,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 2, 5, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 445,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 445, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280524.713347)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280569.132113)
('Worker processing elapsed time: ', 44.418766021728516, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[445]', 'EPOCH LOSS:', 0.023299201395868323, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 445, ']')


'Epoch [446] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 565,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006861033398477803,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 6, 1, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 446,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 446, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280569.137699)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280610.055953)
('Worker processing elapsed time: ', 40.918254137039185, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[446]', 'EPOCH LOSS:', 0.016619480467458245, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 446, ']')


'Epoch [447] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 435,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000983002276027008,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 447,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 447, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280610.062124)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280643.330084)
('Worker processing elapsed time: ', 33.26796007156372, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[447]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 447, ']')


'Epoch [448] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 287,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00044662099458490706,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 4, 1, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 448,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 448, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280643.335665)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280686.300659)
('Worker processing elapsed time: ', 42.964993953704834, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[448]', 'EPOCH LOSS:', 0.025001759285112928, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 448, ']')


'Epoch [449] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 493,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008673743459086179,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 449,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 449, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280686.306205)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280723.316539)
('Worker processing elapsed time: ', 37.01033401489258, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[449]', 'EPOCH LOSS:', 0.018653067933110144, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 449, ']')


'Epoch [450] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 551,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000312550773921187,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 3, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 450,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 450, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280723.323548)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280770.778442)
('Worker processing elapsed time: ', 47.454893827438354, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[450]', 'EPOCH LOSS:', 0.017270899446932515, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 450, ']')


'Epoch [451] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 520,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009548062296055207,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 451,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 451, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280770.783408)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280808.335253)
('Worker processing elapsed time: ', 37.55184507369995, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[451]', 'EPOCH LOSS:', 0.10109432099294016, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 451, ']')


'Epoch [452] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 516,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009850578894412738,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1, 4, 6, 5, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 452,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 452, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280808.340919)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280852.791502)
('Worker processing elapsed time: ', 44.45058298110962, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[452]', 'EPOCH LOSS:', 0.024185869820557232, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 452, ']')


'Epoch [453] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 438,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008329477984246272,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 453,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 453, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280852.797523)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280891.355196)
('Worker processing elapsed time: ', 38.55767297744751, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[453]', 'EPOCH LOSS:', 0.021061021860881178, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 453, ']')


'Epoch [454] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 387,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006663786709653427,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 454,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 454, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280891.360682)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280931.629876)
('Worker processing elapsed time: ', 40.26919388771057, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[454]', 'EPOCH LOSS:', 0.00097091451697508397, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 454, ']')


'Epoch [455] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 509,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007793166659468822,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 4, 6, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 455,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 455, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280931.635871)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494280965.338074)
('Worker processing elapsed time: ', 33.702203035354614, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[455]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 455, ']')


'Epoch [456] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 606,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008285430955132333,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 1, 1, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 456,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 456, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494280965.343225)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281010.014982)
('Worker processing elapsed time: ', 44.671756982803345, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[456]', 'EPOCH LOSS:', 19.379216738534165, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 456, ']')


'Epoch [457] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 600,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007589852505960089,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 457,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 457, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281010.020157)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281043.438626)
('Worker processing elapsed time: ', 33.418468952178955, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[457]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 457, ']')


'Epoch [458] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 371,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008988328628965734,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 458,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 458, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281043.444608)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281085.722041)
('Worker processing elapsed time: ', 42.277432918548584, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[458]', 'EPOCH LOSS:', 0.017120461771417111, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 458, ']')


'Epoch [459] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 446,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00038704995807637105,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 459,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 459, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281085.727633)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281123.696138)
('Worker processing elapsed time: ', 37.968504905700684, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[459]', 'EPOCH LOSS:', 0.00082407132982659323, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 459, ']')


'Epoch [460] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 612,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00019078759591840413,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 6, 5, 3, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 460,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 460, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281123.702034)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281172.942518)
('Worker processing elapsed time: ', 49.24048399925232, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[460]', 'EPOCH LOSS:', 0.013979473952025995, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 460, ']')


'Epoch [461] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 393,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006726081052998556,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 461,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 461, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281172.947191)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281210.705115)
('Worker processing elapsed time: ', 37.75792407989502, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[461]', 'EPOCH LOSS:', 0.0020130706671311451, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 461, ']')


'Epoch [462] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 491,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005209019707317444,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 4, 4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 462,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 462, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281210.710716)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281257.58556)
('Worker processing elapsed time: ', 46.87484407424927, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[462]', 'EPOCH LOSS:', 0.01750850860467585, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 462, ']')


'Epoch [463] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 477,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003218254292564266,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 2, 1, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 463,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 463, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281257.592016)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281300.926621)
('Worker processing elapsed time: ', 43.3346049785614, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[463]', 'EPOCH LOSS:', 0.017617359616684144, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 463, ']')


'Epoch [464] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 522,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005074968583438601,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 464,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 464, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281300.932819)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281334.456971)
('Worker processing elapsed time: ', 33.52415204048157, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[464]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 464, ']')


'Epoch [465] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 569,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005300774014191767,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 465,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 465, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281334.46268)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281369.795796)
('Worker processing elapsed time: ', 35.33311581611633, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[465]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 465, ']')


'Epoch [466] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 611,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00017740393239919629,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 466,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 466, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281369.800898)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281405.406072)
('Worker processing elapsed time: ', 35.60517382621765, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[466]', 'EPOCH LOSS:', 1.7798808739296694, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 466, ']')


'Epoch [467] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 592,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006911993296621194,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 2, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 467,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 467, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281405.411524)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281452.832014)
('Worker processing elapsed time: ', 47.420490026474, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[467]', 'EPOCH LOSS:', 0.016551986236267191, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 467, ']')


'Epoch [468] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 259,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00043380569437513996,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 2, 6, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 468,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 468, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281452.838068)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281494.569139)
('Worker processing elapsed time: ', 41.73107099533081, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[468]', 'EPOCH LOSS:', 0.00029122889632689789, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 468, ']')


'Epoch [469] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 388,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00031377302718861046,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 469,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 469, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281494.574407)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281534.620683)
('Worker processing elapsed time: ', 40.04627585411072, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[469]', 'EPOCH LOSS:', 0.0027983923099596927, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 469, ']')


'Epoch [470] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 305,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003124863336404465,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1, 2, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 470,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 470, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281534.626717)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281581.082984)
('Worker processing elapsed time: ', 46.4562668800354, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[470]', 'EPOCH LOSS:', 0.0021421034339101264, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 470, ']')


'Epoch [471] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 461,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005267485024583348,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 5, 6, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 471,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 471, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281581.089146)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281628.478819)
('Worker processing elapsed time: ', 47.38967299461365, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[471]', 'EPOCH LOSS:', 0.00083560105962649325, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 471, ']')


'Epoch [472] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 373,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006868032900950126,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 472,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 472, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281628.484687)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281663.083539)
('Worker processing elapsed time: ', 34.598851919174194, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[472]', 'EPOCH LOSS:', 0.01961966509832368, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 472, ']')


'Epoch [473] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 403,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005324145044314384,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 1, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 473,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 473, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281663.088549)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281701.04097)
('Worker processing elapsed time: ', 37.95242118835449, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[473]', 'EPOCH LOSS:', nan, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 473, ']')


'Epoch [474] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 431,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00010801090956750223,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 1, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 474,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 474, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281701.046253)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281745.619557)
('Worker processing elapsed time: ', 44.57330393791199, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[474]', 'EPOCH LOSS:', 0.018492229922035652, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 474, ']')


'Epoch [475] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 603,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009696981715775099,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 475,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 475, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281745.625104)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281788.766142)
('Worker processing elapsed time: ', 43.141037940979004, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[475]', 'EPOCH LOSS:', 0.017980728439193707, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 475, ']')


'Epoch [476] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 400,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004165215488843069,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 5, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 476,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 476, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281788.770957)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281827.425074)
('Worker processing elapsed time: ', 38.65411710739136, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[476]', 'EPOCH LOSS:', 0.0207026131271251, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 476, ']')


'Epoch [477] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 467,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000743911995431078,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 5, 2, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 477,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 477, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281827.431092)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281869.620202)
('Worker processing elapsed time: ', 42.18911004066467, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[477]', 'EPOCH LOSS:', 0.017969416388877918, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 477, ']')


'Epoch [478] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 447,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00043507711549046135,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 2, 6, 6, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 478,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 478, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281869.626495)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281916.407404)
('Worker processing elapsed time: ', 46.780909061431885, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[478]', 'EPOCH LOSS:', 0.02189456403387216, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 478, ']')


'Epoch [479] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 341,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006517426433407373,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 1, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 479,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 479, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281916.413414)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281956.444554)
('Worker processing elapsed time: ', 40.031140089035034, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[479]', 'EPOCH LOSS:', 3.8042358272514232, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 479, ']')


'Epoch [480] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 462,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00027015421035215105,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 2, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 480,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 480, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281956.449847)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494281999.173498)
('Worker processing elapsed time: ', 42.72365093231201, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[480]', 'EPOCH LOSS:', 0.20578470005987892, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 480, ']')


'Epoch [481] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 334,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0004772594406760786,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 481,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 481, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494281999.178822)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282039.113874)
('Worker processing elapsed time: ', 39.935051918029785, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[481]', 'EPOCH LOSS:', 0.016245896364372309, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 481, ']')


'Epoch [482] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 428,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0008476522271094777,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 6, 3, 4, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 482,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 482, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282039.119285)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282083.306259)
('Worker processing elapsed time: ', 44.18697381019592, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[482]', 'EPOCH LOSS:', 9999.0, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 482, ']')


'Epoch [483] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 601,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009843150610717956,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 1, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 483,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 483, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282083.311196)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282126.639783)
('Worker processing elapsed time: ', 43.32858681678772, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[483]', 'EPOCH LOSS:', 0.020232755636878513, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 483, ']')


'Epoch [484] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 310,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00011818167051439452,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 1, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 484,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 484, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282126.645717)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282166.945973)
('Worker processing elapsed time: ', 40.30025601387024, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[484]', 'EPOCH LOSS:', 7.4234239974302838, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 484, ']')


'Epoch [485] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 513,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00040300914384134765,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 6, 4, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 485,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 485, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282166.951375)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282212.471204)
('Worker processing elapsed time: ', 45.5198290348053, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[485]', 'EPOCH LOSS:', 0.00036111092514181818, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 485, ']')


'Epoch [486] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 485,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009684061106014255,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 486,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 486, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282212.476373)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282257.21509)
('Worker processing elapsed time: ', 44.7387170791626, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[486]', 'EPOCH LOSS:', 0.016924073418285005, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 486, ']')


'Epoch [487] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 545,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0006612654181040318,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 6, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 487,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 487, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282257.22031)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282296.790982)
('Worker processing elapsed time: ', 39.570672035217285, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[487]', 'EPOCH LOSS:', 1.4325720133859177, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 487, ']')


'Epoch [488] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 516,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005497556808254232,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 488,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 488, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282296.796953)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282333.947236)
('Worker processing elapsed time: ', 37.150283098220825, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[488]', 'EPOCH LOSS:', 0.018758817622152899, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 488, ']')


'Epoch [489] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 463,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007932185185251612,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 489,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 489, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282333.952425)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282372.959296)
('Worker processing elapsed time: ', 39.00687098503113, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[489]', 'EPOCH LOSS:', 0.066235407395067225, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 489, ']')


'Epoch [490] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 550,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0001248047285121143,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 4, 5, 2, 3, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 490,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 490, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282372.964766)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282418.912382)
('Worker processing elapsed time: ', 45.9476158618927, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[490]', 'EPOCH LOSS:', 4.4794961750591522, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 490, ']')


'Epoch [491] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 318,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000985672183304083,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 1, 2, 2, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 491,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 491, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282418.917593)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282459.805564)
('Worker processing elapsed time: ', 40.88797092437744, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[491]', 'EPOCH LOSS:', 0.00056018912382469616, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 491, ']')


'Epoch [492] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 369,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0005965397326682335,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 492,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 492, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282459.811583)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282494.634083)
('Worker processing elapsed time: ', 34.82249999046326, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[492]', 'EPOCH LOSS:', 0.49066763195137997, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 492, ']')


'Epoch [493] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 395,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00029731016220943107,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 5, 5, 3, 4, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 493,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 493, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282494.639648)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282536.767125)
('Worker processing elapsed time: ', 42.127476930618286, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[493]', 'EPOCH LOSS:', 0.018487912903529077, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 493, ']')


'Epoch [494] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 443,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007189046956604984,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 3, 2, 4, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 494,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 494, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282536.772184)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282581.3009)
('Worker processing elapsed time: ', 44.52871608734131, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[494]', 'EPOCH LOSS:', 0.022689989570272285, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 494, ']')


'Epoch [495] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 424,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.000923472838465945,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 2, 3, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 495,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 495, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282581.305804)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282618.163003)
('Worker processing elapsed time: ', 36.85719895362854, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[495]', 'EPOCH LOSS:', 0.025000387957542662, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 495, ']')


'Epoch [496] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 407,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009816229063604302,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 496,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 496, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282618.168725)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282653.073678)
('Worker processing elapsed time: ', 34.90495300292969, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[496]', 'EPOCH LOSS:', 0.30589387225720444, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 496, ']')


'Epoch [497] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 312,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.00033130566026410993,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 3, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 497,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 497, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282653.079324)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282695.19585)
('Worker processing elapsed time: ', 42.11652588844299, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[497]', 'EPOCH LOSS:', 0.0029278630328856648, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 497, ']')


'Epoch [498] config:'
{'activation': 'relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 298,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0003966342795434561,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 3, 2, 6, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 498,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 498, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282695.201631)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282733.472061)
('Worker processing elapsed time: ', 38.270429849624634, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[498]', 'EPOCH LOSS:', 0.017804937010911098, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 498, ']')


'Epoch [499] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 354,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0007069214471855915,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 5, 3, 3, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 499,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 499, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282733.47696)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282768.964941)
('Worker processing elapsed time: ', 35.48798108100891, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[499]', 'EPOCH LOSS:', nan, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 499, ']')


'Epoch [500] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 511,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009504295718777201,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 6, 2, 2, 2, 5, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 500,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
('START OF Optimizer EPOCH ====================>> [', 500, ']')
('Deleteing old train dir, sync_replica is enabled...', '/tmp/nn_dist/train_logs')
('Forking Worker/PS Cluster Jobs @ ', 1494282768.970043)
Waiting for Worker process to complete....
(1, '/', 4, ' of Worker Process Done..!!')
(2, '/', 4, ' of Worker Process Done..!!')
(3, '/', 4, ' of Worker Process Done..!!')
(4, '/', 4, ' of Worker Process DONE..!!')
('Worker processing ends @ ', 1494282816.505373)
('Worker processing elapsed time: ', 47.535330057144165, 'secs')
Cleanup Worker/PS Cluster Jobs..
('[500]', 'EPOCH LOSS:', 0.0011729871434412705, 'BEST LOSS:', 6.3920782384329833e-05)
('END OF Optimizer EPOCH =====================>>[', 500, ']')


END OF STAGED EPOCH #######################>>[ 3 ]
=========================
Summary:
=========================
('Stages.......:', [[1000, 1000], [5000, 750], [10000, 500]])
('FINAL LOSS...:', 6.3920782384329833e-05)
=========================
'BEST Epoch config:'
--------------------
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 375,
 'data_features': 4,
 'data_instances': 10000,
 'file2distribute': 'nn_distributed.py',
 'hidden_layer_bounds': [1, 7],
 'input_dim': 4,
 'learning_rate': 0.0009233924655534873,
 'load_data': False,
 'load_data_dir': 'data/source_code',
 'nn_model': 'Regressor',
 'nodes_per_layer': [4, 4, 5, 2, 2, 2, 1, 1],
 'num_gpus': 1,
 'opt_epoch': 500,
 'opt_epoch_iter': 41,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223,localhost:2224',
 'rnn_max_seq_length': 100,
 'rnn_state_dim': [50, 50, 50],
 'running_stage': 3,
 'sync_replicas': False,
 'train_epoch': 10000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adam',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225,localhost:2226,localhost:2227,localhost:2228'}
